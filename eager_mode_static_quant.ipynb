{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.quant = torch.ao.quantization.QuantStub()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, 3, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(8, 8*8, 3, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(3, 2),\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256, 128), \n",
    "            nn.Dropout(),\n",
    "            nn.Linear(128, 10) \n",
    "            # nn.Softmax()\n",
    "        )\n",
    "        self.dequant = torch.ao.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        y = self.model(x)\n",
    "        return self.dequant(y)\n",
    "    \n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X = X.to(\"cuda\")\n",
    "        y = y.to(\"cuda\")\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier()\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.296914  [   64/60000]\n",
      "loss: 2.291912  [ 6464/60000]\n",
      "loss: 2.294836  [12864/60000]\n",
      "loss: 2.288554  [19264/60000]\n",
      "loss: 2.309775  [25664/60000]\n",
      "loss: 2.286273  [32064/60000]\n",
      "loss: 2.322832  [38464/60000]\n",
      "loss: 2.305559  [44864/60000]\n",
      "loss: 2.295563  [51264/60000]\n",
      "loss: 2.307890  [57664/60000]\n",
      "loss: 2.297695  [   64/60000]\n",
      "loss: 2.284710  [ 6464/60000]\n",
      "loss: 2.295141  [12864/60000]\n",
      "loss: 2.285764  [19264/60000]\n",
      "loss: 2.301398  [25664/60000]\n",
      "loss: 2.281679  [32064/60000]\n",
      "loss: 2.315851  [38464/60000]\n",
      "loss: 2.290140  [44864/60000]\n",
      "loss: 2.298728  [51264/60000]\n",
      "loss: 2.291737  [57664/60000]\n",
      "loss: 2.282570  [   64/60000]\n",
      "loss: 2.296183  [ 6464/60000]\n",
      "loss: 2.305233  [12864/60000]\n",
      "loss: 2.283379  [19264/60000]\n",
      "loss: 2.302626  [25664/60000]\n",
      "loss: 2.267376  [32064/60000]\n",
      "loss: 2.306221  [38464/60000]\n",
      "loss: 2.285654  [44864/60000]\n",
      "loss: 2.290928  [51264/60000]\n",
      "loss: 2.281875  [57664/60000]\n",
      "loss: 2.292846  [   64/60000]\n",
      "loss: 2.284919  [ 6464/60000]\n",
      "loss: 2.280387  [12864/60000]\n",
      "loss: 2.268121  [19264/60000]\n",
      "loss: 2.306406  [25664/60000]\n",
      "loss: 2.254903  [32064/60000]\n",
      "loss: 2.286246  [38464/60000]\n",
      "loss: 2.282933  [44864/60000]\n",
      "loss: 2.281529  [51264/60000]\n",
      "loss: 2.258528  [57664/60000]\n",
      "loss: 2.307843  [   64/60000]\n",
      "loss: 2.273869  [ 6464/60000]\n",
      "loss: 2.253314  [12864/60000]\n",
      "loss: 2.259849  [19264/60000]\n",
      "loss: 2.283562  [25664/60000]\n",
      "loss: 2.232274  [32064/60000]\n",
      "loss: 2.278446  [38464/60000]\n",
      "loss: 2.261590  [44864/60000]\n",
      "loss: 2.276636  [51264/60000]\n",
      "loss: 2.244184  [57664/60000]\n",
      "loss: 2.268093  [   64/60000]\n",
      "loss: 2.265302  [ 6464/60000]\n",
      "loss: 2.231915  [12864/60000]\n",
      "loss: 2.227702  [19264/60000]\n",
      "loss: 2.267839  [25664/60000]\n",
      "loss: 2.237991  [32064/60000]\n",
      "loss: 2.235929  [38464/60000]\n",
      "loss: 2.245381  [44864/60000]\n",
      "loss: 2.243620  [51264/60000]\n",
      "loss: 2.203897  [57664/60000]\n",
      "loss: 2.231515  [   64/60000]\n",
      "loss: 2.270835  [ 6464/60000]\n",
      "loss: 2.228090  [12864/60000]\n",
      "loss: 2.215429  [19264/60000]\n",
      "loss: 2.211668  [25664/60000]\n",
      "loss: 2.199087  [32064/60000]\n",
      "loss: 2.214606  [38464/60000]\n",
      "loss: 2.207132  [44864/60000]\n",
      "loss: 2.210429  [51264/60000]\n",
      "loss: 2.155796  [57664/60000]\n",
      "loss: 2.215249  [   64/60000]\n",
      "loss: 2.169730  [ 6464/60000]\n",
      "loss: 2.170717  [12864/60000]\n",
      "loss: 2.146525  [19264/60000]\n",
      "loss: 2.184452  [25664/60000]\n",
      "loss: 2.120438  [32064/60000]\n",
      "loss: 2.129248  [38464/60000]\n",
      "loss: 2.144587  [44864/60000]\n",
      "loss: 2.128526  [51264/60000]\n",
      "loss: 2.123750  [57664/60000]\n",
      "loss: 2.133308  [   64/60000]\n",
      "loss: 2.106375  [ 6464/60000]\n",
      "loss: 2.078509  [12864/60000]\n",
      "loss: 2.052323  [19264/60000]\n",
      "loss: 2.076754  [25664/60000]\n",
      "loss: 1.969382  [32064/60000]\n",
      "loss: 2.072842  [38464/60000]\n",
      "loss: 2.023691  [44864/60000]\n",
      "loss: 2.006128  [51264/60000]\n",
      "loss: 1.985393  [57664/60000]\n",
      "loss: 2.009143  [   64/60000]\n",
      "loss: 1.939933  [ 6464/60000]\n",
      "loss: 1.959713  [12864/60000]\n",
      "loss: 1.979222  [19264/60000]\n",
      "loss: 1.898352  [25664/60000]\n",
      "loss: 1.914258  [32064/60000]\n",
      "loss: 1.860809  [38464/60000]\n",
      "loss: 1.915380  [44864/60000]\n",
      "loss: 1.931140  [51264/60000]\n",
      "loss: 1.801583  [57664/60000]\n",
      "loss: 1.911487  [   64/60000]\n",
      "loss: 1.799587  [ 6464/60000]\n",
      "loss: 1.861441  [12864/60000]\n",
      "loss: 1.833879  [19264/60000]\n",
      "loss: 1.942818  [25664/60000]\n",
      "loss: 1.815547  [32064/60000]\n",
      "loss: 1.746418  [38464/60000]\n",
      "loss: 1.867750  [44864/60000]\n",
      "loss: 1.761404  [51264/60000]\n",
      "loss: 1.658317  [57664/60000]\n",
      "loss: 1.941005  [   64/60000]\n",
      "loss: 1.861532  [ 6464/60000]\n",
      "loss: 1.668774  [12864/60000]\n",
      "loss: 1.709658  [19264/60000]\n",
      "loss: 1.811235  [25664/60000]\n",
      "loss: 1.620034  [32064/60000]\n",
      "loss: 1.673114  [38464/60000]\n",
      "loss: 1.814447  [44864/60000]\n",
      "loss: 1.537034  [51264/60000]\n",
      "loss: 1.522863  [57664/60000]\n",
      "loss: 1.705038  [   64/60000]\n",
      "loss: 1.616050  [ 6464/60000]\n",
      "loss: 1.603694  [12864/60000]\n",
      "loss: 1.626748  [19264/60000]\n",
      "loss: 1.532946  [25664/60000]\n",
      "loss: 1.611802  [32064/60000]\n",
      "loss: 1.560371  [38464/60000]\n",
      "loss: 1.600847  [44864/60000]\n",
      "loss: 1.722118  [51264/60000]\n",
      "loss: 1.488720  [57664/60000]\n",
      "loss: 1.706157  [   64/60000]\n",
      "loss: 1.490766  [ 6464/60000]\n",
      "loss: 1.458162  [12864/60000]\n",
      "loss: 1.567036  [19264/60000]\n",
      "loss: 1.619944  [25664/60000]\n",
      "loss: 1.535180  [32064/60000]\n",
      "loss: 1.623741  [38464/60000]\n",
      "loss: 1.658059  [44864/60000]\n",
      "loss: 1.505155  [51264/60000]\n",
      "loss: 1.480345  [57664/60000]\n",
      "loss: 1.690053  [   64/60000]\n",
      "loss: 1.620724  [ 6464/60000]\n",
      "loss: 1.360252  [12864/60000]\n",
      "loss: 1.560254  [19264/60000]\n",
      "loss: 1.563829  [25664/60000]\n",
      "loss: 1.506102  [32064/60000]\n",
      "loss: 1.448621  [38464/60000]\n",
      "loss: 1.581970  [44864/60000]\n",
      "loss: 1.466888  [51264/60000]\n",
      "loss: 1.418444  [57664/60000]\n",
      "loss: 1.466746  [   64/60000]\n",
      "loss: 1.566737  [ 6464/60000]\n",
      "loss: 1.265573  [12864/60000]\n",
      "loss: 1.613136  [19264/60000]\n",
      "loss: 1.557078  [25664/60000]\n",
      "loss: 1.516531  [32064/60000]\n",
      "loss: 1.640307  [38464/60000]\n",
      "loss: 1.540967  [44864/60000]\n",
      "loss: 1.518289  [51264/60000]\n",
      "loss: 1.326154  [57664/60000]\n",
      "loss: 1.396937  [   64/60000]\n",
      "loss: 1.426927  [ 6464/60000]\n",
      "loss: 1.380765  [12864/60000]\n",
      "loss: 1.427811  [19264/60000]\n",
      "loss: 1.561968  [25664/60000]\n",
      "loss: 1.485593  [32064/60000]\n",
      "loss: 1.447066  [38464/60000]\n",
      "loss: 1.456571  [44864/60000]\n",
      "loss: 1.378840  [51264/60000]\n",
      "loss: 1.424201  [57664/60000]\n",
      "loss: 1.501083  [   64/60000]\n",
      "loss: 1.455178  [ 6464/60000]\n",
      "loss: 1.335731  [12864/60000]\n",
      "loss: 1.344248  [19264/60000]\n",
      "loss: 1.478958  [25664/60000]\n",
      "loss: 1.505144  [32064/60000]\n",
      "loss: 1.409248  [38464/60000]\n",
      "loss: 1.544122  [44864/60000]\n",
      "loss: 1.393037  [51264/60000]\n",
      "loss: 1.404907  [57664/60000]\n",
      "loss: 1.445403  [   64/60000]\n",
      "loss: 1.398789  [ 6464/60000]\n",
      "loss: 1.290816  [12864/60000]\n",
      "loss: 1.396978  [19264/60000]\n",
      "loss: 1.481693  [25664/60000]\n",
      "loss: 1.377822  [32064/60000]\n",
      "loss: 1.449598  [38464/60000]\n",
      "loss: 1.470600  [44864/60000]\n",
      "loss: 1.484354  [51264/60000]\n",
      "loss: 1.309112  [57664/60000]\n",
      "loss: 1.442372  [   64/60000]\n",
      "loss: 1.371274  [ 6464/60000]\n",
      "loss: 1.274760  [12864/60000]\n",
      "loss: 1.469402  [19264/60000]\n",
      "loss: 1.527567  [25664/60000]\n",
      "loss: 1.313767  [32064/60000]\n",
      "loss: 1.411761  [38464/60000]\n",
      "loss: 1.424479  [44864/60000]\n",
      "loss: 1.344545  [51264/60000]\n",
      "loss: 1.379169  [57664/60000]\n",
      "loss: 1.428843  [   64/60000]\n",
      "loss: 1.472115  [ 6464/60000]\n",
      "loss: 1.302330  [12864/60000]\n",
      "loss: 1.365371  [19264/60000]\n",
      "loss: 1.412752  [25664/60000]\n",
      "loss: 1.325446  [32064/60000]\n",
      "loss: 1.398871  [38464/60000]\n",
      "loss: 1.348266  [44864/60000]\n",
      "loss: 1.276243  [51264/60000]\n",
      "loss: 1.404073  [57664/60000]\n",
      "loss: 1.430493  [   64/60000]\n",
      "loss: 1.161857  [ 6464/60000]\n",
      "loss: 1.157772  [12864/60000]\n",
      "loss: 1.288907  [19264/60000]\n",
      "loss: 1.371921  [25664/60000]\n",
      "loss: 1.421078  [32064/60000]\n",
      "loss: 1.352494  [38464/60000]\n",
      "loss: 1.321546  [44864/60000]\n",
      "loss: 1.524085  [51264/60000]\n",
      "loss: 1.237343  [57664/60000]\n",
      "loss: 1.290568  [   64/60000]\n",
      "loss: 1.344241  [ 6464/60000]\n",
      "loss: 1.220184  [12864/60000]\n",
      "loss: 1.343745  [19264/60000]\n",
      "loss: 1.474596  [25664/60000]\n",
      "loss: 1.361799  [32064/60000]\n",
      "loss: 1.228137  [38464/60000]\n",
      "loss: 1.255918  [44864/60000]\n",
      "loss: 1.257169  [51264/60000]\n",
      "loss: 1.221509  [57664/60000]\n",
      "loss: 1.328191  [   64/60000]\n",
      "loss: 1.366118  [ 6464/60000]\n",
      "loss: 1.166419  [12864/60000]\n",
      "loss: 1.288252  [19264/60000]\n",
      "loss: 1.496387  [25664/60000]\n",
      "loss: 1.355427  [32064/60000]\n",
      "loss: 1.273209  [38464/60000]\n",
      "loss: 1.283390  [44864/60000]\n",
      "loss: 1.346256  [51264/60000]\n",
      "loss: 1.207124  [57664/60000]\n",
      "loss: 1.490577  [   64/60000]\n",
      "loss: 1.169741  [ 6464/60000]\n",
      "loss: 1.176231  [12864/60000]\n",
      "loss: 1.304115  [19264/60000]\n",
      "loss: 1.362091  [25664/60000]\n",
      "loss: 1.401311  [32064/60000]\n",
      "loss: 1.379368  [38464/60000]\n",
      "loss: 1.349706  [44864/60000]\n",
      "loss: 1.390239  [51264/60000]\n",
      "loss: 1.259870  [57664/60000]\n",
      "loss: 1.256161  [   64/60000]\n",
      "loss: 1.185730  [ 6464/60000]\n",
      "loss: 1.244707  [12864/60000]\n",
      "loss: 1.230585  [19264/60000]\n",
      "loss: 1.372542  [25664/60000]\n",
      "loss: 1.157636  [32064/60000]\n",
      "loss: 1.301878  [38464/60000]\n",
      "loss: 1.288520  [44864/60000]\n",
      "loss: 1.292701  [51264/60000]\n",
      "loss: 1.251647  [57664/60000]\n",
      "loss: 1.254824  [   64/60000]\n",
      "loss: 1.221374  [ 6464/60000]\n",
      "loss: 1.148622  [12864/60000]\n",
      "loss: 1.410002  [19264/60000]\n",
      "loss: 1.403867  [25664/60000]\n",
      "loss: 1.252753  [32064/60000]\n",
      "loss: 1.209537  [38464/60000]\n",
      "loss: 1.235566  [44864/60000]\n",
      "loss: 1.191767  [51264/60000]\n",
      "loss: 1.106853  [57664/60000]\n",
      "loss: 1.316867  [   64/60000]\n",
      "loss: 1.159586  [ 6464/60000]\n",
      "loss: 1.058102  [12864/60000]\n",
      "loss: 1.363745  [19264/60000]\n",
      "loss: 1.233085  [25664/60000]\n",
      "loss: 1.344383  [32064/60000]\n",
      "loss: 1.272573  [38464/60000]\n",
      "loss: 1.203215  [44864/60000]\n",
      "loss: 1.384073  [51264/60000]\n",
      "loss: 1.063842  [57664/60000]\n",
      "loss: 1.358817  [   64/60000]\n",
      "loss: 1.232837  [ 6464/60000]\n",
      "loss: 1.074676  [12864/60000]\n",
      "loss: 1.222405  [19264/60000]\n",
      "loss: 1.348558  [25664/60000]\n",
      "loss: 1.331153  [32064/60000]\n",
      "loss: 1.207096  [38464/60000]\n",
      "loss: 1.259356  [44864/60000]\n",
      "loss: 1.233691  [51264/60000]\n",
      "loss: 1.099300  [57664/60000]\n",
      "loss: 1.219475  [   64/60000]\n",
      "loss: 1.180691  [ 6464/60000]\n",
      "loss: 1.040093  [12864/60000]\n",
      "loss: 1.181282  [19264/60000]\n",
      "loss: 1.328388  [25664/60000]\n",
      "loss: 1.262460  [32064/60000]\n",
      "loss: 1.184958  [38464/60000]\n",
      "loss: 1.209905  [44864/60000]\n",
      "loss: 1.322109  [51264/60000]\n",
      "loss: 1.150476  [57664/60000]\n",
      "loss: 1.176456  [   64/60000]\n",
      "loss: 1.239044  [ 6464/60000]\n",
      "loss: 1.000978  [12864/60000]\n",
      "loss: 1.299324  [19264/60000]\n",
      "loss: 1.379795  [25664/60000]\n",
      "loss: 1.242777  [32064/60000]\n",
      "loss: 1.128490  [38464/60000]\n",
      "loss: 1.162549  [44864/60000]\n",
      "loss: 1.238188  [51264/60000]\n",
      "loss: 1.184397  [57664/60000]\n",
      "loss: 1.289986  [   64/60000]\n",
      "loss: 1.185083  [ 6464/60000]\n",
      "loss: 1.160244  [12864/60000]\n",
      "loss: 1.371183  [19264/60000]\n",
      "loss: 1.269461  [25664/60000]\n",
      "loss: 1.268080  [32064/60000]\n",
      "loss: 1.220143  [38464/60000]\n",
      "loss: 1.104987  [44864/60000]\n",
      "loss: 1.292669  [51264/60000]\n",
      "loss: 1.117513  [57664/60000]\n",
      "loss: 1.184401  [   64/60000]\n",
      "loss: 1.226413  [ 6464/60000]\n",
      "loss: 0.979407  [12864/60000]\n",
      "loss: 1.154119  [19264/60000]\n",
      "loss: 1.361793  [25664/60000]\n",
      "loss: 1.268683  [32064/60000]\n",
      "loss: 1.093849  [38464/60000]\n",
      "loss: 1.198507  [44864/60000]\n",
      "loss: 1.080153  [51264/60000]\n",
      "loss: 1.066888  [57664/60000]\n",
      "loss: 1.217581  [   64/60000]\n",
      "loss: 1.106852  [ 6464/60000]\n",
      "loss: 0.993890  [12864/60000]\n",
      "loss: 1.194441  [19264/60000]\n",
      "loss: 1.281845  [25664/60000]\n",
      "loss: 1.167707  [32064/60000]\n",
      "loss: 1.173519  [38464/60000]\n",
      "loss: 1.184103  [44864/60000]\n",
      "loss: 1.199363  [51264/60000]\n",
      "loss: 1.008959  [57664/60000]\n",
      "loss: 1.160523  [   64/60000]\n",
      "loss: 1.182720  [ 6464/60000]\n",
      "loss: 1.013529  [12864/60000]\n",
      "loss: 1.222402  [19264/60000]\n",
      "loss: 1.210674  [25664/60000]\n",
      "loss: 1.149827  [32064/60000]\n",
      "loss: 1.048211  [38464/60000]\n",
      "loss: 1.081436  [44864/60000]\n",
      "loss: 1.221573  [51264/60000]\n",
      "loss: 1.105154  [57664/60000]\n",
      "loss: 1.065456  [   64/60000]\n",
      "loss: 1.128765  [ 6464/60000]\n",
      "loss: 0.956543  [12864/60000]\n",
      "loss: 1.266498  [19264/60000]\n",
      "loss: 1.183675  [25664/60000]\n",
      "loss: 1.092950  [32064/60000]\n",
      "loss: 1.102014  [38464/60000]\n",
      "loss: 1.131670  [44864/60000]\n",
      "loss: 1.211041  [51264/60000]\n",
      "loss: 1.022949  [57664/60000]\n",
      "loss: 1.157479  [   64/60000]\n",
      "loss: 1.041338  [ 6464/60000]\n",
      "loss: 0.989284  [12864/60000]\n",
      "loss: 1.211326  [19264/60000]\n",
      "loss: 1.229130  [25664/60000]\n",
      "loss: 1.146312  [32064/60000]\n",
      "loss: 1.050285  [38464/60000]\n",
      "loss: 1.139913  [44864/60000]\n",
      "loss: 1.297881  [51264/60000]\n",
      "loss: 1.109848  [57664/60000]\n",
      "loss: 1.247064  [   64/60000]\n",
      "loss: 1.024845  [ 6464/60000]\n",
      "loss: 1.031800  [12864/60000]\n",
      "loss: 1.165650  [19264/60000]\n",
      "loss: 1.135407  [25664/60000]\n",
      "loss: 1.100005  [32064/60000]\n",
      "loss: 1.082289  [38464/60000]\n",
      "loss: 1.072786  [44864/60000]\n",
      "loss: 1.078440  [51264/60000]\n",
      "loss: 1.039179  [57664/60000]\n",
      "loss: 1.013796  [   64/60000]\n",
      "loss: 1.045266  [ 6464/60000]\n",
      "loss: 1.033311  [12864/60000]\n",
      "loss: 1.137057  [19264/60000]\n",
      "loss: 1.151118  [25664/60000]\n",
      "loss: 1.183449  [32064/60000]\n",
      "loss: 1.100967  [38464/60000]\n",
      "loss: 1.158881  [44864/60000]\n",
      "loss: 1.132040  [51264/60000]\n",
      "loss: 0.948200  [57664/60000]\n",
      "loss: 1.051559  [   64/60000]\n",
      "loss: 1.056295  [ 6464/60000]\n",
      "loss: 0.978990  [12864/60000]\n",
      "loss: 1.088959  [19264/60000]\n",
      "loss: 1.165947  [25664/60000]\n",
      "loss: 1.035838  [32064/60000]\n",
      "loss: 1.197618  [38464/60000]\n",
      "loss: 1.013835  [44864/60000]\n",
      "loss: 1.286981  [51264/60000]\n",
      "loss: 1.028058  [57664/60000]\n",
      "loss: 1.023261  [   64/60000]\n",
      "loss: 0.982661  [ 6464/60000]\n",
      "loss: 0.951219  [12864/60000]\n",
      "loss: 1.107658  [19264/60000]\n",
      "loss: 1.131182  [25664/60000]\n",
      "loss: 1.074692  [32064/60000]\n",
      "loss: 1.063735  [38464/60000]\n",
      "loss: 1.085005  [44864/60000]\n",
      "loss: 1.082937  [51264/60000]\n",
      "loss: 1.033732  [57664/60000]\n",
      "loss: 1.025926  [   64/60000]\n",
      "loss: 1.075241  [ 6464/60000]\n",
      "loss: 0.940931  [12864/60000]\n",
      "loss: 1.080083  [19264/60000]\n",
      "loss: 1.180263  [25664/60000]\n",
      "loss: 1.000848  [32064/60000]\n",
      "loss: 1.049165  [38464/60000]\n",
      "loss: 1.108704  [44864/60000]\n",
      "loss: 1.169454  [51264/60000]\n",
      "loss: 0.979577  [57664/60000]\n",
      "loss: 0.986562  [   64/60000]\n",
      "loss: 1.069165  [ 6464/60000]\n",
      "loss: 0.895116  [12864/60000]\n",
      "loss: 1.229431  [19264/60000]\n",
      "loss: 1.210018  [25664/60000]\n",
      "loss: 1.016571  [32064/60000]\n",
      "loss: 1.148745  [38464/60000]\n",
      "loss: 0.993945  [44864/60000]\n",
      "loss: 1.104187  [51264/60000]\n",
      "loss: 0.960326  [57664/60000]\n",
      "loss: 1.020082  [   64/60000]\n",
      "loss: 1.102749  [ 6464/60000]\n",
      "loss: 0.950963  [12864/60000]\n",
      "loss: 1.169604  [19264/60000]\n",
      "loss: 1.089275  [25664/60000]\n",
      "loss: 1.039643  [32064/60000]\n",
      "loss: 0.969879  [38464/60000]\n",
      "loss: 1.081524  [44864/60000]\n",
      "loss: 1.129734  [51264/60000]\n",
      "loss: 0.977869  [57664/60000]\n",
      "loss: 1.093798  [   64/60000]\n",
      "loss: 0.964637  [ 6464/60000]\n",
      "loss: 0.861482  [12864/60000]\n",
      "loss: 1.158525  [19264/60000]\n",
      "loss: 1.074910  [25664/60000]\n",
      "loss: 1.043306  [32064/60000]\n",
      "loss: 0.997399  [38464/60000]\n",
      "loss: 1.112297  [44864/60000]\n",
      "loss: 1.214326  [51264/60000]\n",
      "loss: 0.932532  [57664/60000]\n",
      "loss: 1.036157  [   64/60000]\n",
      "loss: 0.999690  [ 6464/60000]\n",
      "loss: 0.887914  [12864/60000]\n",
      "loss: 1.202542  [19264/60000]\n",
      "loss: 1.253088  [25664/60000]\n",
      "loss: 0.946027  [32064/60000]\n",
      "loss: 1.081134  [38464/60000]\n",
      "loss: 1.063876  [44864/60000]\n",
      "loss: 1.077730  [51264/60000]\n",
      "loss: 0.959641  [57664/60000]\n",
      "loss: 0.999412  [   64/60000]\n",
      "loss: 1.129545  [ 6464/60000]\n",
      "loss: 0.903361  [12864/60000]\n",
      "loss: 1.031127  [19264/60000]\n",
      "loss: 1.262908  [25664/60000]\n",
      "loss: 0.967688  [32064/60000]\n",
      "loss: 0.921797  [38464/60000]\n",
      "loss: 1.034973  [44864/60000]\n",
      "loss: 1.160657  [51264/60000]\n",
      "loss: 0.977145  [57664/60000]\n",
      "loss: 0.998734  [   64/60000]\n",
      "loss: 0.997377  [ 6464/60000]\n",
      "loss: 0.862092  [12864/60000]\n",
      "loss: 1.093162  [19264/60000]\n",
      "loss: 1.175810  [25664/60000]\n",
      "loss: 1.080194  [32064/60000]\n",
      "loss: 0.858027  [38464/60000]\n",
      "loss: 1.010121  [44864/60000]\n",
      "loss: 1.086607  [51264/60000]\n",
      "loss: 0.915505  [57664/60000]\n",
      "loss: 0.952768  [   64/60000]\n",
      "loss: 1.013957  [ 6464/60000]\n",
      "loss: 0.884239  [12864/60000]\n",
      "loss: 0.991079  [19264/60000]\n",
      "loss: 1.218405  [25664/60000]\n",
      "loss: 0.967121  [32064/60000]\n",
      "loss: 0.958919  [38464/60000]\n",
      "loss: 1.031630  [44864/60000]\n",
      "loss: 1.073936  [51264/60000]\n",
      "loss: 0.933837  [57664/60000]\n",
      "loss: 0.866593  [   64/60000]\n",
      "loss: 0.964067  [ 6464/60000]\n",
      "loss: 0.926823  [12864/60000]\n",
      "loss: 1.077195  [19264/60000]\n",
      "loss: 1.087982  [25664/60000]\n",
      "loss: 1.028417  [32064/60000]\n",
      "loss: 1.007180  [38464/60000]\n",
      "loss: 1.020954  [44864/60000]\n",
      "loss: 1.051348  [51264/60000]\n",
      "loss: 0.973698  [57664/60000]\n",
      "loss: 0.966145  [   64/60000]\n",
      "loss: 0.900515  [ 6464/60000]\n",
      "loss: 0.798535  [12864/60000]\n",
      "loss: 1.141169  [19264/60000]\n",
      "loss: 1.093975  [25664/60000]\n",
      "loss: 0.952663  [32064/60000]\n",
      "loss: 0.923243  [38464/60000]\n",
      "loss: 0.964595  [44864/60000]\n",
      "loss: 1.056260  [51264/60000]\n",
      "loss: 0.971212  [57664/60000]\n",
      "loss: 0.944887  [   64/60000]\n",
      "loss: 1.029034  [ 6464/60000]\n",
      "loss: 0.808013  [12864/60000]\n",
      "loss: 1.137315  [19264/60000]\n",
      "loss: 0.972061  [25664/60000]\n",
      "loss: 0.910854  [32064/60000]\n",
      "loss: 1.028158  [38464/60000]\n",
      "loss: 0.846815  [44864/60000]\n",
      "loss: 1.062055  [51264/60000]\n",
      "loss: 0.917423  [57664/60000]\n",
      "loss: 0.937959  [   64/60000]\n",
      "loss: 0.961367  [ 6464/60000]\n",
      "loss: 0.809329  [12864/60000]\n",
      "loss: 1.136179  [19264/60000]\n",
      "loss: 1.044477  [25664/60000]\n",
      "loss: 0.947125  [32064/60000]\n",
      "loss: 1.008530  [38464/60000]\n",
      "loss: 0.948361  [44864/60000]\n",
      "loss: 1.106752  [51264/60000]\n",
      "loss: 0.947492  [57664/60000]\n",
      "loss: 0.964442  [   64/60000]\n",
      "loss: 0.880751  [ 6464/60000]\n",
      "loss: 0.803851  [12864/60000]\n",
      "loss: 0.962352  [19264/60000]\n",
      "loss: 0.983225  [25664/60000]\n",
      "loss: 0.974139  [32064/60000]\n",
      "loss: 0.886185  [38464/60000]\n",
      "loss: 0.935096  [44864/60000]\n",
      "loss: 1.056366  [51264/60000]\n",
      "loss: 0.945566  [57664/60000]\n",
      "loss: 0.846234  [   64/60000]\n",
      "loss: 0.909433  [ 6464/60000]\n",
      "loss: 0.876539  [12864/60000]\n",
      "loss: 1.059902  [19264/60000]\n",
      "loss: 1.165232  [25664/60000]\n",
      "loss: 1.128028  [32064/60000]\n",
      "loss: 0.938299  [38464/60000]\n",
      "loss: 0.960719  [44864/60000]\n",
      "loss: 1.098886  [51264/60000]\n",
      "loss: 0.945032  [57664/60000]\n",
      "loss: 0.993602  [   64/60000]\n",
      "loss: 0.932827  [ 6464/60000]\n",
      "loss: 0.874731  [12864/60000]\n",
      "loss: 1.080633  [19264/60000]\n",
      "loss: 1.018993  [25664/60000]\n",
      "loss: 0.939900  [32064/60000]\n",
      "loss: 0.973601  [38464/60000]\n",
      "loss: 0.926270  [44864/60000]\n",
      "loss: 1.165282  [51264/60000]\n",
      "loss: 0.912973  [57664/60000]\n",
      "loss: 0.865054  [   64/60000]\n",
      "loss: 0.955864  [ 6464/60000]\n",
      "loss: 0.858628  [12864/60000]\n",
      "loss: 1.067223  [19264/60000]\n",
      "loss: 1.071085  [25664/60000]\n",
      "loss: 0.895119  [32064/60000]\n",
      "loss: 0.899718  [38464/60000]\n",
      "loss: 0.966058  [44864/60000]\n",
      "loss: 1.028898  [51264/60000]\n",
      "loss: 0.902003  [57664/60000]\n",
      "loss: 0.895705  [   64/60000]\n",
      "loss: 0.904142  [ 6464/60000]\n",
      "loss: 0.796330  [12864/60000]\n",
      "loss: 1.029551  [19264/60000]\n",
      "loss: 0.988333  [25664/60000]\n",
      "loss: 0.930616  [32064/60000]\n",
      "loss: 0.866153  [38464/60000]\n",
      "loss: 1.165204  [44864/60000]\n",
      "loss: 1.035504  [51264/60000]\n",
      "loss: 0.869963  [57664/60000]\n",
      "loss: 0.859179  [   64/60000]\n",
      "loss: 0.954961  [ 6464/60000]\n",
      "loss: 0.728818  [12864/60000]\n",
      "loss: 0.961424  [19264/60000]\n",
      "loss: 0.934606  [25664/60000]\n",
      "loss: 0.907067  [32064/60000]\n",
      "loss: 0.897855  [38464/60000]\n",
      "loss: 0.996683  [44864/60000]\n",
      "loss: 1.141604  [51264/60000]\n",
      "loss: 0.951481  [57664/60000]\n",
      "loss: 0.942368  [   64/60000]\n",
      "loss: 0.886187  [ 6464/60000]\n",
      "loss: 0.870137  [12864/60000]\n",
      "loss: 0.965128  [19264/60000]\n",
      "loss: 1.052512  [25664/60000]\n",
      "loss: 0.969340  [32064/60000]\n",
      "loss: 0.848054  [38464/60000]\n",
      "loss: 0.985158  [44864/60000]\n",
      "loss: 1.001293  [51264/60000]\n",
      "loss: 0.869787  [57664/60000]\n",
      "loss: 0.845452  [   64/60000]\n",
      "loss: 0.845152  [ 6464/60000]\n",
      "loss: 0.755834  [12864/60000]\n",
      "loss: 1.002629  [19264/60000]\n",
      "loss: 1.098698  [25664/60000]\n",
      "loss: 0.989627  [32064/60000]\n",
      "loss: 0.817380  [38464/60000]\n",
      "loss: 0.916159  [44864/60000]\n",
      "loss: 0.895692  [51264/60000]\n",
      "loss: 0.918293  [57664/60000]\n",
      "loss: 0.855941  [   64/60000]\n",
      "loss: 1.006508  [ 6464/60000]\n",
      "loss: 0.781865  [12864/60000]\n",
      "loss: 0.989610  [19264/60000]\n",
      "loss: 1.129301  [25664/60000]\n",
      "loss: 0.885415  [32064/60000]\n",
      "loss: 0.813750  [38464/60000]\n",
      "loss: 0.854855  [44864/60000]\n",
      "loss: 1.027974  [51264/60000]\n",
      "loss: 0.950797  [57664/60000]\n",
      "loss: 0.831836  [   64/60000]\n",
      "loss: 0.943023  [ 6464/60000]\n",
      "loss: 0.735582  [12864/60000]\n",
      "loss: 1.041422  [19264/60000]\n",
      "loss: 1.032074  [25664/60000]\n",
      "loss: 0.933323  [32064/60000]\n",
      "loss: 0.854610  [38464/60000]\n",
      "loss: 0.885776  [44864/60000]\n",
      "loss: 1.129401  [51264/60000]\n",
      "loss: 0.912130  [57664/60000]\n",
      "loss: 0.875595  [   64/60000]\n",
      "loss: 0.926446  [ 6464/60000]\n",
      "loss: 0.662687  [12864/60000]\n",
      "loss: 1.063109  [19264/60000]\n",
      "loss: 1.049646  [25664/60000]\n",
      "loss: 0.876151  [32064/60000]\n",
      "loss: 0.813505  [38464/60000]\n",
      "loss: 0.873693  [44864/60000]\n",
      "loss: 0.925880  [51264/60000]\n",
      "loss: 0.953477  [57664/60000]\n",
      "loss: 0.816810  [   64/60000]\n",
      "loss: 0.807395  [ 6464/60000]\n",
      "loss: 0.742300  [12864/60000]\n",
      "loss: 0.955070  [19264/60000]\n",
      "loss: 0.987748  [25664/60000]\n",
      "loss: 0.806370  [32064/60000]\n",
      "loss: 0.823711  [38464/60000]\n",
      "loss: 0.953375  [44864/60000]\n",
      "loss: 0.942279  [51264/60000]\n",
      "loss: 0.816903  [57664/60000]\n",
      "loss: 0.901859  [   64/60000]\n",
      "loss: 0.790915  [ 6464/60000]\n",
      "loss: 0.770546  [12864/60000]\n",
      "loss: 0.995283  [19264/60000]\n",
      "loss: 0.868213  [25664/60000]\n",
      "loss: 0.853768  [32064/60000]\n",
      "loss: 0.885284  [38464/60000]\n",
      "loss: 0.913729  [44864/60000]\n",
      "loss: 0.966114  [51264/60000]\n",
      "loss: 0.856377  [57664/60000]\n",
      "loss: 0.831156  [   64/60000]\n",
      "loss: 0.837273  [ 6464/60000]\n",
      "loss: 0.801685  [12864/60000]\n",
      "loss: 0.963801  [19264/60000]\n",
      "loss: 0.846339  [25664/60000]\n",
      "loss: 0.928522  [32064/60000]\n",
      "loss: 0.730136  [38464/60000]\n",
      "loss: 0.971824  [44864/60000]\n",
      "loss: 0.955419  [51264/60000]\n",
      "loss: 0.902304  [57664/60000]\n",
      "loss: 0.781668  [   64/60000]\n",
      "loss: 0.850133  [ 6464/60000]\n",
      "loss: 0.718221  [12864/60000]\n",
      "loss: 0.959940  [19264/60000]\n",
      "loss: 1.048348  [25664/60000]\n",
      "loss: 0.894749  [32064/60000]\n",
      "loss: 0.830015  [38464/60000]\n",
      "loss: 0.918914  [44864/60000]\n",
      "loss: 0.910339  [51264/60000]\n",
      "loss: 0.868954  [57664/60000]\n",
      "loss: 0.756908  [   64/60000]\n",
      "loss: 0.866364  [ 6464/60000]\n",
      "loss: 0.710595  [12864/60000]\n",
      "loss: 1.002280  [19264/60000]\n",
      "loss: 0.972106  [25664/60000]\n",
      "loss: 0.791582  [32064/60000]\n",
      "loss: 0.800791  [38464/60000]\n",
      "loss: 0.921371  [44864/60000]\n",
      "loss: 1.014564  [51264/60000]\n",
      "loss: 0.864819  [57664/60000]\n",
      "loss: 0.825114  [   64/60000]\n",
      "loss: 0.767947  [ 6464/60000]\n",
      "loss: 0.733145  [12864/60000]\n",
      "loss: 0.898113  [19264/60000]\n",
      "loss: 0.942916  [25664/60000]\n",
      "loss: 0.853853  [32064/60000]\n",
      "loss: 0.822758  [38464/60000]\n",
      "loss: 0.777299  [44864/60000]\n",
      "loss: 1.093653  [51264/60000]\n",
      "loss: 0.734428  [57664/60000]\n",
      "loss: 0.842701  [   64/60000]\n",
      "loss: 0.723054  [ 6464/60000]\n",
      "loss: 0.656983  [12864/60000]\n",
      "loss: 1.049359  [19264/60000]\n",
      "loss: 1.032577  [25664/60000]\n",
      "loss: 0.913261  [32064/60000]\n",
      "loss: 0.792862  [38464/60000]\n",
      "loss: 0.829287  [44864/60000]\n",
      "loss: 0.933645  [51264/60000]\n",
      "loss: 0.910725  [57664/60000]\n",
      "loss: 0.791412  [   64/60000]\n",
      "loss: 0.817576  [ 6464/60000]\n",
      "loss: 0.713809  [12864/60000]\n",
      "loss: 0.975690  [19264/60000]\n",
      "loss: 0.885082  [25664/60000]\n",
      "loss: 0.879900  [32064/60000]\n",
      "loss: 0.809230  [38464/60000]\n",
      "loss: 0.948196  [44864/60000]\n",
      "loss: 1.024631  [51264/60000]\n",
      "loss: 0.765067  [57664/60000]\n",
      "loss: 0.719361  [   64/60000]\n",
      "loss: 0.804342  [ 6464/60000]\n",
      "loss: 0.640714  [12864/60000]\n",
      "loss: 1.063714  [19264/60000]\n",
      "loss: 0.874619  [25664/60000]\n",
      "loss: 0.819864  [32064/60000]\n",
      "loss: 0.917891  [38464/60000]\n",
      "loss: 0.810035  [44864/60000]\n",
      "loss: 0.858946  [51264/60000]\n",
      "loss: 0.937129  [57664/60000]\n",
      "loss: 0.846278  [   64/60000]\n",
      "loss: 0.845754  [ 6464/60000]\n",
      "loss: 0.635170  [12864/60000]\n",
      "loss: 0.931778  [19264/60000]\n",
      "loss: 0.899621  [25664/60000]\n",
      "loss: 0.942434  [32064/60000]\n",
      "loss: 0.871789  [38464/60000]\n",
      "loss: 0.850564  [44864/60000]\n",
      "loss: 0.911348  [51264/60000]\n",
      "loss: 0.862992  [57664/60000]\n",
      "loss: 0.783759  [   64/60000]\n",
      "loss: 0.800908  [ 6464/60000]\n",
      "loss: 0.653894  [12864/60000]\n",
      "loss: 1.041769  [19264/60000]\n",
      "loss: 1.074775  [25664/60000]\n",
      "loss: 0.903832  [32064/60000]\n",
      "loss: 0.867763  [38464/60000]\n",
      "loss: 0.910789  [44864/60000]\n",
      "loss: 0.943986  [51264/60000]\n",
      "loss: 0.876418  [57664/60000]\n",
      "loss: 0.826666  [   64/60000]\n",
      "loss: 0.765656  [ 6464/60000]\n",
      "loss: 0.695033  [12864/60000]\n",
      "loss: 0.890512  [19264/60000]\n",
      "loss: 0.956330  [25664/60000]\n",
      "loss: 0.832026  [32064/60000]\n",
      "loss: 0.815930  [38464/60000]\n",
      "loss: 0.985580  [44864/60000]\n",
      "loss: 1.042987  [51264/60000]\n",
      "loss: 0.848107  [57664/60000]\n",
      "loss: 0.746643  [   64/60000]\n",
      "loss: 0.763970  [ 6464/60000]\n",
      "loss: 0.631667  [12864/60000]\n",
      "loss: 0.931444  [19264/60000]\n",
      "loss: 0.958235  [25664/60000]\n",
      "loss: 0.859975  [32064/60000]\n",
      "loss: 0.776626  [38464/60000]\n",
      "loss: 0.921680  [44864/60000]\n",
      "loss: 0.871009  [51264/60000]\n",
      "loss: 0.818120  [57664/60000]\n",
      "loss: 0.797035  [   64/60000]\n",
      "loss: 0.758573  [ 6464/60000]\n",
      "loss: 0.659653  [12864/60000]\n",
      "loss: 0.924725  [19264/60000]\n",
      "loss: 0.948543  [25664/60000]\n",
      "loss: 0.860390  [32064/60000]\n",
      "loss: 0.831065  [38464/60000]\n",
      "loss: 0.827174  [44864/60000]\n",
      "loss: 0.985664  [51264/60000]\n",
      "loss: 0.924743  [57664/60000]\n",
      "loss: 0.710849  [   64/60000]\n",
      "loss: 0.706080  [ 6464/60000]\n",
      "loss: 0.635583  [12864/60000]\n",
      "loss: 0.953647  [19264/60000]\n",
      "loss: 0.873494  [25664/60000]\n",
      "loss: 0.793802  [32064/60000]\n",
      "loss: 0.840641  [38464/60000]\n",
      "loss: 0.792300  [44864/60000]\n",
      "loss: 0.956633  [51264/60000]\n",
      "loss: 0.813686  [57664/60000]\n",
      "loss: 0.829300  [   64/60000]\n",
      "loss: 0.789910  [ 6464/60000]\n",
      "loss: 0.606968  [12864/60000]\n",
      "loss: 0.875583  [19264/60000]\n",
      "loss: 1.020979  [25664/60000]\n",
      "loss: 0.726351  [32064/60000]\n",
      "loss: 0.742019  [38464/60000]\n",
      "loss: 0.729819  [44864/60000]\n",
      "loss: 0.824870  [51264/60000]\n",
      "loss: 0.827748  [57664/60000]\n",
      "loss: 0.710614  [   64/60000]\n",
      "loss: 0.791723  [ 6464/60000]\n",
      "loss: 0.604706  [12864/60000]\n",
      "loss: 0.979741  [19264/60000]\n",
      "loss: 0.828772  [25664/60000]\n",
      "loss: 0.883940  [32064/60000]\n",
      "loss: 0.783463  [38464/60000]\n",
      "loss: 0.774001  [44864/60000]\n",
      "loss: 0.859633  [51264/60000]\n",
      "loss: 0.864395  [57664/60000]\n",
      "loss: 0.759350  [   64/60000]\n",
      "loss: 0.772242  [ 6464/60000]\n",
      "loss: 0.645601  [12864/60000]\n",
      "loss: 1.047598  [19264/60000]\n",
      "loss: 1.173243  [25664/60000]\n",
      "loss: 0.779662  [32064/60000]\n",
      "loss: 0.880532  [38464/60000]\n",
      "loss: 0.875623  [44864/60000]\n",
      "loss: 0.803272  [51264/60000]\n",
      "loss: 0.908507  [57664/60000]\n",
      "loss: 0.767034  [   64/60000]\n",
      "loss: 0.769500  [ 6464/60000]\n",
      "loss: 0.656244  [12864/60000]\n",
      "loss: 0.884891  [19264/60000]\n",
      "loss: 0.857683  [25664/60000]\n",
      "loss: 0.885846  [32064/60000]\n",
      "loss: 0.773098  [38464/60000]\n",
      "loss: 0.862345  [44864/60000]\n",
      "loss: 0.893508  [51264/60000]\n",
      "loss: 0.890343  [57664/60000]\n",
      "loss: 0.754806  [   64/60000]\n",
      "loss: 0.665483  [ 6464/60000]\n",
      "loss: 0.643210  [12864/60000]\n",
      "loss: 0.871056  [19264/60000]\n",
      "loss: 0.890410  [25664/60000]\n",
      "loss: 0.883017  [32064/60000]\n",
      "loss: 0.737104  [38464/60000]\n",
      "loss: 0.770481  [44864/60000]\n",
      "loss: 0.935663  [51264/60000]\n",
      "loss: 0.818125  [57664/60000]\n",
      "loss: 0.761430  [   64/60000]\n",
      "loss: 0.803485  [ 6464/60000]\n",
      "loss: 0.606944  [12864/60000]\n",
      "loss: 0.991614  [19264/60000]\n",
      "loss: 0.963366  [25664/60000]\n",
      "loss: 0.791482  [32064/60000]\n",
      "loss: 0.820943  [38464/60000]\n",
      "loss: 0.910061  [44864/60000]\n",
      "loss: 0.987797  [51264/60000]\n",
      "loss: 0.828508  [57664/60000]\n",
      "loss: 0.767409  [   64/60000]\n",
      "loss: 0.776308  [ 6464/60000]\n",
      "loss: 0.627609  [12864/60000]\n",
      "loss: 0.972156  [19264/60000]\n",
      "loss: 0.962813  [25664/60000]\n",
      "loss: 0.828869  [32064/60000]\n",
      "loss: 0.858835  [38464/60000]\n",
      "loss: 0.846195  [44864/60000]\n",
      "loss: 0.867682  [51264/60000]\n",
      "loss: 0.805724  [57664/60000]\n",
      "loss: 0.761372  [   64/60000]\n",
      "loss: 0.662402  [ 6464/60000]\n",
      "loss: 0.556830  [12864/60000]\n",
      "loss: 0.984710  [19264/60000]\n",
      "loss: 0.889623  [25664/60000]\n",
      "loss: 0.828452  [32064/60000]\n",
      "loss: 0.978646  [38464/60000]\n",
      "loss: 0.732750  [44864/60000]\n",
      "loss: 0.832964  [51264/60000]\n",
      "loss: 0.795545  [57664/60000]\n",
      "loss: 0.625762  [   64/60000]\n",
      "loss: 0.836340  [ 6464/60000]\n",
      "loss: 0.716296  [12864/60000]\n",
      "loss: 0.853566  [19264/60000]\n",
      "loss: 0.860016  [25664/60000]\n",
      "loss: 0.853689  [32064/60000]\n",
      "loss: 0.727155  [38464/60000]\n",
      "loss: 0.856027  [44864/60000]\n",
      "loss: 0.921069  [51264/60000]\n",
      "loss: 0.823342  [57664/60000]\n",
      "loss: 0.737768  [   64/60000]\n",
      "loss: 0.656959  [ 6464/60000]\n",
      "loss: 0.549089  [12864/60000]\n",
      "loss: 0.857457  [19264/60000]\n",
      "loss: 0.881806  [25664/60000]\n",
      "loss: 0.697284  [32064/60000]\n",
      "loss: 0.807289  [38464/60000]\n",
      "loss: 0.770531  [44864/60000]\n",
      "loss: 0.953495  [51264/60000]\n",
      "loss: 0.808688  [57664/60000]\n",
      "loss: 0.829382  [   64/60000]\n",
      "loss: 0.677502  [ 6464/60000]\n",
      "loss: 0.665417  [12864/60000]\n",
      "loss: 0.881226  [19264/60000]\n",
      "loss: 0.887249  [25664/60000]\n",
      "loss: 0.869550  [32064/60000]\n",
      "loss: 0.772175  [38464/60000]\n",
      "loss: 0.750115  [44864/60000]\n",
      "loss: 0.990870  [51264/60000]\n",
      "loss: 0.786080  [57664/60000]\n",
      "loss: 0.705876  [   64/60000]\n",
      "loss: 0.744204  [ 6464/60000]\n",
      "loss: 0.678180  [12864/60000]\n",
      "loss: 0.926223  [19264/60000]\n",
      "loss: 1.005205  [25664/60000]\n",
      "loss: 0.789704  [32064/60000]\n",
      "loss: 0.735105  [38464/60000]\n",
      "loss: 0.786285  [44864/60000]\n",
      "loss: 0.896319  [51264/60000]\n",
      "loss: 0.812837  [57664/60000]\n",
      "loss: 0.708649  [   64/60000]\n",
      "loss: 0.682899  [ 6464/60000]\n",
      "loss: 0.701310  [12864/60000]\n",
      "loss: 0.982452  [19264/60000]\n",
      "loss: 0.901868  [25664/60000]\n",
      "loss: 0.693017  [32064/60000]\n",
      "loss: 0.865831  [38464/60000]\n",
      "loss: 0.819847  [44864/60000]\n",
      "loss: 0.924275  [51264/60000]\n",
      "loss: 0.778828  [57664/60000]\n",
      "loss: 0.658869  [   64/60000]\n",
      "loss: 0.733107  [ 6464/60000]\n",
      "loss: 0.633355  [12864/60000]\n",
      "loss: 0.949466  [19264/60000]\n",
      "loss: 0.953261  [25664/60000]\n",
      "loss: 0.730630  [32064/60000]\n",
      "loss: 0.868203  [38464/60000]\n",
      "loss: 0.871333  [44864/60000]\n",
      "loss: 0.820603  [51264/60000]\n",
      "loss: 0.727662  [57664/60000]\n",
      "loss: 0.697204  [   64/60000]\n",
      "loss: 0.767919  [ 6464/60000]\n",
      "loss: 0.640997  [12864/60000]\n",
      "loss: 0.888740  [19264/60000]\n",
      "loss: 0.971494  [25664/60000]\n",
      "loss: 0.712879  [32064/60000]\n",
      "loss: 0.760006  [38464/60000]\n",
      "loss: 0.737872  [44864/60000]\n",
      "loss: 1.026617  [51264/60000]\n",
      "loss: 0.780625  [57664/60000]\n",
      "loss: 0.701607  [   64/60000]\n",
      "loss: 0.734073  [ 6464/60000]\n",
      "loss: 0.544606  [12864/60000]\n",
      "loss: 0.960489  [19264/60000]\n",
      "loss: 0.944507  [25664/60000]\n",
      "loss: 0.703336  [32064/60000]\n",
      "loss: 0.702784  [38464/60000]\n",
      "loss: 0.846559  [44864/60000]\n",
      "loss: 0.901713  [51264/60000]\n",
      "loss: 0.806107  [57664/60000]\n",
      "loss: 0.705179  [   64/60000]\n",
      "loss: 0.742947  [ 6464/60000]\n",
      "loss: 0.640546  [12864/60000]\n",
      "loss: 0.901766  [19264/60000]\n",
      "loss: 0.949919  [25664/60000]\n",
      "loss: 0.723633  [32064/60000]\n",
      "loss: 0.684208  [38464/60000]\n",
      "loss: 0.750973  [44864/60000]\n",
      "loss: 0.933425  [51264/60000]\n",
      "loss: 0.856446  [57664/60000]\n",
      "loss: 0.757112  [   64/60000]\n",
      "loss: 0.742094  [ 6464/60000]\n",
      "loss: 0.676259  [12864/60000]\n",
      "loss: 0.928482  [19264/60000]\n",
      "loss: 0.923967  [25664/60000]\n",
      "loss: 0.672797  [32064/60000]\n",
      "loss: 0.767387  [38464/60000]\n",
      "loss: 0.888760  [44864/60000]\n",
      "loss: 0.906352  [51264/60000]\n",
      "loss: 0.747793  [57664/60000]\n",
      "loss: 0.728228  [   64/60000]\n",
      "loss: 0.733560  [ 6464/60000]\n",
      "loss: 0.595352  [12864/60000]\n",
      "loss: 0.836445  [19264/60000]\n",
      "loss: 0.863291  [25664/60000]\n",
      "loss: 0.785033  [32064/60000]\n",
      "loss: 0.690053  [38464/60000]\n",
      "loss: 0.823835  [44864/60000]\n",
      "loss: 0.852784  [51264/60000]\n",
      "loss: 0.860835  [57664/60000]\n",
      "loss: 0.717911  [   64/60000]\n",
      "loss: 0.680190  [ 6464/60000]\n",
      "loss: 0.620117  [12864/60000]\n",
      "loss: 0.836027  [19264/60000]\n",
      "loss: 0.839798  [25664/60000]\n",
      "loss: 0.772752  [32064/60000]\n",
      "loss: 0.722473  [38464/60000]\n",
      "loss: 0.802365  [44864/60000]\n",
      "loss: 1.034970  [51264/60000]\n",
      "loss: 0.788956  [57664/60000]\n",
      "loss: 0.769665  [   64/60000]\n",
      "loss: 0.764973  [ 6464/60000]\n",
      "loss: 0.566211  [12864/60000]\n",
      "loss: 0.948819  [19264/60000]\n",
      "loss: 0.948564  [25664/60000]\n",
      "loss: 0.723355  [32064/60000]\n",
      "loss: 0.804307  [38464/60000]\n",
      "loss: 0.870496  [44864/60000]\n",
      "loss: 0.783190  [51264/60000]\n",
      "loss: 0.820539  [57664/60000]\n"
     ]
    }
   ],
   "source": [
    "model = model.to(\"cuda\")\n",
    "for i in range(100):\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "model = model.to(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 74.5%, Avg loss: 0.703184 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 74.5%, Avg loss: 0.703184 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 74.5%, Avg loss: 0.703184 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 74.5%, Avg loss: 0.703184 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 74.5%, Avg loss: 0.703184 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 74.5%, Avg loss: 0.703184 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 74.5%, Avg loss: 0.703184 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 74.5%, Avg loss: 0.703184 \n",
      "\n",
      "1.26 s ± 66.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "test(test_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arthur/miniconda3/lib/python3.11/site-packages/torch/ao/quantization/observer.py:220: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n",
      "/home/arthur/miniconda3/lib/python3.11/site-packages/torch/ao/quantization/observer.py:1272: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = model.to(\"cpu\")\n",
    "\n",
    "model.eval()\n",
    "backend = \"x86\"\n",
    "model.qconfig = torch.quantization.get_default_qconfig(backend)\n",
    "torch.backends.quantized.engine = backend\n",
    "\n",
    "model_static_quantized = torch.quantization.prepare(model, inplace=False)\n",
    "model_static_quantized = torch.quantization.convert(model_static_quantized, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 61.5%, Avg loss: 1.097411 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.5%, Avg loss: 1.097411 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.5%, Avg loss: 1.097411 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.5%, Avg loss: 1.097411 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.5%, Avg loss: 1.097411 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.5%, Avg loss: 1.097411 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.5%, Avg loss: 1.097411 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.5%, Avg loss: 1.097411 \n",
      "\n",
      "885 ms ± 400 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "test(test_dataloader, model_static_quantized, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_static_quantized, \"model_post_static_quantized.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
