{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.quant = torch.ao.quantization.QuantStub()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, 3, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(8, 8*8, 3, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(3, 2),\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256, 128), \n",
    "            nn.Dropout(),\n",
    "            nn.Linear(128, 10) \n",
    "            # nn.Softmax()\n",
    "        )\n",
    "        self.dequant = torch.ao.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        y = self.model(x)\n",
    "        return self.dequant(y)\n",
    "    \n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X = X.to(\"cuda\")\n",
    "        y = y.to(\"cuda\")\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier()\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.312921  [   64/60000]\n",
      "loss: 2.308734  [ 6464/60000]\n",
      "loss: 2.291307  [12864/60000]\n",
      "loss: 2.306224  [19264/60000]\n",
      "loss: 2.318503  [25664/60000]\n",
      "loss: 2.294582  [32064/60000]\n",
      "loss: 2.308161  [38464/60000]\n",
      "loss: 2.300468  [44864/60000]\n",
      "loss: 2.304066  [51264/60000]\n",
      "loss: 2.300486  [57664/60000]\n",
      "loss: 2.308843  [   64/60000]\n",
      "loss: 2.305158  [ 6464/60000]\n",
      "loss: 2.296551  [12864/60000]\n",
      "loss: 2.291258  [19264/60000]\n",
      "loss: 2.317136  [25664/60000]\n",
      "loss: 2.292890  [32064/60000]\n",
      "loss: 2.309354  [38464/60000]\n",
      "loss: 2.296918  [44864/60000]\n",
      "loss: 2.306238  [51264/60000]\n",
      "loss: 2.303803  [57664/60000]\n",
      "loss: 2.301648  [   64/60000]\n",
      "loss: 2.317602  [ 6464/60000]\n",
      "loss: 2.292736  [12864/60000]\n",
      "loss: 2.293813  [19264/60000]\n",
      "loss: 2.318864  [25664/60000]\n",
      "loss: 2.292829  [32064/60000]\n",
      "loss: 2.306476  [38464/60000]\n",
      "loss: 2.293043  [44864/60000]\n",
      "loss: 2.302885  [51264/60000]\n",
      "loss: 2.302706  [57664/60000]\n",
      "loss: 2.292832  [   64/60000]\n",
      "loss: 2.301071  [ 6464/60000]\n",
      "loss: 2.268039  [12864/60000]\n",
      "loss: 2.286169  [19264/60000]\n",
      "loss: 2.304909  [25664/60000]\n",
      "loss: 2.288447  [32064/60000]\n",
      "loss: 2.303582  [38464/60000]\n",
      "loss: 2.286101  [44864/60000]\n",
      "loss: 2.300036  [51264/60000]\n",
      "loss: 2.294414  [57664/60000]\n",
      "loss: 2.296107  [   64/60000]\n",
      "loss: 2.297330  [ 6464/60000]\n",
      "loss: 2.289643  [12864/60000]\n",
      "loss: 2.292664  [19264/60000]\n",
      "loss: 2.297990  [25664/60000]\n",
      "loss: 2.267011  [32064/60000]\n",
      "loss: 2.279857  [38464/60000]\n",
      "loss: 2.276857  [44864/60000]\n",
      "loss: 2.269997  [51264/60000]\n",
      "loss: 2.277199  [57664/60000]\n",
      "loss: 2.279084  [   64/60000]\n",
      "loss: 2.272791  [ 6464/60000]\n",
      "loss: 2.253654  [12864/60000]\n",
      "loss: 2.285878  [19264/60000]\n",
      "loss: 2.307194  [25664/60000]\n",
      "loss: 2.242058  [32064/60000]\n",
      "loss: 2.273781  [38464/60000]\n",
      "loss: 2.256217  [44864/60000]\n",
      "loss: 2.271438  [51264/60000]\n",
      "loss: 2.277139  [57664/60000]\n",
      "loss: 2.280063  [   64/60000]\n",
      "loss: 2.265786  [ 6464/60000]\n",
      "loss: 2.234866  [12864/60000]\n",
      "loss: 2.251656  [19264/60000]\n",
      "loss: 2.283431  [25664/60000]\n",
      "loss: 2.211523  [32064/60000]\n",
      "loss: 2.257598  [38464/60000]\n",
      "loss: 2.221008  [44864/60000]\n",
      "loss: 2.233522  [51264/60000]\n",
      "loss: 2.251351  [57664/60000]\n",
      "loss: 2.269629  [   64/60000]\n",
      "loss: 2.256890  [ 6464/60000]\n",
      "loss: 2.211119  [12864/60000]\n",
      "loss: 2.226612  [19264/60000]\n",
      "loss: 2.279797  [25664/60000]\n",
      "loss: 2.186297  [32064/60000]\n",
      "loss: 2.244660  [38464/60000]\n",
      "loss: 2.202158  [44864/60000]\n",
      "loss: 2.227636  [51264/60000]\n",
      "loss: 2.235903  [57664/60000]\n",
      "loss: 2.232796  [   64/60000]\n",
      "loss: 2.210035  [ 6464/60000]\n",
      "loss: 2.186054  [12864/60000]\n",
      "loss: 2.208093  [19264/60000]\n",
      "loss: 2.197329  [25664/60000]\n",
      "loss: 2.139293  [32064/60000]\n",
      "loss: 2.167768  [38464/60000]\n",
      "loss: 2.137323  [44864/60000]\n",
      "loss: 2.216400  [51264/60000]\n",
      "loss: 2.132718  [57664/60000]\n",
      "loss: 2.164251  [   64/60000]\n",
      "loss: 2.164805  [ 6464/60000]\n",
      "loss: 2.107811  [12864/60000]\n",
      "loss: 2.163918  [19264/60000]\n",
      "loss: 2.186981  [25664/60000]\n",
      "loss: 2.068077  [32064/60000]\n",
      "loss: 2.114187  [38464/60000]\n",
      "loss: 2.059737  [44864/60000]\n",
      "loss: 2.096920  [51264/60000]\n",
      "loss: 2.053162  [57664/60000]\n",
      "loss: 2.112774  [   64/60000]\n",
      "loss: 2.074657  [ 6464/60000]\n",
      "loss: 2.009784  [12864/60000]\n",
      "loss: 1.969081  [19264/60000]\n",
      "loss: 2.010086  [25664/60000]\n",
      "loss: 1.977007  [32064/60000]\n",
      "loss: 2.020134  [38464/60000]\n",
      "loss: 1.950287  [44864/60000]\n",
      "loss: 1.992990  [51264/60000]\n",
      "loss: 1.943955  [57664/60000]\n",
      "loss: 1.969518  [   64/60000]\n",
      "loss: 1.967035  [ 6464/60000]\n",
      "loss: 1.851253  [12864/60000]\n",
      "loss: 1.948528  [19264/60000]\n",
      "loss: 1.963511  [25664/60000]\n",
      "loss: 1.891668  [32064/60000]\n",
      "loss: 1.787750  [38464/60000]\n",
      "loss: 1.916288  [44864/60000]\n",
      "loss: 1.829319  [51264/60000]\n",
      "loss: 1.716758  [57664/60000]\n",
      "loss: 1.787437  [   64/60000]\n",
      "loss: 1.800298  [ 6464/60000]\n",
      "loss: 1.755283  [12864/60000]\n",
      "loss: 1.744888  [19264/60000]\n",
      "loss: 1.669833  [25664/60000]\n",
      "loss: 1.882409  [32064/60000]\n",
      "loss: 1.813563  [38464/60000]\n",
      "loss: 1.685983  [44864/60000]\n",
      "loss: 1.769525  [51264/60000]\n",
      "loss: 1.664201  [57664/60000]\n",
      "loss: 1.762492  [   64/60000]\n",
      "loss: 1.650184  [ 6464/60000]\n",
      "loss: 1.717986  [12864/60000]\n",
      "loss: 1.687568  [19264/60000]\n",
      "loss: 1.860222  [25664/60000]\n",
      "loss: 1.633865  [32064/60000]\n",
      "loss: 1.693512  [38464/60000]\n",
      "loss: 1.707066  [44864/60000]\n",
      "loss: 1.669929  [51264/60000]\n",
      "loss: 1.466213  [57664/60000]\n",
      "loss: 1.640498  [   64/60000]\n",
      "loss: 1.666880  [ 6464/60000]\n",
      "loss: 1.526013  [12864/60000]\n",
      "loss: 1.648989  [19264/60000]\n",
      "loss: 1.687912  [25664/60000]\n",
      "loss: 1.540198  [32064/60000]\n",
      "loss: 1.508953  [38464/60000]\n",
      "loss: 1.612782  [44864/60000]\n",
      "loss: 1.700530  [51264/60000]\n",
      "loss: 1.489869  [57664/60000]\n",
      "loss: 1.624983  [   64/60000]\n",
      "loss: 1.635337  [ 6464/60000]\n",
      "loss: 1.518176  [12864/60000]\n",
      "loss: 1.484416  [19264/60000]\n",
      "loss: 1.572463  [25664/60000]\n",
      "loss: 1.577197  [32064/60000]\n",
      "loss: 1.539215  [38464/60000]\n",
      "loss: 1.502458  [44864/60000]\n",
      "loss: 1.556287  [51264/60000]\n",
      "loss: 1.469168  [57664/60000]\n",
      "loss: 1.726377  [   64/60000]\n",
      "loss: 1.485011  [ 6464/60000]\n",
      "loss: 1.429638  [12864/60000]\n",
      "loss: 1.659731  [19264/60000]\n",
      "loss: 1.609317  [25664/60000]\n",
      "loss: 1.585301  [32064/60000]\n",
      "loss: 1.527598  [38464/60000]\n",
      "loss: 1.698459  [44864/60000]\n",
      "loss: 1.452062  [51264/60000]\n",
      "loss: 1.349504  [57664/60000]\n",
      "loss: 1.441356  [   64/60000]\n",
      "loss: 1.463281  [ 6464/60000]\n",
      "loss: 1.367041  [12864/60000]\n",
      "loss: 1.644946  [19264/60000]\n",
      "loss: 1.627835  [25664/60000]\n",
      "loss: 1.549781  [32064/60000]\n",
      "loss: 1.500197  [38464/60000]\n",
      "loss: 1.538002  [44864/60000]\n",
      "loss: 1.489719  [51264/60000]\n",
      "loss: 1.580108  [57664/60000]\n",
      "loss: 1.418939  [   64/60000]\n",
      "loss: 1.538886  [ 6464/60000]\n",
      "loss: 1.306961  [12864/60000]\n",
      "loss: 1.549410  [19264/60000]\n",
      "loss: 1.428643  [25664/60000]\n",
      "loss: 1.479542  [32064/60000]\n",
      "loss: 1.603974  [38464/60000]\n",
      "loss: 1.531603  [44864/60000]\n",
      "loss: 1.536061  [51264/60000]\n",
      "loss: 1.432719  [57664/60000]\n",
      "loss: 1.614945  [   64/60000]\n",
      "loss: 1.393483  [ 6464/60000]\n",
      "loss: 1.316833  [12864/60000]\n",
      "loss: 1.426582  [19264/60000]\n",
      "loss: 1.732921  [25664/60000]\n",
      "loss: 1.479203  [32064/60000]\n",
      "loss: 1.476053  [38464/60000]\n",
      "loss: 1.300293  [44864/60000]\n",
      "loss: 1.542417  [51264/60000]\n",
      "loss: 1.269754  [57664/60000]\n",
      "loss: 1.425156  [   64/60000]\n",
      "loss: 1.510413  [ 6464/60000]\n",
      "loss: 1.368436  [12864/60000]\n",
      "loss: 1.345450  [19264/60000]\n",
      "loss: 1.475363  [25664/60000]\n",
      "loss: 1.486188  [32064/60000]\n",
      "loss: 1.404781  [38464/60000]\n",
      "loss: 1.377202  [44864/60000]\n",
      "loss: 1.321173  [51264/60000]\n",
      "loss: 1.232477  [57664/60000]\n",
      "loss: 1.366706  [   64/60000]\n",
      "loss: 1.340739  [ 6464/60000]\n",
      "loss: 1.213527  [12864/60000]\n",
      "loss: 1.462356  [19264/60000]\n",
      "loss: 1.452931  [25664/60000]\n",
      "loss: 1.453120  [32064/60000]\n",
      "loss: 1.450738  [38464/60000]\n",
      "loss: 1.449277  [44864/60000]\n",
      "loss: 1.491988  [51264/60000]\n",
      "loss: 1.317212  [57664/60000]\n",
      "loss: 1.365538  [   64/60000]\n",
      "loss: 1.418011  [ 6464/60000]\n",
      "loss: 1.258969  [12864/60000]\n",
      "loss: 1.411777  [19264/60000]\n",
      "loss: 1.473673  [25664/60000]\n",
      "loss: 1.361523  [32064/60000]\n",
      "loss: 1.303965  [38464/60000]\n",
      "loss: 1.395027  [44864/60000]\n",
      "loss: 1.328770  [51264/60000]\n",
      "loss: 1.307621  [57664/60000]\n",
      "loss: 1.311589  [   64/60000]\n",
      "loss: 1.287872  [ 6464/60000]\n",
      "loss: 1.257159  [12864/60000]\n",
      "loss: 1.283175  [19264/60000]\n",
      "loss: 1.372333  [25664/60000]\n",
      "loss: 1.290235  [32064/60000]\n",
      "loss: 1.278388  [38464/60000]\n",
      "loss: 1.456841  [44864/60000]\n",
      "loss: 1.396221  [51264/60000]\n",
      "loss: 1.267567  [57664/60000]\n",
      "loss: 1.334600  [   64/60000]\n",
      "loss: 1.330573  [ 6464/60000]\n",
      "loss: 1.140158  [12864/60000]\n",
      "loss: 1.270867  [19264/60000]\n",
      "loss: 1.288283  [25664/60000]\n",
      "loss: 1.387206  [32064/60000]\n",
      "loss: 1.372831  [38464/60000]\n",
      "loss: 1.396353  [44864/60000]\n",
      "loss: 1.291449  [51264/60000]\n",
      "loss: 1.214540  [57664/60000]\n",
      "loss: 1.376722  [   64/60000]\n",
      "loss: 1.282579  [ 6464/60000]\n",
      "loss: 1.205552  [12864/60000]\n",
      "loss: 1.430857  [19264/60000]\n",
      "loss: 1.394438  [25664/60000]\n",
      "loss: 1.388046  [32064/60000]\n",
      "loss: 1.380636  [38464/60000]\n",
      "loss: 1.325283  [44864/60000]\n",
      "loss: 1.477643  [51264/60000]\n",
      "loss: 1.358754  [57664/60000]\n",
      "loss: 1.335374  [   64/60000]\n",
      "loss: 1.309916  [ 6464/60000]\n",
      "loss: 1.194413  [12864/60000]\n",
      "loss: 1.342827  [19264/60000]\n",
      "loss: 1.494333  [25664/60000]\n",
      "loss: 1.408876  [32064/60000]\n",
      "loss: 1.378415  [38464/60000]\n",
      "loss: 1.341778  [44864/60000]\n",
      "loss: 1.360757  [51264/60000]\n",
      "loss: 1.190521  [57664/60000]\n",
      "loss: 1.291869  [   64/60000]\n",
      "loss: 1.273496  [ 6464/60000]\n",
      "loss: 1.249959  [12864/60000]\n",
      "loss: 1.309916  [19264/60000]\n",
      "loss: 1.374169  [25664/60000]\n",
      "loss: 1.288437  [32064/60000]\n",
      "loss: 1.347291  [38464/60000]\n",
      "loss: 1.283593  [44864/60000]\n",
      "loss: 1.318057  [51264/60000]\n",
      "loss: 1.149376  [57664/60000]\n",
      "loss: 1.290678  [   64/60000]\n",
      "loss: 1.207269  [ 6464/60000]\n",
      "loss: 1.084817  [12864/60000]\n",
      "loss: 1.277747  [19264/60000]\n",
      "loss: 1.357246  [25664/60000]\n",
      "loss: 1.295333  [32064/60000]\n",
      "loss: 1.231952  [38464/60000]\n",
      "loss: 1.316481  [44864/60000]\n",
      "loss: 1.417690  [51264/60000]\n",
      "loss: 1.169933  [57664/60000]\n",
      "loss: 1.234157  [   64/60000]\n",
      "loss: 1.202312  [ 6464/60000]\n",
      "loss: 1.248978  [12864/60000]\n",
      "loss: 1.366914  [19264/60000]\n",
      "loss: 1.309757  [25664/60000]\n",
      "loss: 1.245098  [32064/60000]\n",
      "loss: 1.204821  [38464/60000]\n",
      "loss: 1.240462  [44864/60000]\n",
      "loss: 1.195201  [51264/60000]\n",
      "loss: 1.153659  [57664/60000]\n",
      "loss: 1.220387  [   64/60000]\n",
      "loss: 1.246419  [ 6464/60000]\n",
      "loss: 1.097597  [12864/60000]\n",
      "loss: 1.302323  [19264/60000]\n",
      "loss: 1.378469  [25664/60000]\n",
      "loss: 1.195968  [32064/60000]\n",
      "loss: 1.150984  [38464/60000]\n",
      "loss: 1.205178  [44864/60000]\n",
      "loss: 1.242418  [51264/60000]\n",
      "loss: 1.140155  [57664/60000]\n",
      "loss: 1.258602  [   64/60000]\n",
      "loss: 1.208108  [ 6464/60000]\n",
      "loss: 1.038334  [12864/60000]\n",
      "loss: 1.241661  [19264/60000]\n",
      "loss: 1.400261  [25664/60000]\n",
      "loss: 1.305108  [32064/60000]\n",
      "loss: 1.327952  [38464/60000]\n",
      "loss: 1.207632  [44864/60000]\n",
      "loss: 1.263095  [51264/60000]\n",
      "loss: 1.142408  [57664/60000]\n",
      "loss: 1.361485  [   64/60000]\n",
      "loss: 1.211236  [ 6464/60000]\n",
      "loss: 1.069394  [12864/60000]\n",
      "loss: 1.249549  [19264/60000]\n",
      "loss: 1.492789  [25664/60000]\n",
      "loss: 1.292040  [32064/60000]\n",
      "loss: 1.188155  [38464/60000]\n",
      "loss: 1.269185  [44864/60000]\n",
      "loss: 1.199746  [51264/60000]\n",
      "loss: 1.178631  [57664/60000]\n",
      "loss: 1.165321  [   64/60000]\n",
      "loss: 1.156714  [ 6464/60000]\n",
      "loss: 1.076028  [12864/60000]\n",
      "loss: 1.285049  [19264/60000]\n",
      "loss: 1.221637  [25664/60000]\n",
      "loss: 1.218624  [32064/60000]\n",
      "loss: 1.227892  [38464/60000]\n",
      "loss: 1.187668  [44864/60000]\n",
      "loss: 1.193759  [51264/60000]\n",
      "loss: 1.151621  [57664/60000]\n",
      "loss: 1.227858  [   64/60000]\n",
      "loss: 1.217246  [ 6464/60000]\n",
      "loss: 1.016775  [12864/60000]\n",
      "loss: 1.348861  [19264/60000]\n",
      "loss: 1.218373  [25664/60000]\n",
      "loss: 1.237100  [32064/60000]\n",
      "loss: 1.194969  [38464/60000]\n",
      "loss: 1.276680  [44864/60000]\n",
      "loss: 1.161065  [51264/60000]\n",
      "loss: 1.225515  [57664/60000]\n",
      "loss: 1.218359  [   64/60000]\n",
      "loss: 1.177725  [ 6464/60000]\n",
      "loss: 1.026686  [12864/60000]\n",
      "loss: 1.137867  [19264/60000]\n",
      "loss: 1.275751  [25664/60000]\n",
      "loss: 1.101803  [32064/60000]\n",
      "loss: 1.279040  [38464/60000]\n",
      "loss: 1.221903  [44864/60000]\n",
      "loss: 1.273400  [51264/60000]\n",
      "loss: 1.112582  [57664/60000]\n",
      "loss: 1.203679  [   64/60000]\n",
      "loss: 1.128087  [ 6464/60000]\n",
      "loss: 0.944816  [12864/60000]\n",
      "loss: 1.175935  [19264/60000]\n",
      "loss: 1.260683  [25664/60000]\n",
      "loss: 1.208847  [32064/60000]\n",
      "loss: 1.173896  [38464/60000]\n",
      "loss: 1.185184  [44864/60000]\n",
      "loss: 1.226378  [51264/60000]\n",
      "loss: 1.143096  [57664/60000]\n",
      "loss: 1.346574  [   64/60000]\n",
      "loss: 1.193831  [ 6464/60000]\n",
      "loss: 0.935486  [12864/60000]\n",
      "loss: 1.215861  [19264/60000]\n",
      "loss: 1.258068  [25664/60000]\n",
      "loss: 1.087497  [32064/60000]\n",
      "loss: 1.165209  [38464/60000]\n",
      "loss: 1.113436  [44864/60000]\n",
      "loss: 1.305603  [51264/60000]\n",
      "loss: 1.146091  [57664/60000]\n",
      "loss: 1.169504  [   64/60000]\n",
      "loss: 1.272190  [ 6464/60000]\n",
      "loss: 0.964704  [12864/60000]\n",
      "loss: 1.186727  [19264/60000]\n",
      "loss: 1.136281  [25664/60000]\n",
      "loss: 1.173217  [32064/60000]\n",
      "loss: 1.178525  [38464/60000]\n",
      "loss: 1.188263  [44864/60000]\n",
      "loss: 1.179656  [51264/60000]\n",
      "loss: 1.099116  [57664/60000]\n",
      "loss: 1.136660  [   64/60000]\n",
      "loss: 1.239379  [ 6464/60000]\n",
      "loss: 1.062938  [12864/60000]\n",
      "loss: 1.193250  [19264/60000]\n",
      "loss: 1.209386  [25664/60000]\n",
      "loss: 1.020438  [32064/60000]\n",
      "loss: 1.145972  [38464/60000]\n",
      "loss: 1.164088  [44864/60000]\n",
      "loss: 1.181958  [51264/60000]\n",
      "loss: 1.191910  [57664/60000]\n",
      "loss: 1.010313  [   64/60000]\n",
      "loss: 1.097592  [ 6464/60000]\n",
      "loss: 0.960983  [12864/60000]\n",
      "loss: 1.340549  [19264/60000]\n",
      "loss: 1.182763  [25664/60000]\n",
      "loss: 1.175250  [32064/60000]\n",
      "loss: 1.227735  [38464/60000]\n",
      "loss: 1.178397  [44864/60000]\n",
      "loss: 1.243870  [51264/60000]\n",
      "loss: 1.064982  [57664/60000]\n",
      "loss: 1.080541  [   64/60000]\n",
      "loss: 1.043304  [ 6464/60000]\n",
      "loss: 0.978917  [12864/60000]\n",
      "loss: 1.118212  [19264/60000]\n",
      "loss: 1.079930  [25664/60000]\n",
      "loss: 1.112846  [32064/60000]\n",
      "loss: 0.989905  [38464/60000]\n",
      "loss: 1.069017  [44864/60000]\n",
      "loss: 0.983186  [51264/60000]\n",
      "loss: 1.035693  [57664/60000]\n",
      "loss: 1.104613  [   64/60000]\n",
      "loss: 1.145689  [ 6464/60000]\n",
      "loss: 0.915195  [12864/60000]\n",
      "loss: 1.201479  [19264/60000]\n",
      "loss: 1.277920  [25664/60000]\n",
      "loss: 1.062486  [32064/60000]\n",
      "loss: 1.094085  [38464/60000]\n",
      "loss: 1.076061  [44864/60000]\n",
      "loss: 1.107632  [51264/60000]\n",
      "loss: 1.021352  [57664/60000]\n",
      "loss: 1.040877  [   64/60000]\n",
      "loss: 0.964286  [ 6464/60000]\n",
      "loss: 1.025625  [12864/60000]\n",
      "loss: 1.262021  [19264/60000]\n",
      "loss: 1.180683  [25664/60000]\n",
      "loss: 1.099514  [32064/60000]\n",
      "loss: 1.125990  [38464/60000]\n",
      "loss: 1.054361  [44864/60000]\n",
      "loss: 1.182811  [51264/60000]\n",
      "loss: 1.019609  [57664/60000]\n",
      "loss: 1.026433  [   64/60000]\n",
      "loss: 1.075570  [ 6464/60000]\n",
      "loss: 0.855997  [12864/60000]\n",
      "loss: 1.056316  [19264/60000]\n",
      "loss: 1.223438  [25664/60000]\n",
      "loss: 1.055680  [32064/60000]\n",
      "loss: 1.124547  [38464/60000]\n",
      "loss: 1.124624  [44864/60000]\n",
      "loss: 1.076550  [51264/60000]\n",
      "loss: 1.023864  [57664/60000]\n",
      "loss: 0.988046  [   64/60000]\n",
      "loss: 0.969066  [ 6464/60000]\n",
      "loss: 0.861956  [12864/60000]\n",
      "loss: 1.184448  [19264/60000]\n",
      "loss: 1.307751  [25664/60000]\n",
      "loss: 1.112698  [32064/60000]\n",
      "loss: 1.085389  [38464/60000]\n",
      "loss: 1.262625  [44864/60000]\n",
      "loss: 1.180070  [51264/60000]\n",
      "loss: 1.016922  [57664/60000]\n",
      "loss: 1.018698  [   64/60000]\n",
      "loss: 1.089982  [ 6464/60000]\n",
      "loss: 0.929684  [12864/60000]\n",
      "loss: 1.200552  [19264/60000]\n",
      "loss: 1.059780  [25664/60000]\n",
      "loss: 1.071926  [32064/60000]\n",
      "loss: 0.959248  [38464/60000]\n",
      "loss: 0.978291  [44864/60000]\n",
      "loss: 1.046451  [51264/60000]\n",
      "loss: 1.052916  [57664/60000]\n",
      "loss: 1.032038  [   64/60000]\n",
      "loss: 1.027246  [ 6464/60000]\n",
      "loss: 0.959327  [12864/60000]\n",
      "loss: 1.232497  [19264/60000]\n",
      "loss: 1.257361  [25664/60000]\n",
      "loss: 1.013184  [32064/60000]\n",
      "loss: 1.048900  [38464/60000]\n",
      "loss: 1.031536  [44864/60000]\n",
      "loss: 1.157340  [51264/60000]\n",
      "loss: 1.036544  [57664/60000]\n",
      "loss: 0.989886  [   64/60000]\n",
      "loss: 1.032898  [ 6464/60000]\n",
      "loss: 0.928570  [12864/60000]\n",
      "loss: 1.169971  [19264/60000]\n",
      "loss: 1.097518  [25664/60000]\n",
      "loss: 1.135448  [32064/60000]\n",
      "loss: 0.995856  [38464/60000]\n",
      "loss: 1.023108  [44864/60000]\n",
      "loss: 1.060416  [51264/60000]\n",
      "loss: 1.023241  [57664/60000]\n",
      "loss: 0.994777  [   64/60000]\n",
      "loss: 1.126407  [ 6464/60000]\n",
      "loss: 0.886426  [12864/60000]\n",
      "loss: 1.185653  [19264/60000]\n",
      "loss: 1.241641  [25664/60000]\n",
      "loss: 0.955820  [32064/60000]\n",
      "loss: 0.913304  [38464/60000]\n",
      "loss: 1.036296  [44864/60000]\n",
      "loss: 1.164429  [51264/60000]\n",
      "loss: 0.934549  [57664/60000]\n",
      "loss: 1.004579  [   64/60000]\n",
      "loss: 1.047910  [ 6464/60000]\n",
      "loss: 0.899447  [12864/60000]\n",
      "loss: 1.114342  [19264/60000]\n",
      "loss: 1.158545  [25664/60000]\n",
      "loss: 1.005692  [32064/60000]\n",
      "loss: 1.006420  [38464/60000]\n",
      "loss: 1.035154  [44864/60000]\n",
      "loss: 1.013426  [51264/60000]\n",
      "loss: 1.035914  [57664/60000]\n",
      "loss: 0.990192  [   64/60000]\n",
      "loss: 0.960934  [ 6464/60000]\n",
      "loss: 0.961904  [12864/60000]\n",
      "loss: 1.037104  [19264/60000]\n",
      "loss: 1.088996  [25664/60000]\n",
      "loss: 1.089854  [32064/60000]\n",
      "loss: 0.885354  [38464/60000]\n",
      "loss: 1.031136  [44864/60000]\n",
      "loss: 1.131577  [51264/60000]\n",
      "loss: 0.947101  [57664/60000]\n",
      "loss: 1.049505  [   64/60000]\n",
      "loss: 0.926679  [ 6464/60000]\n",
      "loss: 0.951873  [12864/60000]\n",
      "loss: 1.196877  [19264/60000]\n",
      "loss: 1.127822  [25664/60000]\n",
      "loss: 1.078373  [32064/60000]\n",
      "loss: 0.983489  [38464/60000]\n",
      "loss: 1.078197  [44864/60000]\n",
      "loss: 1.135157  [51264/60000]\n",
      "loss: 0.955987  [57664/60000]\n",
      "loss: 0.934786  [   64/60000]\n",
      "loss: 0.933476  [ 6464/60000]\n",
      "loss: 0.840509  [12864/60000]\n",
      "loss: 1.105340  [19264/60000]\n",
      "loss: 1.184899  [25664/60000]\n",
      "loss: 0.974430  [32064/60000]\n",
      "loss: 0.879636  [38464/60000]\n",
      "loss: 0.980718  [44864/60000]\n",
      "loss: 1.099667  [51264/60000]\n",
      "loss: 0.973999  [57664/60000]\n",
      "loss: 0.998011  [   64/60000]\n",
      "loss: 0.992986  [ 6464/60000]\n",
      "loss: 0.792076  [12864/60000]\n",
      "loss: 0.953588  [19264/60000]\n",
      "loss: 1.145875  [25664/60000]\n",
      "loss: 1.041703  [32064/60000]\n",
      "loss: 0.992163  [38464/60000]\n",
      "loss: 1.065519  [44864/60000]\n",
      "loss: 0.967152  [51264/60000]\n",
      "loss: 0.990987  [57664/60000]\n",
      "loss: 1.014699  [   64/60000]\n",
      "loss: 0.930070  [ 6464/60000]\n",
      "loss: 0.834595  [12864/60000]\n",
      "loss: 1.104996  [19264/60000]\n",
      "loss: 1.198870  [25664/60000]\n",
      "loss: 1.042813  [32064/60000]\n",
      "loss: 0.960553  [38464/60000]\n",
      "loss: 0.958300  [44864/60000]\n",
      "loss: 0.976838  [51264/60000]\n",
      "loss: 1.029756  [57664/60000]\n",
      "loss: 0.938325  [   64/60000]\n",
      "loss: 0.984741  [ 6464/60000]\n",
      "loss: 0.839401  [12864/60000]\n",
      "loss: 0.992107  [19264/60000]\n",
      "loss: 1.166193  [25664/60000]\n",
      "loss: 0.952050  [32064/60000]\n",
      "loss: 1.027804  [38464/60000]\n",
      "loss: 0.948301  [44864/60000]\n",
      "loss: 1.076461  [51264/60000]\n",
      "loss: 0.892683  [57664/60000]\n",
      "loss: 0.906906  [   64/60000]\n",
      "loss: 1.036725  [ 6464/60000]\n",
      "loss: 0.730686  [12864/60000]\n",
      "loss: 1.009995  [19264/60000]\n",
      "loss: 1.160819  [25664/60000]\n",
      "loss: 1.005835  [32064/60000]\n",
      "loss: 0.922923  [38464/60000]\n",
      "loss: 0.917033  [44864/60000]\n",
      "loss: 1.085731  [51264/60000]\n",
      "loss: 0.946571  [57664/60000]\n",
      "loss: 0.919349  [   64/60000]\n",
      "loss: 0.967562  [ 6464/60000]\n",
      "loss: 0.889481  [12864/60000]\n",
      "loss: 1.055328  [19264/60000]\n",
      "loss: 1.238068  [25664/60000]\n",
      "loss: 0.937329  [32064/60000]\n",
      "loss: 1.052722  [38464/60000]\n",
      "loss: 0.957924  [44864/60000]\n",
      "loss: 1.058664  [51264/60000]\n",
      "loss: 1.100303  [57664/60000]\n",
      "loss: 0.925030  [   64/60000]\n",
      "loss: 0.944117  [ 6464/60000]\n",
      "loss: 0.845103  [12864/60000]\n",
      "loss: 1.007244  [19264/60000]\n",
      "loss: 1.046695  [25664/60000]\n",
      "loss: 0.934209  [32064/60000]\n",
      "loss: 1.059200  [38464/60000]\n",
      "loss: 0.944456  [44864/60000]\n",
      "loss: 1.121332  [51264/60000]\n",
      "loss: 0.952279  [57664/60000]\n",
      "loss: 0.935073  [   64/60000]\n",
      "loss: 0.877935  [ 6464/60000]\n",
      "loss: 0.825550  [12864/60000]\n",
      "loss: 1.005775  [19264/60000]\n",
      "loss: 0.908587  [25664/60000]\n",
      "loss: 0.929150  [32064/60000]\n",
      "loss: 0.846385  [38464/60000]\n",
      "loss: 0.918418  [44864/60000]\n",
      "loss: 1.068801  [51264/60000]\n",
      "loss: 0.811972  [57664/60000]\n",
      "loss: 0.980869  [   64/60000]\n",
      "loss: 0.882313  [ 6464/60000]\n",
      "loss: 0.790269  [12864/60000]\n",
      "loss: 1.129246  [19264/60000]\n",
      "loss: 1.019372  [25664/60000]\n",
      "loss: 0.900501  [32064/60000]\n",
      "loss: 0.961688  [38464/60000]\n",
      "loss: 1.015892  [44864/60000]\n",
      "loss: 1.033118  [51264/60000]\n",
      "loss: 0.927558  [57664/60000]\n",
      "loss: 0.885122  [   64/60000]\n",
      "loss: 0.877478  [ 6464/60000]\n",
      "loss: 0.823747  [12864/60000]\n",
      "loss: 1.076091  [19264/60000]\n",
      "loss: 1.231195  [25664/60000]\n",
      "loss: 0.930844  [32064/60000]\n",
      "loss: 0.891798  [38464/60000]\n",
      "loss: 1.106584  [44864/60000]\n",
      "loss: 0.976448  [51264/60000]\n",
      "loss: 0.845660  [57664/60000]\n",
      "loss: 0.904693  [   64/60000]\n",
      "loss: 0.936296  [ 6464/60000]\n",
      "loss: 0.800372  [12864/60000]\n",
      "loss: 1.017650  [19264/60000]\n",
      "loss: 1.056164  [25664/60000]\n",
      "loss: 0.957706  [32064/60000]\n",
      "loss: 0.935745  [38464/60000]\n",
      "loss: 0.958160  [44864/60000]\n",
      "loss: 1.006494  [51264/60000]\n",
      "loss: 0.969267  [57664/60000]\n",
      "loss: 0.925351  [   64/60000]\n",
      "loss: 0.909870  [ 6464/60000]\n",
      "loss: 0.819749  [12864/60000]\n",
      "loss: 1.039769  [19264/60000]\n",
      "loss: 0.930850  [25664/60000]\n",
      "loss: 0.949641  [32064/60000]\n",
      "loss: 0.846829  [38464/60000]\n",
      "loss: 1.020958  [44864/60000]\n",
      "loss: 0.992330  [51264/60000]\n",
      "loss: 0.892805  [57664/60000]\n",
      "loss: 0.954800  [   64/60000]\n",
      "loss: 0.945126  [ 6464/60000]\n",
      "loss: 0.815873  [12864/60000]\n",
      "loss: 1.101188  [19264/60000]\n",
      "loss: 1.030500  [25664/60000]\n",
      "loss: 0.981384  [32064/60000]\n",
      "loss: 0.861551  [38464/60000]\n",
      "loss: 0.908138  [44864/60000]\n",
      "loss: 1.068592  [51264/60000]\n",
      "loss: 0.951667  [57664/60000]\n",
      "loss: 0.919250  [   64/60000]\n",
      "loss: 0.875546  [ 6464/60000]\n",
      "loss: 0.780949  [12864/60000]\n",
      "loss: 0.982338  [19264/60000]\n",
      "loss: 0.951735  [25664/60000]\n",
      "loss: 0.807533  [32064/60000]\n",
      "loss: 0.974540  [38464/60000]\n",
      "loss: 0.877827  [44864/60000]\n",
      "loss: 0.858076  [51264/60000]\n",
      "loss: 0.836721  [57664/60000]\n",
      "loss: 0.804509  [   64/60000]\n",
      "loss: 0.836503  [ 6464/60000]\n",
      "loss: 0.743901  [12864/60000]\n",
      "loss: 0.970741  [19264/60000]\n",
      "loss: 0.966063  [25664/60000]\n",
      "loss: 0.858892  [32064/60000]\n",
      "loss: 0.857673  [38464/60000]\n",
      "loss: 0.882770  [44864/60000]\n",
      "loss: 0.982753  [51264/60000]\n",
      "loss: 0.925123  [57664/60000]\n",
      "loss: 0.796684  [   64/60000]\n",
      "loss: 0.813165  [ 6464/60000]\n",
      "loss: 0.737117  [12864/60000]\n",
      "loss: 1.064622  [19264/60000]\n",
      "loss: 1.047054  [25664/60000]\n",
      "loss: 0.891659  [32064/60000]\n",
      "loss: 0.968641  [38464/60000]\n",
      "loss: 0.913045  [44864/60000]\n",
      "loss: 1.034536  [51264/60000]\n",
      "loss: 0.963187  [57664/60000]\n",
      "loss: 0.872754  [   64/60000]\n",
      "loss: 0.751164  [ 6464/60000]\n",
      "loss: 0.813511  [12864/60000]\n",
      "loss: 0.984429  [19264/60000]\n",
      "loss: 0.945441  [25664/60000]\n",
      "loss: 0.900649  [32064/60000]\n",
      "loss: 0.839656  [38464/60000]\n",
      "loss: 0.785408  [44864/60000]\n",
      "loss: 0.983018  [51264/60000]\n",
      "loss: 0.915631  [57664/60000]\n",
      "loss: 0.894760  [   64/60000]\n",
      "loss: 0.822793  [ 6464/60000]\n",
      "loss: 0.687018  [12864/60000]\n",
      "loss: 0.994958  [19264/60000]\n",
      "loss: 1.021792  [25664/60000]\n",
      "loss: 0.881428  [32064/60000]\n",
      "loss: 0.921244  [38464/60000]\n",
      "loss: 0.819841  [44864/60000]\n",
      "loss: 0.860798  [51264/60000]\n",
      "loss: 0.795358  [57664/60000]\n",
      "loss: 0.847796  [   64/60000]\n",
      "loss: 0.833051  [ 6464/60000]\n",
      "loss: 0.780936  [12864/60000]\n",
      "loss: 1.001695  [19264/60000]\n",
      "loss: 1.042054  [25664/60000]\n",
      "loss: 1.041633  [32064/60000]\n",
      "loss: 0.848386  [38464/60000]\n",
      "loss: 0.871735  [44864/60000]\n",
      "loss: 1.057929  [51264/60000]\n",
      "loss: 0.977415  [57664/60000]\n",
      "loss: 0.741982  [   64/60000]\n",
      "loss: 0.831355  [ 6464/60000]\n",
      "loss: 0.819341  [12864/60000]\n",
      "loss: 1.040045  [19264/60000]\n",
      "loss: 1.018452  [25664/60000]\n",
      "loss: 0.790478  [32064/60000]\n",
      "loss: 0.820665  [38464/60000]\n",
      "loss: 0.820993  [44864/60000]\n",
      "loss: 0.862469  [51264/60000]\n",
      "loss: 0.843374  [57664/60000]\n",
      "loss: 0.833667  [   64/60000]\n",
      "loss: 0.898535  [ 6464/60000]\n",
      "loss: 0.756828  [12864/60000]\n",
      "loss: 0.879127  [19264/60000]\n",
      "loss: 1.057769  [25664/60000]\n",
      "loss: 0.932670  [32064/60000]\n",
      "loss: 0.863414  [38464/60000]\n",
      "loss: 0.895089  [44864/60000]\n",
      "loss: 1.036920  [51264/60000]\n",
      "loss: 0.860288  [57664/60000]\n",
      "loss: 0.803162  [   64/60000]\n",
      "loss: 0.869310  [ 6464/60000]\n",
      "loss: 0.719460  [12864/60000]\n",
      "loss: 1.148174  [19264/60000]\n",
      "loss: 1.119335  [25664/60000]\n",
      "loss: 0.802942  [32064/60000]\n",
      "loss: 0.772270  [38464/60000]\n",
      "loss: 0.869494  [44864/60000]\n",
      "loss: 0.870871  [51264/60000]\n",
      "loss: 0.856085  [57664/60000]\n",
      "loss: 0.841333  [   64/60000]\n",
      "loss: 0.849629  [ 6464/60000]\n",
      "loss: 0.791949  [12864/60000]\n",
      "loss: 0.905830  [19264/60000]\n",
      "loss: 1.050071  [25664/60000]\n",
      "loss: 0.834811  [32064/60000]\n",
      "loss: 0.836513  [38464/60000]\n",
      "loss: 0.805316  [44864/60000]\n",
      "loss: 0.953734  [51264/60000]\n",
      "loss: 0.911614  [57664/60000]\n",
      "loss: 0.882787  [   64/60000]\n",
      "loss: 0.875594  [ 6464/60000]\n",
      "loss: 0.754951  [12864/60000]\n",
      "loss: 0.970316  [19264/60000]\n",
      "loss: 1.116519  [25664/60000]\n",
      "loss: 0.829025  [32064/60000]\n",
      "loss: 0.876032  [38464/60000]\n",
      "loss: 0.858484  [44864/60000]\n",
      "loss: 0.899334  [51264/60000]\n",
      "loss: 0.785016  [57664/60000]\n",
      "loss: 0.926756  [   64/60000]\n",
      "loss: 0.860674  [ 6464/60000]\n",
      "loss: 0.710956  [12864/60000]\n",
      "loss: 0.999905  [19264/60000]\n",
      "loss: 1.008385  [25664/60000]\n",
      "loss: 0.837179  [32064/60000]\n",
      "loss: 0.862381  [38464/60000]\n",
      "loss: 0.918867  [44864/60000]\n",
      "loss: 0.906819  [51264/60000]\n",
      "loss: 0.894682  [57664/60000]\n",
      "loss: 0.829340  [   64/60000]\n",
      "loss: 0.859512  [ 6464/60000]\n",
      "loss: 0.682044  [12864/60000]\n",
      "loss: 0.977729  [19264/60000]\n",
      "loss: 1.012245  [25664/60000]\n",
      "loss: 0.842606  [32064/60000]\n",
      "loss: 0.779483  [38464/60000]\n",
      "loss: 0.924183  [44864/60000]\n",
      "loss: 0.841002  [51264/60000]\n",
      "loss: 0.860665  [57664/60000]\n",
      "loss: 0.898088  [   64/60000]\n",
      "loss: 0.719594  [ 6464/60000]\n",
      "loss: 0.752222  [12864/60000]\n",
      "loss: 1.025855  [19264/60000]\n",
      "loss: 0.975112  [25664/60000]\n",
      "loss: 0.742280  [32064/60000]\n",
      "loss: 0.804558  [38464/60000]\n",
      "loss: 0.825586  [44864/60000]\n",
      "loss: 0.925145  [51264/60000]\n",
      "loss: 0.897487  [57664/60000]\n",
      "loss: 0.836207  [   64/60000]\n",
      "loss: 0.787983  [ 6464/60000]\n",
      "loss: 0.658951  [12864/60000]\n",
      "loss: 1.011721  [19264/60000]\n",
      "loss: 0.919944  [25664/60000]\n",
      "loss: 0.851512  [32064/60000]\n",
      "loss: 0.869698  [38464/60000]\n",
      "loss: 0.875237  [44864/60000]\n",
      "loss: 0.964340  [51264/60000]\n",
      "loss: 0.975266  [57664/60000]\n",
      "loss: 0.777629  [   64/60000]\n",
      "loss: 0.737857  [ 6464/60000]\n",
      "loss: 0.744350  [12864/60000]\n",
      "loss: 1.010234  [19264/60000]\n",
      "loss: 1.028570  [25664/60000]\n",
      "loss: 0.886429  [32064/60000]\n",
      "loss: 0.812177  [38464/60000]\n",
      "loss: 0.786258  [44864/60000]\n",
      "loss: 0.961140  [51264/60000]\n",
      "loss: 0.880701  [57664/60000]\n",
      "loss: 0.773704  [   64/60000]\n",
      "loss: 0.904277  [ 6464/60000]\n",
      "loss: 0.654956  [12864/60000]\n",
      "loss: 0.981606  [19264/60000]\n",
      "loss: 0.898668  [25664/60000]\n",
      "loss: 0.803997  [32064/60000]\n",
      "loss: 0.738107  [38464/60000]\n",
      "loss: 0.833169  [44864/60000]\n",
      "loss: 1.035854  [51264/60000]\n",
      "loss: 0.814629  [57664/60000]\n",
      "loss: 0.809639  [   64/60000]\n",
      "loss: 0.837775  [ 6464/60000]\n",
      "loss: 0.680896  [12864/60000]\n",
      "loss: 0.960060  [19264/60000]\n",
      "loss: 1.022421  [25664/60000]\n",
      "loss: 0.814936  [32064/60000]\n",
      "loss: 0.854840  [38464/60000]\n",
      "loss: 0.878539  [44864/60000]\n",
      "loss: 0.902443  [51264/60000]\n",
      "loss: 0.788586  [57664/60000]\n",
      "loss: 0.751759  [   64/60000]\n",
      "loss: 0.875234  [ 6464/60000]\n",
      "loss: 0.700867  [12864/60000]\n",
      "loss: 0.861276  [19264/60000]\n",
      "loss: 1.012044  [25664/60000]\n",
      "loss: 0.817833  [32064/60000]\n",
      "loss: 0.760177  [38464/60000]\n",
      "loss: 0.817110  [44864/60000]\n",
      "loss: 0.862101  [51264/60000]\n",
      "loss: 0.921926  [57664/60000]\n",
      "loss: 0.765614  [   64/60000]\n",
      "loss: 0.849217  [ 6464/60000]\n",
      "loss: 0.744668  [12864/60000]\n",
      "loss: 0.924530  [19264/60000]\n",
      "loss: 1.012908  [25664/60000]\n",
      "loss: 0.729053  [32064/60000]\n",
      "loss: 0.790171  [38464/60000]\n",
      "loss: 0.839608  [44864/60000]\n",
      "loss: 0.940657  [51264/60000]\n",
      "loss: 0.788673  [57664/60000]\n",
      "loss: 0.767781  [   64/60000]\n",
      "loss: 0.854568  [ 6464/60000]\n",
      "loss: 0.772330  [12864/60000]\n",
      "loss: 0.996415  [19264/60000]\n",
      "loss: 0.892727  [25664/60000]\n",
      "loss: 0.789169  [32064/60000]\n",
      "loss: 0.812331  [38464/60000]\n",
      "loss: 0.794563  [44864/60000]\n",
      "loss: 0.968850  [51264/60000]\n",
      "loss: 0.841545  [57664/60000]\n",
      "loss: 0.751880  [   64/60000]\n",
      "loss: 0.762924  [ 6464/60000]\n",
      "loss: 0.709249  [12864/60000]\n",
      "loss: 0.968450  [19264/60000]\n",
      "loss: 0.837778  [25664/60000]\n",
      "loss: 0.851936  [32064/60000]\n",
      "loss: 0.921851  [38464/60000]\n",
      "loss: 0.790030  [44864/60000]\n",
      "loss: 0.809200  [51264/60000]\n",
      "loss: 0.800435  [57664/60000]\n",
      "loss: 0.825442  [   64/60000]\n",
      "loss: 0.800445  [ 6464/60000]\n",
      "loss: 0.677134  [12864/60000]\n",
      "loss: 1.010784  [19264/60000]\n",
      "loss: 0.970051  [25664/60000]\n",
      "loss: 0.904199  [32064/60000]\n",
      "loss: 0.818084  [38464/60000]\n",
      "loss: 0.822906  [44864/60000]\n",
      "loss: 0.934146  [51264/60000]\n",
      "loss: 0.777403  [57664/60000]\n",
      "loss: 0.730932  [   64/60000]\n",
      "loss: 0.754053  [ 6464/60000]\n",
      "loss: 0.711973  [12864/60000]\n",
      "loss: 0.931343  [19264/60000]\n",
      "loss: 0.987970  [25664/60000]\n",
      "loss: 0.759839  [32064/60000]\n",
      "loss: 0.841281  [38464/60000]\n",
      "loss: 0.695682  [44864/60000]\n",
      "loss: 0.962402  [51264/60000]\n",
      "loss: 0.818960  [57664/60000]\n",
      "loss: 0.761859  [   64/60000]\n",
      "loss: 0.768226  [ 6464/60000]\n",
      "loss: 0.585721  [12864/60000]\n",
      "loss: 0.897073  [19264/60000]\n",
      "loss: 0.990461  [25664/60000]\n",
      "loss: 0.714109  [32064/60000]\n",
      "loss: 0.800639  [38464/60000]\n",
      "loss: 0.887810  [44864/60000]\n",
      "loss: 0.894991  [51264/60000]\n",
      "loss: 0.746964  [57664/60000]\n",
      "loss: 0.680907  [   64/60000]\n",
      "loss: 0.734950  [ 6464/60000]\n",
      "loss: 0.615063  [12864/60000]\n",
      "loss: 0.945535  [19264/60000]\n",
      "loss: 0.977916  [25664/60000]\n",
      "loss: 0.889649  [32064/60000]\n",
      "loss: 0.748489  [38464/60000]\n",
      "loss: 0.799005  [44864/60000]\n",
      "loss: 1.009772  [51264/60000]\n",
      "loss: 0.780422  [57664/60000]\n",
      "loss: 0.749822  [   64/60000]\n",
      "loss: 0.795041  [ 6464/60000]\n",
      "loss: 0.585246  [12864/60000]\n",
      "loss: 0.914712  [19264/60000]\n",
      "loss: 0.950249  [25664/60000]\n",
      "loss: 0.889366  [32064/60000]\n",
      "loss: 0.850726  [38464/60000]\n",
      "loss: 0.798552  [44864/60000]\n",
      "loss: 0.834423  [51264/60000]\n",
      "loss: 0.766241  [57664/60000]\n",
      "loss: 0.810977  [   64/60000]\n",
      "loss: 0.805020  [ 6464/60000]\n",
      "loss: 0.697075  [12864/60000]\n",
      "loss: 0.844727  [19264/60000]\n",
      "loss: 0.972734  [25664/60000]\n",
      "loss: 0.717046  [32064/60000]\n",
      "loss: 0.797124  [38464/60000]\n",
      "loss: 0.784243  [44864/60000]\n",
      "loss: 0.969146  [51264/60000]\n",
      "loss: 0.913325  [57664/60000]\n",
      "loss: 0.681412  [   64/60000]\n",
      "loss: 0.799093  [ 6464/60000]\n",
      "loss: 0.657425  [12864/60000]\n",
      "loss: 0.895737  [19264/60000]\n",
      "loss: 0.879185  [25664/60000]\n",
      "loss: 0.855889  [32064/60000]\n",
      "loss: 0.789716  [38464/60000]\n",
      "loss: 0.786809  [44864/60000]\n",
      "loss: 0.945316  [51264/60000]\n",
      "loss: 0.825940  [57664/60000]\n",
      "loss: 0.766569  [   64/60000]\n",
      "loss: 0.835240  [ 6464/60000]\n",
      "loss: 0.637718  [12864/60000]\n",
      "loss: 0.976852  [19264/60000]\n",
      "loss: 0.956048  [25664/60000]\n",
      "loss: 0.792525  [32064/60000]\n",
      "loss: 0.806913  [38464/60000]\n",
      "loss: 0.840019  [44864/60000]\n",
      "loss: 0.832334  [51264/60000]\n",
      "loss: 0.840576  [57664/60000]\n",
      "loss: 0.735183  [   64/60000]\n",
      "loss: 0.710731  [ 6464/60000]\n",
      "loss: 0.708324  [12864/60000]\n",
      "loss: 0.958549  [19264/60000]\n",
      "loss: 0.964432  [25664/60000]\n",
      "loss: 0.749008  [32064/60000]\n",
      "loss: 0.766640  [38464/60000]\n",
      "loss: 0.815198  [44864/60000]\n",
      "loss: 0.978821  [51264/60000]\n",
      "loss: 0.786162  [57664/60000]\n",
      "loss: 0.711370  [   64/60000]\n",
      "loss: 0.829795  [ 6464/60000]\n",
      "loss: 0.637110  [12864/60000]\n",
      "loss: 0.875512  [19264/60000]\n",
      "loss: 0.912170  [25664/60000]\n",
      "loss: 0.742654  [32064/60000]\n",
      "loss: 0.772074  [38464/60000]\n",
      "loss: 0.833834  [44864/60000]\n",
      "loss: 0.895492  [51264/60000]\n",
      "loss: 0.740867  [57664/60000]\n",
      "loss: 0.764915  [   64/60000]\n",
      "loss: 0.690440  [ 6464/60000]\n",
      "loss: 0.655699  [12864/60000]\n",
      "loss: 0.861318  [19264/60000]\n",
      "loss: 0.915739  [25664/60000]\n",
      "loss: 0.696502  [32064/60000]\n",
      "loss: 0.789310  [38464/60000]\n",
      "loss: 0.794799  [44864/60000]\n",
      "loss: 0.824479  [51264/60000]\n",
      "loss: 0.748048  [57664/60000]\n",
      "loss: 0.741398  [   64/60000]\n",
      "loss: 0.763443  [ 6464/60000]\n",
      "loss: 0.601112  [12864/60000]\n",
      "loss: 0.886890  [19264/60000]\n",
      "loss: 0.873532  [25664/60000]\n",
      "loss: 0.741849  [32064/60000]\n",
      "loss: 0.791196  [38464/60000]\n",
      "loss: 0.819640  [44864/60000]\n",
      "loss: 0.831800  [51264/60000]\n",
      "loss: 0.744564  [57664/60000]\n"
     ]
    }
   ],
   "source": [
    "model = model.to(\"cuda\")\n",
    "for i in range(100):\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "model = model.to(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 74.6%, Avg loss: 0.711752 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 74.6%, Avg loss: 0.711752 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 74.6%, Avg loss: 0.711752 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 74.6%, Avg loss: 0.711752 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 74.6%, Avg loss: 0.711752 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 74.6%, Avg loss: 0.711752 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 74.6%, Avg loss: 0.711752 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 74.6%, Avg loss: 0.711752 \n",
      "\n",
      "731 ms ± 256 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "test(test_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arthur/miniconda3/lib/python3.11/site-packages/torch/ao/quantization/observer.py:220: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n",
      "/home/arthur/miniconda3/lib/python3.11/site-packages/torch/ao/quantization/observer.py:1272: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = model.to(\"cpu\")\n",
    "\n",
    "model.eval()\n",
    "backend = \"x86\"\n",
    "model.qconfig = torch.quantization.get_default_qconfig(backend)\n",
    "torch.backends.quantized.engine = backend\n",
    "\n",
    "model_static_quantized = torch.quantization.prepare(model, inplace=False)\n",
    "model_static_quantized = torch.quantization.convert(model_static_quantized, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 60.4%, Avg loss: 1.080245 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.4%, Avg loss: 1.080245 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.4%, Avg loss: 1.080245 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.4%, Avg loss: 1.080245 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.4%, Avg loss: 1.080245 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.4%, Avg loss: 1.080245 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.4%, Avg loss: 1.080245 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.4%, Avg loss: 1.080245 \n",
      "\n",
      "933 ms ± 505 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "test(test_dataloader, model_static_quantized, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_static_quantized, \"model_post_static_quantized.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
