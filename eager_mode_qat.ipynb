{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.quant = torch.ao.quantization.QuantStub()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, 3, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(8, 8*8, 3, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(3, 2),\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256, 128), \n",
    "            nn.Dropout(),\n",
    "            nn.Linear(128, 10) \n",
    "            # nn.Softmax()\n",
    "        )\n",
    "        self.dequant = torch.ao.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        y = self.model(x)\n",
    "        return self.dequant(y)\n",
    "    \n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X = X.to(\"cuda\")\n",
    "        y = y.to(\"cuda\")\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier()\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arthur/miniconda3/lib/python3.11/site-packages/torch/ao/quantization/observer.py:220: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n",
      "/home/arthur/miniconda3/lib/python3.11/site-packages/torch/ao/quantization/fake_quantize.py:353: UserWarning: _aminmax is deprecated as of PyTorch 1.11 and will be removed in a future release. Use aminmax instead. This warning will only appear once per process. (Triggered internally at ../aten/src/ATen/native/ReduceAllOps.cpp:71.)\n",
      "  return torch.fused_moving_avg_obs_fake_quant(\n",
      "/home/arthur/miniconda3/lib/python3.11/site-packages/torch/ao/quantization/fake_quantize.py:353: UserWarning: _aminmax is deprecated as of PyTorch 1.11 and will be removed in a future release. Use aminmax instead. This warning will only appear once per process. (Triggered internally at ../aten/src/ATen/native/TensorCompare.cpp:687.)\n",
      "  return torch.fused_moving_avg_obs_fake_quant(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.302333  [   64/60000]\n",
      "loss: 2.287039  [ 6464/60000]\n",
      "loss: 2.297430  [12864/60000]\n",
      "loss: 2.303822  [19264/60000]\n",
      "loss: 2.301533  [25664/60000]\n",
      "loss: 2.309207  [32064/60000]\n",
      "loss: 2.322660  [38464/60000]\n",
      "loss: 2.307912  [44864/60000]\n",
      "loss: 2.309490  [51264/60000]\n",
      "loss: 2.306941  [57664/60000]\n",
      "loss: 2.298245  [   64/60000]\n",
      "loss: 2.289940  [ 6464/60000]\n",
      "loss: 2.298312  [12864/60000]\n",
      "loss: 2.306079  [19264/60000]\n",
      "loss: 2.311275  [25664/60000]\n",
      "loss: 2.306083  [32064/60000]\n",
      "loss: 2.300441  [38464/60000]\n",
      "loss: 2.304611  [44864/60000]\n",
      "loss: 2.313630  [51264/60000]\n",
      "loss: 2.313148  [57664/60000]\n",
      "loss: 2.294288  [   64/60000]\n",
      "loss: 2.309016  [ 6464/60000]\n",
      "loss: 2.305173  [12864/60000]\n",
      "loss: 2.292884  [19264/60000]\n",
      "loss: 2.299614  [25664/60000]\n",
      "loss: 2.297619  [32064/60000]\n",
      "loss: 2.306399  [38464/60000]\n",
      "loss: 2.298535  [44864/60000]\n",
      "loss: 2.308318  [51264/60000]\n",
      "loss: 2.315697  [57664/60000]\n",
      "loss: 2.293626  [   64/60000]\n",
      "loss: 2.299409  [ 6464/60000]\n",
      "loss: 2.292240  [12864/60000]\n",
      "loss: 2.293131  [19264/60000]\n",
      "loss: 2.323771  [25664/60000]\n",
      "loss: 2.308831  [32064/60000]\n",
      "loss: 2.314269  [38464/60000]\n",
      "loss: 2.301728  [44864/60000]\n",
      "loss: 2.308009  [51264/60000]\n",
      "loss: 2.327555  [57664/60000]\n",
      "loss: 2.292558  [   64/60000]\n",
      "loss: 2.297150  [ 6464/60000]\n",
      "loss: 2.294569  [12864/60000]\n",
      "loss: 2.304825  [19264/60000]\n",
      "loss: 2.306157  [25664/60000]\n",
      "loss: 2.303505  [32064/60000]\n",
      "loss: 2.307568  [38464/60000]\n",
      "loss: 2.304077  [44864/60000]\n",
      "loss: 2.303712  [51264/60000]\n",
      "loss: 2.319821  [57664/60000]\n",
      "loss: 2.303149  [   64/60000]\n",
      "loss: 2.310013  [ 6464/60000]\n",
      "loss: 2.296317  [12864/60000]\n",
      "loss: 2.311605  [19264/60000]\n",
      "loss: 2.316233  [25664/60000]\n",
      "loss: 2.307071  [32064/60000]\n",
      "loss: 2.310414  [38464/60000]\n",
      "loss: 2.297837  [44864/60000]\n",
      "loss: 2.315208  [51264/60000]\n",
      "loss: 2.315385  [57664/60000]\n",
      "loss: 2.291412  [   64/60000]\n",
      "loss: 2.310344  [ 6464/60000]\n",
      "loss: 2.307462  [12864/60000]\n",
      "loss: 2.301541  [19264/60000]\n",
      "loss: 2.304283  [25664/60000]\n",
      "loss: 2.301971  [32064/60000]\n",
      "loss: 2.306851  [38464/60000]\n",
      "loss: 2.307487  [44864/60000]\n",
      "loss: 2.315915  [51264/60000]\n",
      "loss: 2.314311  [57664/60000]\n",
      "loss: 2.287154  [   64/60000]\n",
      "loss: 2.304069  [ 6464/60000]\n",
      "loss: 2.303871  [12864/60000]\n",
      "loss: 2.303426  [19264/60000]\n",
      "loss: 2.313545  [25664/60000]\n",
      "loss: 2.304068  [32064/60000]\n",
      "loss: 2.321697  [38464/60000]\n",
      "loss: 2.306132  [44864/60000]\n",
      "loss: 2.300990  [51264/60000]\n",
      "loss: 2.307298  [57664/60000]\n",
      "loss: 2.298272  [   64/60000]\n",
      "loss: 2.296638  [ 6464/60000]\n",
      "loss: 2.289890  [12864/60000]\n",
      "loss: 2.296766  [19264/60000]\n",
      "loss: 2.315446  [25664/60000]\n",
      "loss: 2.291941  [32064/60000]\n",
      "loss: 2.311819  [38464/60000]\n",
      "loss: 2.301066  [44864/60000]\n",
      "loss: 2.300628  [51264/60000]\n",
      "loss: 2.317654  [57664/60000]\n",
      "loss: 2.308649  [   64/60000]\n",
      "loss: 2.282719  [ 6464/60000]\n",
      "loss: 2.307910  [12864/60000]\n",
      "loss: 2.296034  [19264/60000]\n",
      "loss: 2.313196  [25664/60000]\n",
      "loss: 2.303035  [32064/60000]\n",
      "loss: 2.311746  [38464/60000]\n",
      "loss: 2.309324  [44864/60000]\n",
      "loss: 2.297853  [51264/60000]\n",
      "loss: 2.315270  [57664/60000]\n",
      "loss: 2.296906  [   64/60000]\n",
      "loss: 2.306026  [ 6464/60000]\n",
      "loss: 2.300438  [12864/60000]\n",
      "loss: 2.323620  [19264/60000]\n",
      "loss: 2.314707  [25664/60000]\n",
      "loss: 2.310902  [32064/60000]\n",
      "loss: 2.308328  [38464/60000]\n",
      "loss: 2.305796  [44864/60000]\n",
      "loss: 2.288789  [51264/60000]\n",
      "loss: 2.318619  [57664/60000]\n",
      "loss: 2.301197  [   64/60000]\n",
      "loss: 2.294184  [ 6464/60000]\n",
      "loss: 2.301917  [12864/60000]\n",
      "loss: 2.307758  [19264/60000]\n",
      "loss: 2.304157  [25664/60000]\n",
      "loss: 2.312621  [32064/60000]\n",
      "loss: 2.322814  [38464/60000]\n",
      "loss: 2.310328  [44864/60000]\n",
      "loss: 2.295907  [51264/60000]\n",
      "loss: 2.322134  [57664/60000]\n",
      "loss: 2.298422  [   64/60000]\n",
      "loss: 2.306944  [ 6464/60000]\n",
      "loss: 2.306032  [12864/60000]\n",
      "loss: 2.305156  [19264/60000]\n",
      "loss: 2.303006  [25664/60000]\n",
      "loss: 2.310247  [32064/60000]\n",
      "loss: 2.313296  [38464/60000]\n",
      "loss: 2.290978  [44864/60000]\n",
      "loss: 2.305924  [51264/60000]\n",
      "loss: 2.310132  [57664/60000]\n",
      "loss: 2.299583  [   64/60000]\n",
      "loss: 2.295842  [ 6464/60000]\n",
      "loss: 2.309054  [12864/60000]\n",
      "loss: 2.301097  [19264/60000]\n",
      "loss: 2.307374  [25664/60000]\n",
      "loss: 2.304311  [32064/60000]\n",
      "loss: 2.319396  [38464/60000]\n",
      "loss: 2.303300  [44864/60000]\n",
      "loss: 2.311762  [51264/60000]\n",
      "loss: 2.311340  [57664/60000]\n",
      "loss: 2.303691  [   64/60000]\n",
      "loss: 2.306007  [ 6464/60000]\n",
      "loss: 2.308172  [12864/60000]\n",
      "loss: 2.305386  [19264/60000]\n",
      "loss: 2.312862  [25664/60000]\n",
      "loss: 2.300970  [32064/60000]\n",
      "loss: 2.310785  [38464/60000]\n",
      "loss: 2.301263  [44864/60000]\n",
      "loss: 2.303415  [51264/60000]\n",
      "loss: 2.318636  [57664/60000]\n",
      "loss: 2.303932  [   64/60000]\n",
      "loss: 2.302155  [ 6464/60000]\n",
      "loss: 2.294876  [12864/60000]\n",
      "loss: 2.293913  [19264/60000]\n",
      "loss: 2.307925  [25664/60000]\n",
      "loss: 2.291799  [32064/60000]\n",
      "loss: 2.315799  [38464/60000]\n",
      "loss: 2.317977  [44864/60000]\n",
      "loss: 2.319466  [51264/60000]\n",
      "loss: 2.316697  [57664/60000]\n",
      "loss: 2.303122  [   64/60000]\n",
      "loss: 2.305191  [ 6464/60000]\n",
      "loss: 2.298377  [12864/60000]\n",
      "loss: 2.304927  [19264/60000]\n",
      "loss: 2.312513  [25664/60000]\n",
      "loss: 2.304347  [32064/60000]\n",
      "loss: 2.306223  [38464/60000]\n",
      "loss: 2.301409  [44864/60000]\n",
      "loss: 2.310670  [51264/60000]\n",
      "loss: 2.310742  [57664/60000]\n",
      "loss: 2.278785  [   64/60000]\n",
      "loss: 2.305699  [ 6464/60000]\n",
      "loss: 2.303079  [12864/60000]\n",
      "loss: 2.311522  [19264/60000]\n",
      "loss: 2.302162  [25664/60000]\n",
      "loss: 2.296927  [32064/60000]\n",
      "loss: 2.321169  [38464/60000]\n",
      "loss: 2.310396  [44864/60000]\n",
      "loss: 2.306503  [51264/60000]\n",
      "loss: 2.316722  [57664/60000]\n",
      "loss: 2.302470  [   64/60000]\n",
      "loss: 2.305704  [ 6464/60000]\n",
      "loss: 2.295789  [12864/60000]\n",
      "loss: 2.295582  [19264/60000]\n",
      "loss: 2.313355  [25664/60000]\n",
      "loss: 2.298684  [32064/60000]\n",
      "loss: 2.308031  [38464/60000]\n",
      "loss: 2.299361  [44864/60000]\n",
      "loss: 2.298885  [51264/60000]\n",
      "loss: 2.314222  [57664/60000]\n",
      "loss: 2.308610  [   64/60000]\n",
      "loss: 2.294239  [ 6464/60000]\n",
      "loss: 2.298769  [12864/60000]\n",
      "loss: 2.300032  [19264/60000]\n",
      "loss: 2.312088  [25664/60000]\n",
      "loss: 2.313936  [32064/60000]\n",
      "loss: 2.317991  [38464/60000]\n",
      "loss: 2.288627  [44864/60000]\n",
      "loss: 2.293896  [51264/60000]\n",
      "loss: 2.322620  [57664/60000]\n",
      "loss: 2.289049  [   64/60000]\n",
      "loss: 2.302815  [ 6464/60000]\n",
      "loss: 2.285614  [12864/60000]\n",
      "loss: 2.301297  [19264/60000]\n",
      "loss: 2.313315  [25664/60000]\n",
      "loss: 2.305133  [32064/60000]\n",
      "loss: 2.314915  [38464/60000]\n",
      "loss: 2.303131  [44864/60000]\n",
      "loss: 2.304469  [51264/60000]\n",
      "loss: 2.313459  [57664/60000]\n",
      "loss: 2.296886  [   64/60000]\n",
      "loss: 2.301932  [ 6464/60000]\n",
      "loss: 2.295997  [12864/60000]\n",
      "loss: 2.309311  [19264/60000]\n",
      "loss: 2.311126  [25664/60000]\n",
      "loss: 2.295708  [32064/60000]\n",
      "loss: 2.308815  [38464/60000]\n",
      "loss: 2.306421  [44864/60000]\n",
      "loss: 2.305991  [51264/60000]\n",
      "loss: 2.315584  [57664/60000]\n",
      "loss: 2.315648  [   64/60000]\n",
      "loss: 2.300183  [ 6464/60000]\n",
      "loss: 2.306321  [12864/60000]\n",
      "loss: 2.306887  [19264/60000]\n",
      "loss: 2.310674  [25664/60000]\n",
      "loss: 2.299502  [32064/60000]\n",
      "loss: 2.322761  [38464/60000]\n",
      "loss: 2.300507  [44864/60000]\n",
      "loss: 2.300799  [51264/60000]\n",
      "loss: 2.314393  [57664/60000]\n",
      "loss: 2.299578  [   64/60000]\n",
      "loss: 2.301458  [ 6464/60000]\n",
      "loss: 2.293592  [12864/60000]\n",
      "loss: 2.299553  [19264/60000]\n",
      "loss: 2.306671  [25664/60000]\n",
      "loss: 2.300490  [32064/60000]\n",
      "loss: 2.325745  [38464/60000]\n",
      "loss: 2.308689  [44864/60000]\n",
      "loss: 2.303785  [51264/60000]\n",
      "loss: 2.323968  [57664/60000]\n",
      "loss: 2.293738  [   64/60000]\n",
      "loss: 2.297751  [ 6464/60000]\n",
      "loss: 2.299332  [12864/60000]\n",
      "loss: 2.296678  [19264/60000]\n",
      "loss: 2.310023  [25664/60000]\n",
      "loss: 2.300228  [32064/60000]\n",
      "loss: 2.313794  [38464/60000]\n",
      "loss: 2.293795  [44864/60000]\n",
      "loss: 2.297551  [51264/60000]\n",
      "loss: 2.314571  [57664/60000]\n",
      "loss: 2.299886  [   64/60000]\n",
      "loss: 2.299294  [ 6464/60000]\n",
      "loss: 2.304591  [12864/60000]\n",
      "loss: 2.314923  [19264/60000]\n",
      "loss: 2.307319  [25664/60000]\n",
      "loss: 2.292660  [32064/60000]\n",
      "loss: 2.311922  [38464/60000]\n",
      "loss: 2.312196  [44864/60000]\n",
      "loss: 2.297968  [51264/60000]\n",
      "loss: 2.323819  [57664/60000]\n",
      "loss: 2.306518  [   64/60000]\n",
      "loss: 2.292401  [ 6464/60000]\n",
      "loss: 2.301143  [12864/60000]\n",
      "loss: 2.311695  [19264/60000]\n",
      "loss: 2.303711  [25664/60000]\n",
      "loss: 2.304822  [32064/60000]\n",
      "loss: 2.319621  [38464/60000]\n",
      "loss: 2.302171  [44864/60000]\n",
      "loss: 2.309938  [51264/60000]\n",
      "loss: 2.311591  [57664/60000]\n",
      "loss: 2.300733  [   64/60000]\n",
      "loss: 2.299456  [ 6464/60000]\n",
      "loss: 2.305078  [12864/60000]\n",
      "loss: 2.308938  [19264/60000]\n",
      "loss: 2.304923  [25664/60000]\n",
      "loss: 2.301734  [32064/60000]\n",
      "loss: 2.306724  [38464/60000]\n",
      "loss: 2.310181  [44864/60000]\n",
      "loss: 2.306663  [51264/60000]\n",
      "loss: 2.309876  [57664/60000]\n",
      "loss: 2.300443  [   64/60000]\n",
      "loss: 2.314289  [ 6464/60000]\n",
      "loss: 2.306769  [12864/60000]\n",
      "loss: 2.304259  [19264/60000]\n",
      "loss: 2.313360  [25664/60000]\n",
      "loss: 2.310529  [32064/60000]\n",
      "loss: 2.316175  [38464/60000]\n",
      "loss: 2.295945  [44864/60000]\n",
      "loss: 2.299625  [51264/60000]\n",
      "loss: 2.326369  [57664/60000]\n",
      "loss: 2.295850  [   64/60000]\n",
      "loss: 2.305480  [ 6464/60000]\n",
      "loss: 2.303952  [12864/60000]\n",
      "loss: 2.301599  [19264/60000]\n",
      "loss: 2.316289  [25664/60000]\n",
      "loss: 2.303094  [32064/60000]\n",
      "loss: 2.303756  [38464/60000]\n",
      "loss: 2.304474  [44864/60000]\n",
      "loss: 2.290915  [51264/60000]\n",
      "loss: 2.315642  [57664/60000]\n",
      "loss: 2.292281  [   64/60000]\n",
      "loss: 2.310186  [ 6464/60000]\n",
      "loss: 2.318701  [12864/60000]\n",
      "loss: 2.318152  [19264/60000]\n",
      "loss: 2.307299  [25664/60000]\n",
      "loss: 2.303257  [32064/60000]\n",
      "loss: 2.311620  [38464/60000]\n",
      "loss: 2.302232  [44864/60000]\n",
      "loss: 2.302595  [51264/60000]\n",
      "loss: 2.317205  [57664/60000]\n",
      "loss: 2.307281  [   64/60000]\n",
      "loss: 2.291712  [ 6464/60000]\n",
      "loss: 2.291793  [12864/60000]\n",
      "loss: 2.299862  [19264/60000]\n",
      "loss: 2.310748  [25664/60000]\n",
      "loss: 2.301016  [32064/60000]\n",
      "loss: 2.314964  [38464/60000]\n",
      "loss: 2.302907  [44864/60000]\n",
      "loss: 2.311233  [51264/60000]\n",
      "loss: 2.309398  [57664/60000]\n",
      "loss: 2.294136  [   64/60000]\n",
      "loss: 2.302192  [ 6464/60000]\n",
      "loss: 2.293914  [12864/60000]\n",
      "loss: 2.297593  [19264/60000]\n",
      "loss: 2.314364  [25664/60000]\n",
      "loss: 2.303447  [32064/60000]\n",
      "loss: 2.313005  [38464/60000]\n",
      "loss: 2.302817  [44864/60000]\n",
      "loss: 2.306892  [51264/60000]\n",
      "loss: 2.317407  [57664/60000]\n",
      "loss: 2.288441  [   64/60000]\n",
      "loss: 2.306077  [ 6464/60000]\n",
      "loss: 2.290369  [12864/60000]\n",
      "loss: 2.314281  [19264/60000]\n",
      "loss: 2.312408  [25664/60000]\n",
      "loss: 2.303970  [32064/60000]\n",
      "loss: 2.313315  [38464/60000]\n",
      "loss: 2.319206  [44864/60000]\n",
      "loss: 2.305320  [51264/60000]\n",
      "loss: 2.315675  [57664/60000]\n",
      "loss: 2.301827  [   64/60000]\n",
      "loss: 2.293719  [ 6464/60000]\n",
      "loss: 2.301501  [12864/60000]\n",
      "loss: 2.313791  [19264/60000]\n",
      "loss: 2.308601  [25664/60000]\n",
      "loss: 2.294031  [32064/60000]\n",
      "loss: 2.307791  [38464/60000]\n",
      "loss: 2.310977  [44864/60000]\n",
      "loss: 2.306656  [51264/60000]\n",
      "loss: 2.321861  [57664/60000]\n",
      "loss: 2.302082  [   64/60000]\n",
      "loss: 2.296957  [ 6464/60000]\n",
      "loss: 2.301954  [12864/60000]\n",
      "loss: 2.313647  [19264/60000]\n",
      "loss: 2.303967  [25664/60000]\n",
      "loss: 2.305505  [32064/60000]\n",
      "loss: 2.317556  [38464/60000]\n",
      "loss: 2.296752  [44864/60000]\n",
      "loss: 2.301307  [51264/60000]\n",
      "loss: 2.325756  [57664/60000]\n",
      "loss: 2.313903  [   64/60000]\n",
      "loss: 2.305166  [ 6464/60000]\n",
      "loss: 2.300338  [12864/60000]\n",
      "loss: 2.307923  [19264/60000]\n",
      "loss: 2.308121  [25664/60000]\n",
      "loss: 2.305603  [32064/60000]\n",
      "loss: 2.308871  [38464/60000]\n",
      "loss: 2.307197  [44864/60000]\n",
      "loss: 2.309149  [51264/60000]\n",
      "loss: 2.311229  [57664/60000]\n",
      "loss: 2.298541  [   64/60000]\n",
      "loss: 2.299045  [ 6464/60000]\n",
      "loss: 2.311669  [12864/60000]\n",
      "loss: 2.318370  [19264/60000]\n",
      "loss: 2.317991  [25664/60000]\n",
      "loss: 2.296432  [32064/60000]\n",
      "loss: 2.313009  [38464/60000]\n",
      "loss: 2.321797  [44864/60000]\n",
      "loss: 2.302261  [51264/60000]\n",
      "loss: 2.326046  [57664/60000]\n",
      "loss: 2.303414  [   64/60000]\n",
      "loss: 2.296857  [ 6464/60000]\n",
      "loss: 2.309204  [12864/60000]\n",
      "loss: 2.292207  [19264/60000]\n",
      "loss: 2.306689  [25664/60000]\n",
      "loss: 2.301873  [32064/60000]\n",
      "loss: 2.321324  [38464/60000]\n",
      "loss: 2.306861  [44864/60000]\n",
      "loss: 2.301591  [51264/60000]\n",
      "loss: 2.321822  [57664/60000]\n",
      "loss: 2.295938  [   64/60000]\n",
      "loss: 2.312895  [ 6464/60000]\n",
      "loss: 2.298684  [12864/60000]\n",
      "loss: 2.307570  [19264/60000]\n",
      "loss: 2.304859  [25664/60000]\n",
      "loss: 2.309621  [32064/60000]\n",
      "loss: 2.317286  [38464/60000]\n",
      "loss: 2.305717  [44864/60000]\n",
      "loss: 2.308741  [51264/60000]\n",
      "loss: 2.317703  [57664/60000]\n",
      "loss: 2.312801  [   64/60000]\n",
      "loss: 2.304461  [ 6464/60000]\n",
      "loss: 2.282409  [12864/60000]\n",
      "loss: 2.312091  [19264/60000]\n",
      "loss: 2.311499  [25664/60000]\n",
      "loss: 2.300820  [32064/60000]\n",
      "loss: 2.311249  [38464/60000]\n",
      "loss: 2.322079  [44864/60000]\n",
      "loss: 2.313950  [51264/60000]\n",
      "loss: 2.325563  [57664/60000]\n",
      "loss: 2.285515  [   64/60000]\n",
      "loss: 2.310457  [ 6464/60000]\n",
      "loss: 2.290715  [12864/60000]\n",
      "loss: 2.304044  [19264/60000]\n",
      "loss: 2.304656  [25664/60000]\n",
      "loss: 2.305127  [32064/60000]\n",
      "loss: 2.311739  [38464/60000]\n",
      "loss: 2.315315  [44864/60000]\n",
      "loss: 2.297901  [51264/60000]\n",
      "loss: 2.321596  [57664/60000]\n",
      "loss: 2.296506  [   64/60000]\n",
      "loss: 2.292226  [ 6464/60000]\n",
      "loss: 2.294472  [12864/60000]\n",
      "loss: 2.301555  [19264/60000]\n",
      "loss: 2.295253  [25664/60000]\n",
      "loss: 2.302473  [32064/60000]\n",
      "loss: 2.310660  [38464/60000]\n",
      "loss: 2.302239  [44864/60000]\n",
      "loss: 2.299736  [51264/60000]\n",
      "loss: 2.313286  [57664/60000]\n",
      "loss: 2.302145  [   64/60000]\n",
      "loss: 2.294738  [ 6464/60000]\n",
      "loss: 2.295509  [12864/60000]\n",
      "loss: 2.306876  [19264/60000]\n",
      "loss: 2.315928  [25664/60000]\n",
      "loss: 2.312262  [32064/60000]\n",
      "loss: 2.318971  [38464/60000]\n",
      "loss: 2.305495  [44864/60000]\n",
      "loss: 2.299116  [51264/60000]\n",
      "loss: 2.319384  [57664/60000]\n",
      "loss: 2.305806  [   64/60000]\n",
      "loss: 2.297749  [ 6464/60000]\n",
      "loss: 2.303366  [12864/60000]\n",
      "loss: 2.301142  [19264/60000]\n",
      "loss: 2.303163  [25664/60000]\n",
      "loss: 2.302387  [32064/60000]\n",
      "loss: 2.314814  [38464/60000]\n",
      "loss: 2.303196  [44864/60000]\n",
      "loss: 2.309680  [51264/60000]\n",
      "loss: 2.310013  [57664/60000]\n",
      "loss: 2.299433  [   64/60000]\n",
      "loss: 2.293340  [ 6464/60000]\n",
      "loss: 2.298230  [12864/60000]\n",
      "loss: 2.310293  [19264/60000]\n",
      "loss: 2.315513  [25664/60000]\n",
      "loss: 2.297737  [32064/60000]\n",
      "loss: 2.312822  [38464/60000]\n",
      "loss: 2.302490  [44864/60000]\n",
      "loss: 2.296939  [51264/60000]\n",
      "loss: 2.324274  [57664/60000]\n",
      "loss: 2.292597  [   64/60000]\n",
      "loss: 2.301720  [ 6464/60000]\n",
      "loss: 2.292556  [12864/60000]\n",
      "loss: 2.311718  [19264/60000]\n",
      "loss: 2.312696  [25664/60000]\n",
      "loss: 2.303393  [32064/60000]\n",
      "loss: 2.304731  [38464/60000]\n",
      "loss: 2.315793  [44864/60000]\n",
      "loss: 2.298515  [51264/60000]\n",
      "loss: 2.317922  [57664/60000]\n",
      "loss: 2.292665  [   64/60000]\n",
      "loss: 2.288825  [ 6464/60000]\n",
      "loss: 2.291133  [12864/60000]\n",
      "loss: 2.305742  [19264/60000]\n",
      "loss: 2.311645  [25664/60000]\n",
      "loss: 2.317517  [32064/60000]\n",
      "loss: 2.313840  [38464/60000]\n",
      "loss: 2.300433  [44864/60000]\n",
      "loss: 2.302798  [51264/60000]\n",
      "loss: 2.320652  [57664/60000]\n",
      "loss: 2.301985  [   64/60000]\n",
      "loss: 2.307287  [ 6464/60000]\n",
      "loss: 2.293462  [12864/60000]\n",
      "loss: 2.305001  [19264/60000]\n",
      "loss: 2.309368  [25664/60000]\n",
      "loss: 2.303009  [32064/60000]\n",
      "loss: 2.308633  [38464/60000]\n",
      "loss: 2.307420  [44864/60000]\n",
      "loss: 2.300473  [51264/60000]\n",
      "loss: 2.319152  [57664/60000]\n",
      "loss: 2.301973  [   64/60000]\n",
      "loss: 2.305623  [ 6464/60000]\n",
      "loss: 2.295464  [12864/60000]\n",
      "loss: 2.306246  [19264/60000]\n",
      "loss: 2.309631  [25664/60000]\n",
      "loss: 2.291499  [32064/60000]\n",
      "loss: 2.311440  [38464/60000]\n",
      "loss: 2.310940  [44864/60000]\n",
      "loss: 2.301221  [51264/60000]\n",
      "loss: 2.319108  [57664/60000]\n",
      "loss: 2.296429  [   64/60000]\n",
      "loss: 2.296250  [ 6464/60000]\n",
      "loss: 2.294951  [12864/60000]\n",
      "loss: 2.300995  [19264/60000]\n",
      "loss: 2.304350  [25664/60000]\n",
      "loss: 2.300077  [32064/60000]\n",
      "loss: 2.308213  [38464/60000]\n",
      "loss: 2.298815  [44864/60000]\n",
      "loss: 2.309802  [51264/60000]\n",
      "loss: 2.315436  [57664/60000]\n",
      "loss: 2.311296  [   64/60000]\n",
      "loss: 2.301518  [ 6464/60000]\n",
      "loss: 2.299610  [12864/60000]\n",
      "loss: 2.301210  [19264/60000]\n",
      "loss: 2.312541  [25664/60000]\n",
      "loss: 2.298416  [32064/60000]\n",
      "loss: 2.316374  [38464/60000]\n",
      "loss: 2.316052  [44864/60000]\n",
      "loss: 2.316010  [51264/60000]\n",
      "loss: 2.323056  [57664/60000]\n",
      "loss: 2.303525  [   64/60000]\n",
      "loss: 2.296026  [ 6464/60000]\n",
      "loss: 2.307033  [12864/60000]\n",
      "loss: 2.294219  [19264/60000]\n",
      "loss: 2.315706  [25664/60000]\n",
      "loss: 2.299083  [32064/60000]\n",
      "loss: 2.314237  [38464/60000]\n",
      "loss: 2.309971  [44864/60000]\n",
      "loss: 2.304676  [51264/60000]\n",
      "loss: 2.314234  [57664/60000]\n",
      "loss: 2.307444  [   64/60000]\n",
      "loss: 2.294677  [ 6464/60000]\n",
      "loss: 2.299164  [12864/60000]\n",
      "loss: 2.306033  [19264/60000]\n",
      "loss: 2.305159  [25664/60000]\n",
      "loss: 2.295133  [32064/60000]\n",
      "loss: 2.315870  [38464/60000]\n",
      "loss: 2.303840  [44864/60000]\n",
      "loss: 2.311752  [51264/60000]\n",
      "loss: 2.319775  [57664/60000]\n",
      "loss: 2.314001  [   64/60000]\n",
      "loss: 2.301837  [ 6464/60000]\n",
      "loss: 2.300157  [12864/60000]\n",
      "loss: 2.301268  [19264/60000]\n",
      "loss: 2.310519  [25664/60000]\n",
      "loss: 2.300844  [32064/60000]\n",
      "loss: 2.317528  [38464/60000]\n",
      "loss: 2.304765  [44864/60000]\n",
      "loss: 2.311658  [51264/60000]\n",
      "loss: 2.317404  [57664/60000]\n",
      "loss: 2.307199  [   64/60000]\n",
      "loss: 2.296272  [ 6464/60000]\n",
      "loss: 2.296096  [12864/60000]\n",
      "loss: 2.316516  [19264/60000]\n",
      "loss: 2.316002  [25664/60000]\n",
      "loss: 2.311338  [32064/60000]\n",
      "loss: 2.310221  [38464/60000]\n",
      "loss: 2.292663  [44864/60000]\n",
      "loss: 2.297770  [51264/60000]\n",
      "loss: 2.315892  [57664/60000]\n",
      "loss: 2.297348  [   64/60000]\n",
      "loss: 2.301029  [ 6464/60000]\n",
      "loss: 2.293277  [12864/60000]\n",
      "loss: 2.308338  [19264/60000]\n",
      "loss: 2.311549  [25664/60000]\n",
      "loss: 2.295059  [32064/60000]\n",
      "loss: 2.319004  [38464/60000]\n",
      "loss: 2.300674  [44864/60000]\n",
      "loss: 2.295171  [51264/60000]\n",
      "loss: 2.315622  [57664/60000]\n",
      "loss: 2.287559  [   64/60000]\n",
      "loss: 2.305457  [ 6464/60000]\n",
      "loss: 2.295814  [12864/60000]\n",
      "loss: 2.299064  [19264/60000]\n",
      "loss: 2.306952  [25664/60000]\n",
      "loss: 2.307796  [32064/60000]\n",
      "loss: 2.318012  [38464/60000]\n",
      "loss: 2.308356  [44864/60000]\n",
      "loss: 2.313699  [51264/60000]\n",
      "loss: 2.314814  [57664/60000]\n",
      "loss: 2.300895  [   64/60000]\n",
      "loss: 2.302840  [ 6464/60000]\n",
      "loss: 2.299521  [12864/60000]\n",
      "loss: 2.310945  [19264/60000]\n",
      "loss: 2.302719  [25664/60000]\n",
      "loss: 2.307249  [32064/60000]\n",
      "loss: 2.310437  [38464/60000]\n",
      "loss: 2.298538  [44864/60000]\n",
      "loss: 2.305682  [51264/60000]\n",
      "loss: 2.315686  [57664/60000]\n",
      "loss: 2.314167  [   64/60000]\n",
      "loss: 2.283031  [ 6464/60000]\n",
      "loss: 2.308161  [12864/60000]\n",
      "loss: 2.309844  [19264/60000]\n",
      "loss: 2.315105  [25664/60000]\n",
      "loss: 2.308569  [32064/60000]\n",
      "loss: 2.318867  [38464/60000]\n",
      "loss: 2.282710  [44864/60000]\n",
      "loss: 2.299027  [51264/60000]\n",
      "loss: 2.308731  [57664/60000]\n",
      "loss: 2.304870  [   64/60000]\n",
      "loss: 2.293285  [ 6464/60000]\n",
      "loss: 2.301144  [12864/60000]\n",
      "loss: 2.299125  [19264/60000]\n",
      "loss: 2.301678  [25664/60000]\n",
      "loss: 2.297851  [32064/60000]\n",
      "loss: 2.321727  [38464/60000]\n",
      "loss: 2.299215  [44864/60000]\n",
      "loss: 2.302832  [51264/60000]\n",
      "loss: 2.324905  [57664/60000]\n",
      "loss: 2.290930  [   64/60000]\n",
      "loss: 2.302374  [ 6464/60000]\n",
      "loss: 2.300072  [12864/60000]\n",
      "loss: 2.302567  [19264/60000]\n",
      "loss: 2.313968  [25664/60000]\n",
      "loss: 2.308038  [32064/60000]\n",
      "loss: 2.325572  [38464/60000]\n",
      "loss: 2.303386  [44864/60000]\n",
      "loss: 2.315998  [51264/60000]\n",
      "loss: 2.317881  [57664/60000]\n",
      "loss: 2.300052  [   64/60000]\n",
      "loss: 2.297823  [ 6464/60000]\n",
      "loss: 2.312858  [12864/60000]\n",
      "loss: 2.300090  [19264/60000]\n",
      "loss: 2.301605  [25664/60000]\n",
      "loss: 2.299134  [32064/60000]\n",
      "loss: 2.308286  [38464/60000]\n",
      "loss: 2.300587  [44864/60000]\n",
      "loss: 2.304544  [51264/60000]\n",
      "loss: 2.322750  [57664/60000]\n",
      "loss: 2.304555  [   64/60000]\n",
      "loss: 2.306489  [ 6464/60000]\n",
      "loss: 2.294430  [12864/60000]\n",
      "loss: 2.303407  [19264/60000]\n",
      "loss: 2.315881  [25664/60000]\n",
      "loss: 2.301044  [32064/60000]\n",
      "loss: 2.321947  [38464/60000]\n",
      "loss: 2.309908  [44864/60000]\n",
      "loss: 2.306784  [51264/60000]\n",
      "loss: 2.316994  [57664/60000]\n",
      "loss: 2.304198  [   64/60000]\n",
      "loss: 2.302257  [ 6464/60000]\n",
      "loss: 2.305907  [12864/60000]\n",
      "loss: 2.309968  [19264/60000]\n",
      "loss: 2.311903  [25664/60000]\n",
      "loss: 2.290016  [32064/60000]\n",
      "loss: 2.314097  [38464/60000]\n",
      "loss: 2.306032  [44864/60000]\n",
      "loss: 2.304640  [51264/60000]\n",
      "loss: 2.334974  [57664/60000]\n",
      "loss: 2.304738  [   64/60000]\n",
      "loss: 2.305545  [ 6464/60000]\n",
      "loss: 2.295752  [12864/60000]\n",
      "loss: 2.321152  [19264/60000]\n",
      "loss: 2.307403  [25664/60000]\n",
      "loss: 2.297949  [32064/60000]\n",
      "loss: 2.314092  [38464/60000]\n",
      "loss: 2.296010  [44864/60000]\n",
      "loss: 2.299942  [51264/60000]\n",
      "loss: 2.309052  [57664/60000]\n",
      "loss: 2.296838  [   64/60000]\n",
      "loss: 2.303611  [ 6464/60000]\n",
      "loss: 2.308164  [12864/60000]\n",
      "loss: 2.307747  [19264/60000]\n",
      "loss: 2.307930  [25664/60000]\n",
      "loss: 2.299518  [32064/60000]\n",
      "loss: 2.306551  [38464/60000]\n",
      "loss: 2.299955  [44864/60000]\n",
      "loss: 2.296327  [51264/60000]\n",
      "loss: 2.323593  [57664/60000]\n",
      "loss: 2.296516  [   64/60000]\n",
      "loss: 2.293231  [ 6464/60000]\n",
      "loss: 2.298402  [12864/60000]\n",
      "loss: 2.311575  [19264/60000]\n",
      "loss: 2.301100  [25664/60000]\n",
      "loss: 2.303652  [32064/60000]\n",
      "loss: 2.324454  [38464/60000]\n",
      "loss: 2.310310  [44864/60000]\n",
      "loss: 2.303485  [51264/60000]\n",
      "loss: 2.318651  [57664/60000]\n",
      "loss: 2.307790  [   64/60000]\n",
      "loss: 2.315171  [ 6464/60000]\n",
      "loss: 2.303464  [12864/60000]\n",
      "loss: 2.308756  [19264/60000]\n",
      "loss: 2.299263  [25664/60000]\n",
      "loss: 2.306974  [32064/60000]\n",
      "loss: 2.314144  [38464/60000]\n",
      "loss: 2.295093  [44864/60000]\n",
      "loss: 2.300170  [51264/60000]\n",
      "loss: 2.320999  [57664/60000]\n",
      "loss: 2.298761  [   64/60000]\n",
      "loss: 2.297008  [ 6464/60000]\n",
      "loss: 2.305172  [12864/60000]\n",
      "loss: 2.298010  [19264/60000]\n",
      "loss: 2.312613  [25664/60000]\n",
      "loss: 2.296558  [32064/60000]\n",
      "loss: 2.324819  [38464/60000]\n",
      "loss: 2.308843  [44864/60000]\n",
      "loss: 2.300542  [51264/60000]\n",
      "loss: 2.321468  [57664/60000]\n",
      "loss: 2.306592  [   64/60000]\n",
      "loss: 2.297823  [ 6464/60000]\n",
      "loss: 2.305834  [12864/60000]\n",
      "loss: 2.307695  [19264/60000]\n",
      "loss: 2.316587  [25664/60000]\n",
      "loss: 2.300415  [32064/60000]\n",
      "loss: 2.314817  [38464/60000]\n",
      "loss: 2.297600  [44864/60000]\n",
      "loss: 2.314479  [51264/60000]\n",
      "loss: 2.317716  [57664/60000]\n",
      "loss: 2.309694  [   64/60000]\n",
      "loss: 2.296356  [ 6464/60000]\n",
      "loss: 2.297308  [12864/60000]\n",
      "loss: 2.309684  [19264/60000]\n",
      "loss: 2.309531  [25664/60000]\n",
      "loss: 2.310388  [32064/60000]\n",
      "loss: 2.299387  [38464/60000]\n",
      "loss: 2.295662  [44864/60000]\n",
      "loss: 2.297199  [51264/60000]\n",
      "loss: 2.310250  [57664/60000]\n",
      "loss: 2.306107  [   64/60000]\n",
      "loss: 2.309510  [ 6464/60000]\n",
      "loss: 2.311990  [12864/60000]\n",
      "loss: 2.297996  [19264/60000]\n",
      "loss: 2.301490  [25664/60000]\n",
      "loss: 2.303723  [32064/60000]\n",
      "loss: 2.320112  [38464/60000]\n",
      "loss: 2.295462  [44864/60000]\n",
      "loss: 2.300470  [51264/60000]\n",
      "loss: 2.306537  [57664/60000]\n",
      "loss: 2.307324  [   64/60000]\n",
      "loss: 2.301313  [ 6464/60000]\n",
      "loss: 2.296548  [12864/60000]\n",
      "loss: 2.307855  [19264/60000]\n",
      "loss: 2.310017  [25664/60000]\n",
      "loss: 2.296001  [32064/60000]\n",
      "loss: 2.314299  [38464/60000]\n",
      "loss: 2.311777  [44864/60000]\n",
      "loss: 2.308421  [51264/60000]\n",
      "loss: 2.324181  [57664/60000]\n",
      "loss: 2.303556  [   64/60000]\n",
      "loss: 2.306871  [ 6464/60000]\n",
      "loss: 2.298667  [12864/60000]\n",
      "loss: 2.305353  [19264/60000]\n",
      "loss: 2.312439  [25664/60000]\n",
      "loss: 2.309602  [32064/60000]\n",
      "loss: 2.305447  [38464/60000]\n",
      "loss: 2.310757  [44864/60000]\n",
      "loss: 2.304204  [51264/60000]\n",
      "loss: 2.320968  [57664/60000]\n",
      "loss: 2.304469  [   64/60000]\n",
      "loss: 2.303032  [ 6464/60000]\n",
      "loss: 2.297491  [12864/60000]\n",
      "loss: 2.306415  [19264/60000]\n",
      "loss: 2.306380  [25664/60000]\n",
      "loss: 2.301188  [32064/60000]\n",
      "loss: 2.310353  [38464/60000]\n",
      "loss: 2.307209  [44864/60000]\n",
      "loss: 2.297179  [51264/60000]\n",
      "loss: 2.318282  [57664/60000]\n",
      "loss: 2.294977  [   64/60000]\n",
      "loss: 2.302252  [ 6464/60000]\n",
      "loss: 2.291133  [12864/60000]\n",
      "loss: 2.310229  [19264/60000]\n",
      "loss: 2.302342  [25664/60000]\n",
      "loss: 2.286230  [32064/60000]\n",
      "loss: 2.315562  [38464/60000]\n",
      "loss: 2.305515  [44864/60000]\n",
      "loss: 2.303867  [51264/60000]\n",
      "loss: 2.323903  [57664/60000]\n",
      "loss: 2.292591  [   64/60000]\n",
      "loss: 2.292469  [ 6464/60000]\n",
      "loss: 2.292025  [12864/60000]\n",
      "loss: 2.319214  [19264/60000]\n",
      "loss: 2.314570  [25664/60000]\n",
      "loss: 2.295542  [32064/60000]\n",
      "loss: 2.308802  [38464/60000]\n",
      "loss: 2.295530  [44864/60000]\n",
      "loss: 2.293327  [51264/60000]\n",
      "loss: 2.317717  [57664/60000]\n",
      "loss: 2.300853  [   64/60000]\n",
      "loss: 2.297401  [ 6464/60000]\n",
      "loss: 2.314146  [12864/60000]\n",
      "loss: 2.301586  [19264/60000]\n",
      "loss: 2.301058  [25664/60000]\n",
      "loss: 2.309849  [32064/60000]\n",
      "loss: 2.310232  [38464/60000]\n",
      "loss: 2.303338  [44864/60000]\n",
      "loss: 2.307758  [51264/60000]\n",
      "loss: 2.320846  [57664/60000]\n",
      "loss: 2.299506  [   64/60000]\n",
      "loss: 2.295178  [ 6464/60000]\n",
      "loss: 2.297791  [12864/60000]\n",
      "loss: 2.296847  [19264/60000]\n",
      "loss: 2.315109  [25664/60000]\n",
      "loss: 2.299948  [32064/60000]\n",
      "loss: 2.323820  [38464/60000]\n",
      "loss: 2.305914  [44864/60000]\n",
      "loss: 2.295373  [51264/60000]\n",
      "loss: 2.316259  [57664/60000]\n",
      "loss: 2.297544  [   64/60000]\n",
      "loss: 2.304913  [ 6464/60000]\n",
      "loss: 2.299400  [12864/60000]\n",
      "loss: 2.306976  [19264/60000]\n",
      "loss: 2.307005  [25664/60000]\n",
      "loss: 2.296755  [32064/60000]\n",
      "loss: 2.314741  [38464/60000]\n",
      "loss: 2.297595  [44864/60000]\n",
      "loss: 2.311206  [51264/60000]\n",
      "loss: 2.322335  [57664/60000]\n",
      "loss: 2.294931  [   64/60000]\n",
      "loss: 2.298940  [ 6464/60000]\n",
      "loss: 2.304190  [12864/60000]\n",
      "loss: 2.303900  [19264/60000]\n",
      "loss: 2.309842  [25664/60000]\n",
      "loss: 2.300178  [32064/60000]\n",
      "loss: 2.319222  [38464/60000]\n",
      "loss: 2.307424  [44864/60000]\n",
      "loss: 2.291843  [51264/60000]\n",
      "loss: 2.318946  [57664/60000]\n",
      "loss: 2.309477  [   64/60000]\n",
      "loss: 2.297136  [ 6464/60000]\n",
      "loss: 2.292710  [12864/60000]\n",
      "loss: 2.313214  [19264/60000]\n",
      "loss: 2.315699  [25664/60000]\n",
      "loss: 2.300188  [32064/60000]\n",
      "loss: 2.323429  [38464/60000]\n",
      "loss: 2.309939  [44864/60000]\n",
      "loss: 2.304195  [51264/60000]\n",
      "loss: 2.321316  [57664/60000]\n",
      "loss: 2.293385  [   64/60000]\n",
      "loss: 2.289847  [ 6464/60000]\n",
      "loss: 2.303209  [12864/60000]\n",
      "loss: 2.318401  [19264/60000]\n",
      "loss: 2.302790  [25664/60000]\n",
      "loss: 2.303340  [32064/60000]\n",
      "loss: 2.322629  [38464/60000]\n",
      "loss: 2.302909  [44864/60000]\n",
      "loss: 2.303895  [51264/60000]\n",
      "loss: 2.305888  [57664/60000]\n",
      "loss: 2.303536  [   64/60000]\n",
      "loss: 2.306523  [ 6464/60000]\n",
      "loss: 2.294681  [12864/60000]\n",
      "loss: 2.304518  [19264/60000]\n",
      "loss: 2.310074  [25664/60000]\n",
      "loss: 2.307729  [32064/60000]\n",
      "loss: 2.316320  [38464/60000]\n",
      "loss: 2.302543  [44864/60000]\n",
      "loss: 2.307107  [51264/60000]\n",
      "loss: 2.321586  [57664/60000]\n",
      "loss: 2.300196  [   64/60000]\n",
      "loss: 2.314783  [ 6464/60000]\n",
      "loss: 2.293258  [12864/60000]\n",
      "loss: 2.306871  [19264/60000]\n",
      "loss: 2.314634  [25664/60000]\n",
      "loss: 2.305235  [32064/60000]\n",
      "loss: 2.308085  [38464/60000]\n",
      "loss: 2.314371  [44864/60000]\n",
      "loss: 2.304525  [51264/60000]\n",
      "loss: 2.324194  [57664/60000]\n",
      "loss: 2.307196  [   64/60000]\n",
      "loss: 2.297748  [ 6464/60000]\n",
      "loss: 2.296670  [12864/60000]\n",
      "loss: 2.308453  [19264/60000]\n",
      "loss: 2.310605  [25664/60000]\n",
      "loss: 2.312436  [32064/60000]\n",
      "loss: 2.316406  [38464/60000]\n",
      "loss: 2.290063  [44864/60000]\n",
      "loss: 2.297027  [51264/60000]\n",
      "loss: 2.315056  [57664/60000]\n",
      "loss: 2.298583  [   64/60000]\n",
      "loss: 2.308588  [ 6464/60000]\n",
      "loss: 2.299424  [12864/60000]\n",
      "loss: 2.303652  [19264/60000]\n",
      "loss: 2.308361  [25664/60000]\n",
      "loss: 2.302476  [32064/60000]\n",
      "loss: 2.320886  [38464/60000]\n",
      "loss: 2.296377  [44864/60000]\n",
      "loss: 2.305308  [51264/60000]\n",
      "loss: 2.322380  [57664/60000]\n",
      "loss: 2.304880  [   64/60000]\n",
      "loss: 2.297752  [ 6464/60000]\n",
      "loss: 2.297123  [12864/60000]\n",
      "loss: 2.303325  [19264/60000]\n",
      "loss: 2.297372  [25664/60000]\n",
      "loss: 2.313000  [32064/60000]\n",
      "loss: 2.312086  [38464/60000]\n",
      "loss: 2.298422  [44864/60000]\n",
      "loss: 2.300638  [51264/60000]\n",
      "loss: 2.326207  [57664/60000]\n",
      "loss: 2.306052  [   64/60000]\n",
      "loss: 2.300255  [ 6464/60000]\n",
      "loss: 2.301181  [12864/60000]\n",
      "loss: 2.308847  [19264/60000]\n",
      "loss: 2.307151  [25664/60000]\n",
      "loss: 2.305961  [32064/60000]\n",
      "loss: 2.313040  [38464/60000]\n",
      "loss: 2.300504  [44864/60000]\n",
      "loss: 2.303915  [51264/60000]\n",
      "loss: 2.326949  [57664/60000]\n",
      "loss: 2.299890  [   64/60000]\n",
      "loss: 2.306076  [ 6464/60000]\n",
      "loss: 2.296351  [12864/60000]\n",
      "loss: 2.318638  [19264/60000]\n",
      "loss: 2.317392  [25664/60000]\n",
      "loss: 2.316938  [32064/60000]\n",
      "loss: 2.311599  [38464/60000]\n",
      "loss: 2.293783  [44864/60000]\n",
      "loss: 2.294820  [51264/60000]\n",
      "loss: 2.324052  [57664/60000]\n",
      "loss: 2.295940  [   64/60000]\n",
      "loss: 2.302200  [ 6464/60000]\n",
      "loss: 2.296854  [12864/60000]\n",
      "loss: 2.312106  [19264/60000]\n",
      "loss: 2.304696  [25664/60000]\n",
      "loss: 2.309373  [32064/60000]\n",
      "loss: 2.336358  [38464/60000]\n",
      "loss: 2.305969  [44864/60000]\n",
      "loss: 2.324399  [51264/60000]\n",
      "loss: 2.300679  [57664/60000]\n",
      "loss: 2.297321  [   64/60000]\n",
      "loss: 2.295683  [ 6464/60000]\n",
      "loss: 2.310410  [12864/60000]\n",
      "loss: 2.305362  [19264/60000]\n",
      "loss: 2.313317  [25664/60000]\n",
      "loss: 2.302071  [32064/60000]\n",
      "loss: 2.308275  [38464/60000]\n",
      "loss: 2.309340  [44864/60000]\n",
      "loss: 2.315939  [51264/60000]\n",
      "loss: 2.312480  [57664/60000]\n",
      "loss: 2.305357  [   64/60000]\n",
      "loss: 2.303526  [ 6464/60000]\n",
      "loss: 2.287102  [12864/60000]\n",
      "loss: 2.302648  [19264/60000]\n",
      "loss: 2.303958  [25664/60000]\n",
      "loss: 2.313763  [32064/60000]\n",
      "loss: 2.312130  [38464/60000]\n",
      "loss: 2.306308  [44864/60000]\n",
      "loss: 2.296174  [51264/60000]\n",
      "loss: 2.316363  [57664/60000]\n",
      "loss: 2.309309  [   64/60000]\n",
      "loss: 2.306596  [ 6464/60000]\n",
      "loss: 2.294549  [12864/60000]\n",
      "loss: 2.309206  [19264/60000]\n",
      "loss: 2.310270  [25664/60000]\n",
      "loss: 2.300888  [32064/60000]\n",
      "loss: 2.309829  [38464/60000]\n",
      "loss: 2.307530  [44864/60000]\n",
      "loss: 2.303027  [51264/60000]\n",
      "loss: 2.328400  [57664/60000]\n",
      "loss: 2.313154  [   64/60000]\n",
      "loss: 2.299692  [ 6464/60000]\n",
      "loss: 2.303552  [12864/60000]\n",
      "loss: 2.305532  [19264/60000]\n",
      "loss: 2.306636  [25664/60000]\n",
      "loss: 2.305555  [32064/60000]\n",
      "loss: 2.317561  [38464/60000]\n",
      "loss: 2.302898  [44864/60000]\n",
      "loss: 2.291898  [51264/60000]\n",
      "loss: 2.322247  [57664/60000]\n",
      "loss: 2.290168  [   64/60000]\n",
      "loss: 2.303445  [ 6464/60000]\n",
      "loss: 2.312931  [12864/60000]\n",
      "loss: 2.307173  [19264/60000]\n",
      "loss: 2.307796  [25664/60000]\n",
      "loss: 2.308827  [32064/60000]\n",
      "loss: 2.298870  [38464/60000]\n",
      "loss: 2.304948  [44864/60000]\n",
      "loss: 2.306902  [51264/60000]\n",
      "loss: 2.320973  [57664/60000]\n",
      "loss: 2.312527  [   64/60000]\n",
      "loss: 2.300858  [ 6464/60000]\n",
      "loss: 2.305866  [12864/60000]\n",
      "loss: 2.306882  [19264/60000]\n",
      "loss: 2.317458  [25664/60000]\n",
      "loss: 2.294456  [32064/60000]\n",
      "loss: 2.313617  [38464/60000]\n",
      "loss: 2.309333  [44864/60000]\n",
      "loss: 2.312210  [51264/60000]\n",
      "loss: 2.313901  [57664/60000]\n",
      "loss: 2.287897  [   64/60000]\n",
      "loss: 2.298500  [ 6464/60000]\n",
      "loss: 2.290057  [12864/60000]\n",
      "loss: 2.300284  [19264/60000]\n",
      "loss: 2.310560  [25664/60000]\n",
      "loss: 2.298755  [32064/60000]\n",
      "loss: 2.316516  [38464/60000]\n",
      "loss: 2.306585  [44864/60000]\n",
      "loss: 2.315724  [51264/60000]\n",
      "loss: 2.323747  [57664/60000]\n",
      "loss: 2.306715  [   64/60000]\n",
      "loss: 2.303346  [ 6464/60000]\n",
      "loss: 2.301909  [12864/60000]\n",
      "loss: 2.295562  [19264/60000]\n",
      "loss: 2.307890  [25664/60000]\n",
      "loss: 2.297883  [32064/60000]\n",
      "loss: 2.312797  [38464/60000]\n",
      "loss: 2.298202  [44864/60000]\n",
      "loss: 2.310694  [51264/60000]\n",
      "loss: 2.323404  [57664/60000]\n"
     ]
    }
   ],
   "source": [
    "model.to(\"cuda\")\n",
    "model.eval()\n",
    "backend = \"x86\"\n",
    "model.qconfig = torch.quantization.get_default_qat_qconfig(backend)\n",
    "\n",
    "model_fp32_prepared = torch.quantization.prepare_qat(model.train(), inplace=False)\n",
    "\n",
    "for i in range(200):\n",
    "    train(train_dataloader, model_fp32_prepared, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fp32_prepared.to(\"cpu\")\n",
    "model_fp32_prepared.eval()\n",
    "model_int8 = torch.quantization.convert(model_fp32_prepared, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 9.9%, Avg loss: 2.303754 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 9.9%, Avg loss: 2.303747 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 9.9%, Avg loss: 2.303748 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 9.9%, Avg loss: 2.303743 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 9.9%, Avg loss: 2.303737 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 9.9%, Avg loss: 2.303732 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 9.9%, Avg loss: 2.303733 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 9.9%, Avg loss: 2.303732 \n",
      "\n",
      "457 ms ± 2.88 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "test(test_dataloader, model_fp32_prepared, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 9.9%, Avg loss: 2.303760 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 9.9%, Avg loss: 2.303760 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 9.9%, Avg loss: 2.303760 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 9.9%, Avg loss: 2.303760 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 9.9%, Avg loss: 2.303760 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 9.9%, Avg loss: 2.303760 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 9.9%, Avg loss: 2.303760 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 9.9%, Avg loss: 2.303760 \n",
      "\n",
      "690 ms ± 290 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "test(test_dataloader, model_int8, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_int8, \"model_int8.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
