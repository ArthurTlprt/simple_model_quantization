{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.quant = torch.ao.quantization.QuantStub()\n",
    "        self.conv1 = nn.Conv2d(1, 8, 3, stride=2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(8, 8*8, 3, stride=2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.AvgPool2d(3, 2),\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 128), \n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 10) \n",
    "            # nn.Softmax()\n",
    "        )\n",
    "        self.dequant = torch.ao.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        y = self.head(x)\n",
    "        y = self.dequant(y)\n",
    "        return y\n",
    "\n",
    "def fuse_conv_and_bn(conv, bn):\n",
    "    # Fuse convolution and batchnorm layers https://tehnokv.com/posts/fusing-batchnorm-and-conv/\n",
    "    fusedconv = nn.Conv2d(conv.in_channels,\n",
    "                          conv.out_channels,\n",
    "                          kernel_size=conv.kernel_size,\n",
    "                          stride=conv.stride,\n",
    "                          padding=conv.padding,\n",
    "                          groups=conv.groups,\n",
    "                          bias=True).requires_grad_(False).to(conv.weight.device)\n",
    "\n",
    "    # prepare filters\n",
    "    w_conv = conv.weight.clone().view(conv.out_channels, -1)\n",
    "    w_bn = torch.diag(bn.weight.div(torch.sqrt(bn.eps + bn.running_var)))\n",
    "    fusedconv.weight.copy_(torch.mm(w_bn, w_conv).view(fusedconv.weight.shape))\n",
    "\n",
    "    # prepare spatial bias\n",
    "    b_conv = torch.zeros(conv.weight.size(0), device=conv.weight.device) if conv.bias is None else conv.bias\n",
    "    b_bn = bn.bias - bn.weight.mul(bn.running_mean).div(torch.sqrt(bn.running_var + bn.eps))\n",
    "    fusedconv.bias.copy_(torch.mm(w_bn, b_conv.reshape(-1, 1)).reshape(-1) + b_bn)\n",
    "\n",
    "    return fusedconv\n",
    "\n",
    "class ConvBN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, 3, stride=2)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.activation = nn.LeakyReLU(0.1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return self.activation(x)\n",
    "    \n",
    "    def fuse(self):\n",
    "        return fuse_conv_and_bn(self.conv, self.bn)\n",
    "    \n",
    "\n",
    "class Head(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.module = nn.Sequential(\n",
    "            nn.AvgPool2d(3, 2),\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256, 128), \n",
    "            nn.Dropout(),\n",
    "            nn.Linear(128, 10)\n",
    "            # nn.Softmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.module(x)\n",
    "\n",
    "class ComposedClassifier(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.quant = torch.ao.quantization.QuantStub()\n",
    "        self.convbn1 = ConvBN(1, 8)\n",
    "        self.convbn2 = ConvBN(8, 8*8)\n",
    "        self.head = Head()\n",
    "        self.dequant = torch.ao.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = self.convbn1(x)\n",
    "        x = self.convbn2(x)\n",
    "        y = self.head(x)\n",
    "        y = self.dequant(y)\n",
    "        return y\n",
    "    \n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X = X.to(\"cuda\")\n",
    "        y = y.to(\"cuda\")\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            # print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            # print(correct)\n",
    "            # print((len(y) * 100))\n",
    "            # print(correct * 100. / (len(y) * 100))\n",
    "            correct = 0\n",
    "\n",
    "\n",
    "def test(dataloader, model, loss_fn, cuda=False):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            if cuda:\n",
    "                X, y = X.to('cuda'), y.to('cuda')\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ComposedClassifier()\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arthur/miniconda3/lib/python3.11/site-packages/torch/ao/quantization/observer.py:220: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n",
      "/home/arthur/miniconda3/lib/python3.11/site-packages/torch/ao/quantization/fake_quantize.py:353: UserWarning: _aminmax is deprecated as of PyTorch 1.11 and will be removed in a future release. Use aminmax instead. This warning will only appear once per process. (Triggered internally at ../aten/src/ATen/native/ReduceAllOps.cpp:71.)\n",
      "  return torch.fused_moving_avg_obs_fake_quant(\n",
      "/home/arthur/miniconda3/lib/python3.11/site-packages/torch/ao/quantization/fake_quantize.py:353: UserWarning: _aminmax is deprecated as of PyTorch 1.11 and will be removed in a future release. Use aminmax instead. This warning will only appear once per process. (Triggered internally at ../aten/src/ATen/native/TensorCompare.cpp:687.)\n",
      "  return torch.fused_moving_avg_obs_fake_quant(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 47.2%, Avg loss: 1.979681 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 52.4%, Avg loss: 1.701463 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.6%, Avg loss: 1.480321 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.7%, Avg loss: 1.323876 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 1.206758 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 66.1%, Avg loss: 1.123938 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 67.2%, Avg loss: 1.062452 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.2%, Avg loss: 1.008818 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 68.7%, Avg loss: 0.972030 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.940650 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.2%, Avg loss: 0.912938 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 70.5%, Avg loss: 0.889585 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.1%, Avg loss: 0.871707 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.5%, Avg loss: 0.856695 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.4%, Avg loss: 0.842937 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 71.9%, Avg loss: 0.828555 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.2%, Avg loss: 0.815745 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.5%, Avg loss: 0.804035 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.6%, Avg loss: 0.793586 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.9%, Avg loss: 0.785418 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = model.to(\"cuda\")\n",
    "model.eval()\n",
    "backend = \"x86\"\n",
    "model.qconfig = torch.quantization.get_default_qat_qconfig(backend)\n",
    "\n",
    "# model_fp32_fused = torch.ao.quantization.fuse_modules(model,\n",
    "#     ['convbn1', 'convbn2'])\n",
    "\n",
    "model_fp32_prepared = torch.quantization.prepare_qat(model.train(), inplace=False)\n",
    "optimizer = torch.optim.SGD(model_fp32_prepared.parameters(), lr=1e-3)\n",
    "\n",
    "for i in range(20):\n",
    "    model_fp32_prepared = model_fp32_prepared.to('cuda')\n",
    "    train(train_dataloader, model_fp32_prepared, loss_fn, optimizer)\n",
    "    model_fp32_prepared = model_fp32_prepared.to('cpu')\n",
    "    test(test_dataloader, model_fp32_prepared, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fp32_prepared.to(\"cpu\")\n",
    "model_fp32_prepared.eval()\n",
    "model_int8 = torch.quantization.convert(model_fp32_prepared, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 72.9%, Avg loss: 0.785269 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.8%, Avg loss: 0.785155 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.7%, Avg loss: 0.785301 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.7%, Avg loss: 0.784912 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.8%, Avg loss: 0.784845 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.7%, Avg loss: 0.784882 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.7%, Avg loss: 0.784929 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.7%, Avg loss: 0.784903 \n",
      "\n",
      "1.14 s ± 346 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "test(test_dataloader, model_fp32_prepared, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 72.8%, Avg loss: 0.785484 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.8%, Avg loss: 0.785484 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.8%, Avg loss: 0.785484 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.8%, Avg loss: 0.785484 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.8%, Avg loss: 0.785484 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.8%, Avg loss: 0.785484 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.8%, Avg loss: 0.785484 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 72.8%, Avg loss: 0.785484 \n",
      "\n",
      "991 ms ± 32 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "test(test_dataloader, model_int8, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_int8, \"model_int8_qat.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T_destination', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_backward_hooks', '_backward_pre_hooks', '_buffers', '_call_impl', '_compiled_call_impl', '_forward_hooks', '_forward_hooks_always_called', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_get_backward_hooks', '_get_backward_pre_hooks', '_get_name', '_is_full_backward_hook', '_load_from_state_dict', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_maybe_warn_non_full_backward_hook', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_state_dict_pre_hooks', '_version', '_wrapped_call_impl', 'add_module', 'apply', 'bfloat16', 'buffers', 'call_super_init', 'children', 'compile', 'convbn1', 'convbn2', 'cpu', 'cuda', 'dequant', 'double', 'dump_patches', 'eval', 'extra_repr', 'float', 'forward', 'get_buffer', 'get_extra_state', 'get_parameter', 'get_submodule', 'half', 'head', 'ipu', 'load_state_dict', 'modules', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'parameters', 'quant', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_module', 'register_parameter', 'register_state_dict_pre_hook', 'requires_grad_', 'set_extra_state', 'share_memory', 'state_dict', 'to', 'to_empty', 'train', 'training', 'type', 'xpu', 'zero_grad']\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "model = deepcopy(model_fp32_prepared)\n",
    "model.to('cpu')\n",
    "model.eval()\n",
    "model = torch.quantization.convert(model, inplace=False)\n",
    "print(dir(model))\n",
    "torch.save(model, \"test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
