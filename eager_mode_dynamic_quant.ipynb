{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, 3, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(8, 8*8, 3, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(3, 2),\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256, 128), \n",
    "            nn.Dropout(),\n",
    "            nn.Linear(128, 10) \n",
    "            # nn.Softmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.model(x)\n",
    "        return y\n",
    "    \n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X = X.to(\"cuda\")\n",
    "        y = y.to(\"cuda\")\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier()\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.324635  [   64/60000]\n",
      "loss: 2.315197  [ 6464/60000]\n",
      "loss: 2.293364  [12864/60000]\n",
      "loss: 2.308164  [19264/60000]\n",
      "loss: 2.313383  [25664/60000]\n",
      "loss: 2.298207  [32064/60000]\n",
      "loss: 2.291450  [38464/60000]\n",
      "loss: 2.313577  [44864/60000]\n",
      "loss: 2.308894  [51264/60000]\n",
      "loss: 2.299623  [57664/60000]\n",
      "loss: 2.301777  [   64/60000]\n",
      "loss: 2.301740  [ 6464/60000]\n",
      "loss: 2.305539  [12864/60000]\n",
      "loss: 2.295763  [19264/60000]\n",
      "loss: 2.302462  [25664/60000]\n",
      "loss: 2.281150  [32064/60000]\n",
      "loss: 2.294436  [38464/60000]\n",
      "loss: 2.271961  [44864/60000]\n",
      "loss: 2.289284  [51264/60000]\n",
      "loss: 2.294258  [57664/60000]\n",
      "loss: 2.288925  [   64/60000]\n",
      "loss: 2.296598  [ 6464/60000]\n",
      "loss: 2.294809  [12864/60000]\n",
      "loss: 2.284444  [19264/60000]\n",
      "loss: 2.305398  [25664/60000]\n",
      "loss: 2.268583  [32064/60000]\n",
      "loss: 2.306093  [38464/60000]\n",
      "loss: 2.253063  [44864/60000]\n",
      "loss: 2.284512  [51264/60000]\n",
      "loss: 2.260110  [57664/60000]\n",
      "loss: 2.282986  [   64/60000]\n",
      "loss: 2.275275  [ 6464/60000]\n",
      "loss: 2.273907  [12864/60000]\n",
      "loss: 2.284620  [19264/60000]\n",
      "loss: 2.282102  [25664/60000]\n",
      "loss: 2.242465  [32064/60000]\n",
      "loss: 2.279948  [38464/60000]\n",
      "loss: 2.256877  [44864/60000]\n",
      "loss: 2.261905  [51264/60000]\n",
      "loss: 2.242753  [57664/60000]\n",
      "loss: 2.235581  [   64/60000]\n",
      "loss: 2.259789  [ 6464/60000]\n",
      "loss: 2.225501  [12864/60000]\n",
      "loss: 2.237666  [19264/60000]\n",
      "loss: 2.265251  [25664/60000]\n",
      "loss: 2.216602  [32064/60000]\n",
      "loss: 2.246593  [38464/60000]\n",
      "loss: 2.239726  [44864/60000]\n",
      "loss: 2.215070  [51264/60000]\n",
      "loss: 2.227393  [57664/60000]\n",
      "loss: 2.211325  [   64/60000]\n",
      "loss: 2.227812  [ 6464/60000]\n",
      "loss: 2.193621  [12864/60000]\n",
      "loss: 2.204329  [19264/60000]\n",
      "loss: 2.247948  [25664/60000]\n",
      "loss: 2.142444  [32064/60000]\n",
      "loss: 2.197503  [38464/60000]\n",
      "loss: 2.213917  [44864/60000]\n",
      "loss: 2.188912  [51264/60000]\n",
      "loss: 2.158891  [57664/60000]\n",
      "loss: 2.184560  [   64/60000]\n",
      "loss: 2.150457  [ 6464/60000]\n",
      "loss: 2.169161  [12864/60000]\n",
      "loss: 2.179546  [19264/60000]\n",
      "loss: 2.161169  [25664/60000]\n",
      "loss: 2.097477  [32064/60000]\n",
      "loss: 2.098065  [38464/60000]\n",
      "loss: 2.101982  [44864/60000]\n",
      "loss: 2.087358  [51264/60000]\n",
      "loss: 2.070519  [57664/60000]\n",
      "loss: 2.097060  [   64/60000]\n",
      "loss: 2.103130  [ 6464/60000]\n",
      "loss: 2.083300  [12864/60000]\n",
      "loss: 2.059625  [19264/60000]\n",
      "loss: 2.000772  [25664/60000]\n",
      "loss: 1.996971  [32064/60000]\n",
      "loss: 2.002498  [38464/60000]\n",
      "loss: 1.958665  [44864/60000]\n",
      "loss: 2.025493  [51264/60000]\n",
      "loss: 1.943326  [57664/60000]\n",
      "loss: 1.963869  [   64/60000]\n",
      "loss: 2.004350  [ 6464/60000]\n",
      "loss: 1.939876  [12864/60000]\n",
      "loss: 1.932541  [19264/60000]\n",
      "loss: 1.867868  [25664/60000]\n",
      "loss: 1.948438  [32064/60000]\n",
      "loss: 1.915897  [38464/60000]\n",
      "loss: 1.894336  [44864/60000]\n",
      "loss: 1.852929  [51264/60000]\n",
      "loss: 1.780804  [57664/60000]\n",
      "loss: 1.873208  [   64/60000]\n",
      "loss: 1.874567  [ 6464/60000]\n",
      "loss: 1.831443  [12864/60000]\n",
      "loss: 1.870070  [19264/60000]\n",
      "loss: 1.786450  [25664/60000]\n",
      "loss: 1.731738  [32064/60000]\n",
      "loss: 1.799821  [38464/60000]\n",
      "loss: 1.757370  [44864/60000]\n",
      "loss: 1.810468  [51264/60000]\n",
      "loss: 1.759060  [57664/60000]\n",
      "loss: 1.701245  [   64/60000]\n",
      "loss: 1.692071  [ 6464/60000]\n",
      "loss: 1.679929  [12864/60000]\n",
      "loss: 1.756977  [19264/60000]\n",
      "loss: 1.723589  [25664/60000]\n",
      "loss: 1.674131  [32064/60000]\n",
      "loss: 1.669216  [38464/60000]\n",
      "loss: 1.693892  [44864/60000]\n",
      "loss: 1.668509  [51264/60000]\n",
      "loss: 1.637916  [57664/60000]\n",
      "loss: 1.787075  [   64/60000]\n",
      "loss: 1.755156  [ 6464/60000]\n",
      "loss: 1.595917  [12864/60000]\n",
      "loss: 1.635616  [19264/60000]\n",
      "loss: 1.574693  [25664/60000]\n",
      "loss: 1.620845  [32064/60000]\n",
      "loss: 1.593918  [38464/60000]\n",
      "loss: 1.640738  [44864/60000]\n",
      "loss: 1.753107  [51264/60000]\n",
      "loss: 1.571572  [57664/60000]\n",
      "loss: 1.659492  [   64/60000]\n",
      "loss: 1.579390  [ 6464/60000]\n",
      "loss: 1.507722  [12864/60000]\n",
      "loss: 1.661781  [19264/60000]\n",
      "loss: 1.590520  [25664/60000]\n",
      "loss: 1.500811  [32064/60000]\n",
      "loss: 1.606820  [38464/60000]\n",
      "loss: 1.595605  [44864/60000]\n",
      "loss: 1.750509  [51264/60000]\n",
      "loss: 1.437561  [57664/60000]\n",
      "loss: 1.631156  [   64/60000]\n",
      "loss: 1.534319  [ 6464/60000]\n",
      "loss: 1.471213  [12864/60000]\n",
      "loss: 1.628427  [19264/60000]\n",
      "loss: 1.481664  [25664/60000]\n",
      "loss: 1.572907  [32064/60000]\n",
      "loss: 1.520237  [38464/60000]\n",
      "loss: 1.640239  [44864/60000]\n",
      "loss: 1.714474  [51264/60000]\n",
      "loss: 1.367317  [57664/60000]\n",
      "loss: 1.730152  [   64/60000]\n",
      "loss: 1.494572  [ 6464/60000]\n",
      "loss: 1.431695  [12864/60000]\n",
      "loss: 1.534690  [19264/60000]\n",
      "loss: 1.528553  [25664/60000]\n",
      "loss: 1.571634  [32064/60000]\n",
      "loss: 1.535213  [38464/60000]\n",
      "loss: 1.529647  [44864/60000]\n",
      "loss: 1.557868  [51264/60000]\n",
      "loss: 1.336317  [57664/60000]\n",
      "loss: 1.486293  [   64/60000]\n",
      "loss: 1.567363  [ 6464/60000]\n",
      "loss: 1.448792  [12864/60000]\n",
      "loss: 1.632504  [19264/60000]\n",
      "loss: 1.662650  [25664/60000]\n",
      "loss: 1.512992  [32064/60000]\n",
      "loss: 1.461220  [38464/60000]\n",
      "loss: 1.577487  [44864/60000]\n",
      "loss: 1.614134  [51264/60000]\n",
      "loss: 1.310567  [57664/60000]\n",
      "loss: 1.484351  [   64/60000]\n",
      "loss: 1.387157  [ 6464/60000]\n",
      "loss: 1.391113  [12864/60000]\n",
      "loss: 1.552641  [19264/60000]\n",
      "loss: 1.546135  [25664/60000]\n",
      "loss: 1.471491  [32064/60000]\n",
      "loss: 1.465097  [38464/60000]\n",
      "loss: 1.508387  [44864/60000]\n",
      "loss: 1.495440  [51264/60000]\n",
      "loss: 1.173903  [57664/60000]\n",
      "loss: 1.569241  [   64/60000]\n",
      "loss: 1.327482  [ 6464/60000]\n",
      "loss: 1.323769  [12864/60000]\n",
      "loss: 1.459265  [19264/60000]\n",
      "loss: 1.628829  [25664/60000]\n",
      "loss: 1.497278  [32064/60000]\n",
      "loss: 1.418911  [38464/60000]\n",
      "loss: 1.550967  [44864/60000]\n",
      "loss: 1.409855  [51264/60000]\n",
      "loss: 1.456532  [57664/60000]\n",
      "loss: 1.400769  [   64/60000]\n",
      "loss: 1.378227  [ 6464/60000]\n",
      "loss: 1.386035  [12864/60000]\n",
      "loss: 1.482150  [19264/60000]\n",
      "loss: 1.443433  [25664/60000]\n",
      "loss: 1.399587  [32064/60000]\n",
      "loss: 1.521973  [38464/60000]\n",
      "loss: 1.496137  [44864/60000]\n",
      "loss: 1.432592  [51264/60000]\n",
      "loss: 1.307886  [57664/60000]\n",
      "loss: 1.457601  [   64/60000]\n",
      "loss: 1.385822  [ 6464/60000]\n",
      "loss: 1.282132  [12864/60000]\n",
      "loss: 1.427389  [19264/60000]\n",
      "loss: 1.388456  [25664/60000]\n",
      "loss: 1.581722  [32064/60000]\n",
      "loss: 1.416811  [38464/60000]\n",
      "loss: 1.395347  [44864/60000]\n",
      "loss: 1.531085  [51264/60000]\n",
      "loss: 1.422291  [57664/60000]\n",
      "loss: 1.368916  [   64/60000]\n",
      "loss: 1.331334  [ 6464/60000]\n",
      "loss: 1.197485  [12864/60000]\n",
      "loss: 1.485167  [19264/60000]\n",
      "loss: 1.495459  [25664/60000]\n",
      "loss: 1.303640  [32064/60000]\n",
      "loss: 1.365628  [38464/60000]\n",
      "loss: 1.443550  [44864/60000]\n",
      "loss: 1.433137  [51264/60000]\n",
      "loss: 1.279140  [57664/60000]\n",
      "loss: 1.394700  [   64/60000]\n",
      "loss: 1.413002  [ 6464/60000]\n",
      "loss: 1.223933  [12864/60000]\n",
      "loss: 1.343636  [19264/60000]\n",
      "loss: 1.446776  [25664/60000]\n",
      "loss: 1.305322  [32064/60000]\n",
      "loss: 1.362889  [38464/60000]\n",
      "loss: 1.450037  [44864/60000]\n",
      "loss: 1.372284  [51264/60000]\n",
      "loss: 1.280709  [57664/60000]\n",
      "loss: 1.405477  [   64/60000]\n",
      "loss: 1.222594  [ 6464/60000]\n",
      "loss: 1.256128  [12864/60000]\n",
      "loss: 1.385092  [19264/60000]\n",
      "loss: 1.399007  [25664/60000]\n",
      "loss: 1.368714  [32064/60000]\n",
      "loss: 1.313716  [38464/60000]\n",
      "loss: 1.386798  [44864/60000]\n",
      "loss: 1.403351  [51264/60000]\n",
      "loss: 1.317135  [57664/60000]\n",
      "loss: 1.389449  [   64/60000]\n",
      "loss: 1.331862  [ 6464/60000]\n",
      "loss: 1.199083  [12864/60000]\n",
      "loss: 1.409085  [19264/60000]\n",
      "loss: 1.213758  [25664/60000]\n",
      "loss: 1.349794  [32064/60000]\n",
      "loss: 1.347327  [38464/60000]\n",
      "loss: 1.330568  [44864/60000]\n",
      "loss: 1.288986  [51264/60000]\n",
      "loss: 1.303428  [57664/60000]\n",
      "loss: 1.421770  [   64/60000]\n",
      "loss: 1.212007  [ 6464/60000]\n",
      "loss: 1.135021  [12864/60000]\n",
      "loss: 1.321771  [19264/60000]\n",
      "loss: 1.339867  [25664/60000]\n",
      "loss: 1.308359  [32064/60000]\n",
      "loss: 1.317210  [38464/60000]\n",
      "loss: 1.284393  [44864/60000]\n",
      "loss: 1.355515  [51264/60000]\n",
      "loss: 1.115547  [57664/60000]\n",
      "loss: 1.301430  [   64/60000]\n",
      "loss: 1.238208  [ 6464/60000]\n",
      "loss: 1.148242  [12864/60000]\n",
      "loss: 1.337273  [19264/60000]\n",
      "loss: 1.346036  [25664/60000]\n",
      "loss: 1.230230  [32064/60000]\n",
      "loss: 1.200452  [38464/60000]\n",
      "loss: 1.266958  [44864/60000]\n",
      "loss: 1.294582  [51264/60000]\n",
      "loss: 1.113226  [57664/60000]\n",
      "loss: 1.303669  [   64/60000]\n",
      "loss: 1.241644  [ 6464/60000]\n",
      "loss: 1.067146  [12864/60000]\n",
      "loss: 1.311470  [19264/60000]\n",
      "loss: 1.377586  [25664/60000]\n",
      "loss: 1.217803  [32064/60000]\n",
      "loss: 1.199325  [38464/60000]\n",
      "loss: 1.277883  [44864/60000]\n",
      "loss: 1.261911  [51264/60000]\n",
      "loss: 1.095151  [57664/60000]\n",
      "loss: 1.256296  [   64/60000]\n",
      "loss: 1.264485  [ 6464/60000]\n",
      "loss: 1.149620  [12864/60000]\n",
      "loss: 1.372615  [19264/60000]\n",
      "loss: 1.322708  [25664/60000]\n",
      "loss: 1.351004  [32064/60000]\n",
      "loss: 1.314169  [38464/60000]\n",
      "loss: 1.255259  [44864/60000]\n",
      "loss: 1.227413  [51264/60000]\n",
      "loss: 1.199635  [57664/60000]\n",
      "loss: 1.332131  [   64/60000]\n",
      "loss: 1.263027  [ 6464/60000]\n",
      "loss: 1.080138  [12864/60000]\n",
      "loss: 1.262636  [19264/60000]\n",
      "loss: 1.303771  [25664/60000]\n",
      "loss: 1.169136  [32064/60000]\n",
      "loss: 1.173988  [38464/60000]\n",
      "loss: 1.142402  [44864/60000]\n",
      "loss: 1.280903  [51264/60000]\n",
      "loss: 1.120427  [57664/60000]\n",
      "loss: 1.215804  [   64/60000]\n",
      "loss: 1.211990  [ 6464/60000]\n",
      "loss: 1.065194  [12864/60000]\n",
      "loss: 1.136117  [19264/60000]\n",
      "loss: 1.256875  [25664/60000]\n",
      "loss: 1.305362  [32064/60000]\n",
      "loss: 1.157193  [38464/60000]\n",
      "loss: 1.149149  [44864/60000]\n",
      "loss: 1.305915  [51264/60000]\n",
      "loss: 1.049788  [57664/60000]\n",
      "loss: 1.233226  [   64/60000]\n",
      "loss: 1.088895  [ 6464/60000]\n",
      "loss: 1.070253  [12864/60000]\n",
      "loss: 1.227205  [19264/60000]\n",
      "loss: 1.310139  [25664/60000]\n",
      "loss: 1.145993  [32064/60000]\n",
      "loss: 1.089647  [38464/60000]\n",
      "loss: 1.150988  [44864/60000]\n",
      "loss: 1.251172  [51264/60000]\n",
      "loss: 1.126057  [57664/60000]\n",
      "loss: 1.312174  [   64/60000]\n",
      "loss: 1.120259  [ 6464/60000]\n",
      "loss: 1.001866  [12864/60000]\n",
      "loss: 1.252278  [19264/60000]\n",
      "loss: 1.222062  [25664/60000]\n",
      "loss: 1.168522  [32064/60000]\n",
      "loss: 1.213117  [38464/60000]\n",
      "loss: 1.171938  [44864/60000]\n",
      "loss: 1.224141  [51264/60000]\n",
      "loss: 1.102756  [57664/60000]\n",
      "loss: 1.237356  [   64/60000]\n",
      "loss: 1.021267  [ 6464/60000]\n",
      "loss: 0.986261  [12864/60000]\n",
      "loss: 1.230875  [19264/60000]\n",
      "loss: 1.275735  [25664/60000]\n",
      "loss: 1.153044  [32064/60000]\n",
      "loss: 1.149753  [38464/60000]\n",
      "loss: 1.062380  [44864/60000]\n",
      "loss: 1.093980  [51264/60000]\n",
      "loss: 1.136138  [57664/60000]\n",
      "loss: 1.110264  [   64/60000]\n",
      "loss: 1.141558  [ 6464/60000]\n",
      "loss: 1.056126  [12864/60000]\n",
      "loss: 1.393476  [19264/60000]\n",
      "loss: 1.249326  [25664/60000]\n",
      "loss: 1.198032  [32064/60000]\n",
      "loss: 1.114475  [38464/60000]\n",
      "loss: 1.071412  [44864/60000]\n",
      "loss: 1.280413  [51264/60000]\n",
      "loss: 1.174694  [57664/60000]\n",
      "loss: 1.148519  [   64/60000]\n",
      "loss: 1.151442  [ 6464/60000]\n",
      "loss: 0.975682  [12864/60000]\n",
      "loss: 1.249294  [19264/60000]\n",
      "loss: 1.164195  [25664/60000]\n",
      "loss: 1.175414  [32064/60000]\n",
      "loss: 1.111108  [38464/60000]\n",
      "loss: 1.042881  [44864/60000]\n",
      "loss: 1.105095  [51264/60000]\n",
      "loss: 1.089329  [57664/60000]\n",
      "loss: 1.193816  [   64/60000]\n",
      "loss: 1.154132  [ 6464/60000]\n",
      "loss: 0.944284  [12864/60000]\n",
      "loss: 1.253187  [19264/60000]\n",
      "loss: 1.505448  [25664/60000]\n",
      "loss: 1.060172  [32064/60000]\n",
      "loss: 1.079543  [38464/60000]\n",
      "loss: 1.052472  [44864/60000]\n",
      "loss: 1.187908  [51264/60000]\n",
      "loss: 1.059319  [57664/60000]\n",
      "loss: 1.061471  [   64/60000]\n",
      "loss: 1.156327  [ 6464/60000]\n",
      "loss: 1.025838  [12864/60000]\n",
      "loss: 1.067667  [19264/60000]\n",
      "loss: 1.288964  [25664/60000]\n",
      "loss: 1.174176  [32064/60000]\n",
      "loss: 1.136476  [38464/60000]\n",
      "loss: 1.145139  [44864/60000]\n",
      "loss: 1.126753  [51264/60000]\n",
      "loss: 1.052544  [57664/60000]\n",
      "loss: 1.097160  [   64/60000]\n",
      "loss: 1.035774  [ 6464/60000]\n",
      "loss: 1.028547  [12864/60000]\n",
      "loss: 1.133951  [19264/60000]\n",
      "loss: 1.257595  [25664/60000]\n",
      "loss: 1.180611  [32064/60000]\n",
      "loss: 1.053385  [38464/60000]\n",
      "loss: 1.193506  [44864/60000]\n",
      "loss: 1.002936  [51264/60000]\n",
      "loss: 1.042160  [57664/60000]\n",
      "loss: 1.016657  [   64/60000]\n",
      "loss: 1.124811  [ 6464/60000]\n",
      "loss: 0.960008  [12864/60000]\n",
      "loss: 1.064682  [19264/60000]\n",
      "loss: 1.123472  [25664/60000]\n",
      "loss: 1.067486  [32064/60000]\n",
      "loss: 0.974000  [38464/60000]\n",
      "loss: 0.974108  [44864/60000]\n",
      "loss: 1.109098  [51264/60000]\n",
      "loss: 1.000283  [57664/60000]\n",
      "loss: 1.059599  [   64/60000]\n",
      "loss: 1.012436  [ 6464/60000]\n",
      "loss: 0.861171  [12864/60000]\n",
      "loss: 1.134228  [19264/60000]\n",
      "loss: 1.320990  [25664/60000]\n",
      "loss: 0.948062  [32064/60000]\n",
      "loss: 1.096104  [38464/60000]\n",
      "loss: 0.993657  [44864/60000]\n",
      "loss: 1.135679  [51264/60000]\n",
      "loss: 0.895718  [57664/60000]\n",
      "loss: 1.040695  [   64/60000]\n",
      "loss: 0.967945  [ 6464/60000]\n",
      "loss: 0.897307  [12864/60000]\n",
      "loss: 1.204295  [19264/60000]\n",
      "loss: 1.224892  [25664/60000]\n",
      "loss: 1.097407  [32064/60000]\n",
      "loss: 0.973112  [38464/60000]\n",
      "loss: 1.047434  [44864/60000]\n",
      "loss: 1.074091  [51264/60000]\n",
      "loss: 1.024575  [57664/60000]\n",
      "loss: 1.128746  [   64/60000]\n",
      "loss: 1.119223  [ 6464/60000]\n",
      "loss: 0.897816  [12864/60000]\n",
      "loss: 1.158009  [19264/60000]\n",
      "loss: 1.086917  [25664/60000]\n",
      "loss: 1.092602  [32064/60000]\n",
      "loss: 1.072866  [38464/60000]\n",
      "loss: 1.018801  [44864/60000]\n",
      "loss: 1.102806  [51264/60000]\n",
      "loss: 0.986919  [57664/60000]\n",
      "loss: 1.026058  [   64/60000]\n",
      "loss: 0.969197  [ 6464/60000]\n",
      "loss: 0.829052  [12864/60000]\n",
      "loss: 1.056276  [19264/60000]\n",
      "loss: 1.171682  [25664/60000]\n",
      "loss: 1.074589  [32064/60000]\n",
      "loss: 1.116607  [38464/60000]\n",
      "loss: 1.122228  [44864/60000]\n",
      "loss: 1.114514  [51264/60000]\n",
      "loss: 0.929143  [57664/60000]\n",
      "loss: 0.987264  [   64/60000]\n",
      "loss: 1.096613  [ 6464/60000]\n",
      "loss: 0.844305  [12864/60000]\n",
      "loss: 1.196432  [19264/60000]\n",
      "loss: 1.111186  [25664/60000]\n",
      "loss: 1.015506  [32064/60000]\n",
      "loss: 0.983659  [38464/60000]\n",
      "loss: 1.030090  [44864/60000]\n",
      "loss: 1.071878  [51264/60000]\n",
      "loss: 0.997937  [57664/60000]\n",
      "loss: 0.985770  [   64/60000]\n",
      "loss: 1.049718  [ 6464/60000]\n",
      "loss: 0.810485  [12864/60000]\n",
      "loss: 1.140279  [19264/60000]\n",
      "loss: 1.155694  [25664/60000]\n",
      "loss: 1.241780  [32064/60000]\n",
      "loss: 1.077459  [38464/60000]\n",
      "loss: 0.977804  [44864/60000]\n",
      "loss: 1.100630  [51264/60000]\n",
      "loss: 0.965155  [57664/60000]\n",
      "loss: 1.003288  [   64/60000]\n",
      "loss: 1.128302  [ 6464/60000]\n",
      "loss: 0.877745  [12864/60000]\n",
      "loss: 1.113296  [19264/60000]\n",
      "loss: 1.080389  [25664/60000]\n",
      "loss: 1.085750  [32064/60000]\n",
      "loss: 1.012660  [38464/60000]\n",
      "loss: 1.025923  [44864/60000]\n",
      "loss: 1.022262  [51264/60000]\n",
      "loss: 0.960695  [57664/60000]\n",
      "loss: 0.957203  [   64/60000]\n",
      "loss: 0.946650  [ 6464/60000]\n",
      "loss: 0.999145  [12864/60000]\n",
      "loss: 1.034534  [19264/60000]\n",
      "loss: 0.969809  [25664/60000]\n",
      "loss: 1.074645  [32064/60000]\n",
      "loss: 1.049867  [38464/60000]\n",
      "loss: 1.081588  [44864/60000]\n",
      "loss: 1.075444  [51264/60000]\n",
      "loss: 0.994518  [57664/60000]\n",
      "loss: 0.931766  [   64/60000]\n",
      "loss: 0.990655  [ 6464/60000]\n",
      "loss: 0.828097  [12864/60000]\n",
      "loss: 1.048579  [19264/60000]\n",
      "loss: 1.047893  [25664/60000]\n",
      "loss: 1.094067  [32064/60000]\n",
      "loss: 0.931340  [38464/60000]\n",
      "loss: 1.145376  [44864/60000]\n",
      "loss: 0.986703  [51264/60000]\n",
      "loss: 0.921034  [57664/60000]\n",
      "loss: 0.918008  [   64/60000]\n",
      "loss: 0.946337  [ 6464/60000]\n",
      "loss: 0.925674  [12864/60000]\n",
      "loss: 1.001437  [19264/60000]\n",
      "loss: 1.075214  [25664/60000]\n",
      "loss: 1.003089  [32064/60000]\n",
      "loss: 0.949232  [38464/60000]\n",
      "loss: 1.017106  [44864/60000]\n",
      "loss: 1.019665  [51264/60000]\n",
      "loss: 0.984272  [57664/60000]\n",
      "loss: 0.956833  [   64/60000]\n",
      "loss: 0.974286  [ 6464/60000]\n",
      "loss: 0.824433  [12864/60000]\n",
      "loss: 1.089841  [19264/60000]\n",
      "loss: 0.996742  [25664/60000]\n",
      "loss: 0.942573  [32064/60000]\n",
      "loss: 0.906242  [38464/60000]\n",
      "loss: 0.825465  [44864/60000]\n",
      "loss: 1.120620  [51264/60000]\n",
      "loss: 0.891469  [57664/60000]\n",
      "loss: 0.939750  [   64/60000]\n",
      "loss: 0.958801  [ 6464/60000]\n",
      "loss: 0.892704  [12864/60000]\n",
      "loss: 1.026492  [19264/60000]\n",
      "loss: 1.030303  [25664/60000]\n",
      "loss: 1.001972  [32064/60000]\n",
      "loss: 0.863195  [38464/60000]\n",
      "loss: 0.983652  [44864/60000]\n",
      "loss: 0.955564  [51264/60000]\n",
      "loss: 0.872115  [57664/60000]\n",
      "loss: 0.892996  [   64/60000]\n",
      "loss: 1.135822  [ 6464/60000]\n",
      "loss: 0.808582  [12864/60000]\n",
      "loss: 1.044350  [19264/60000]\n",
      "loss: 1.060554  [25664/60000]\n",
      "loss: 0.941692  [32064/60000]\n",
      "loss: 0.972115  [38464/60000]\n",
      "loss: 0.886326  [44864/60000]\n",
      "loss: 1.099364  [51264/60000]\n",
      "loss: 0.955334  [57664/60000]\n",
      "loss: 0.885097  [   64/60000]\n",
      "loss: 0.978387  [ 6464/60000]\n",
      "loss: 0.823959  [12864/60000]\n",
      "loss: 1.056759  [19264/60000]\n",
      "loss: 1.003590  [25664/60000]\n",
      "loss: 0.922518  [32064/60000]\n",
      "loss: 0.826462  [38464/60000]\n",
      "loss: 0.966910  [44864/60000]\n",
      "loss: 1.145026  [51264/60000]\n",
      "loss: 0.965794  [57664/60000]\n",
      "loss: 0.929260  [   64/60000]\n",
      "loss: 0.946673  [ 6464/60000]\n",
      "loss: 0.799212  [12864/60000]\n",
      "loss: 0.994555  [19264/60000]\n",
      "loss: 1.069179  [25664/60000]\n",
      "loss: 0.872183  [32064/60000]\n",
      "loss: 1.025118  [38464/60000]\n",
      "loss: 0.940156  [44864/60000]\n",
      "loss: 0.972753  [51264/60000]\n",
      "loss: 1.105221  [57664/60000]\n",
      "loss: 0.920220  [   64/60000]\n",
      "loss: 0.892700  [ 6464/60000]\n",
      "loss: 0.814277  [12864/60000]\n",
      "loss: 1.013454  [19264/60000]\n",
      "loss: 1.085886  [25664/60000]\n",
      "loss: 1.005410  [32064/60000]\n",
      "loss: 1.000005  [38464/60000]\n",
      "loss: 1.016248  [44864/60000]\n",
      "loss: 1.067058  [51264/60000]\n",
      "loss: 0.852451  [57664/60000]\n",
      "loss: 0.900458  [   64/60000]\n",
      "loss: 0.933911  [ 6464/60000]\n",
      "loss: 0.821484  [12864/60000]\n",
      "loss: 1.029828  [19264/60000]\n",
      "loss: 0.989973  [25664/60000]\n",
      "loss: 0.957417  [32064/60000]\n",
      "loss: 0.842698  [38464/60000]\n",
      "loss: 0.933787  [44864/60000]\n",
      "loss: 1.021225  [51264/60000]\n",
      "loss: 0.826148  [57664/60000]\n",
      "loss: 0.855086  [   64/60000]\n",
      "loss: 0.939037  [ 6464/60000]\n",
      "loss: 0.792930  [12864/60000]\n",
      "loss: 1.021417  [19264/60000]\n",
      "loss: 1.074600  [25664/60000]\n",
      "loss: 0.990404  [32064/60000]\n",
      "loss: 0.920692  [38464/60000]\n",
      "loss: 0.931619  [44864/60000]\n",
      "loss: 1.046781  [51264/60000]\n",
      "loss: 0.847107  [57664/60000]\n",
      "loss: 0.870733  [   64/60000]\n",
      "loss: 0.861791  [ 6464/60000]\n",
      "loss: 0.760681  [12864/60000]\n",
      "loss: 1.072945  [19264/60000]\n",
      "loss: 1.043651  [25664/60000]\n",
      "loss: 0.978006  [32064/60000]\n",
      "loss: 0.918875  [38464/60000]\n",
      "loss: 1.065338  [44864/60000]\n",
      "loss: 1.032595  [51264/60000]\n",
      "loss: 0.875298  [57664/60000]\n",
      "loss: 0.861149  [   64/60000]\n",
      "loss: 0.960168  [ 6464/60000]\n",
      "loss: 0.760021  [12864/60000]\n",
      "loss: 0.978055  [19264/60000]\n",
      "loss: 1.010353  [25664/60000]\n",
      "loss: 0.933635  [32064/60000]\n",
      "loss: 0.958667  [38464/60000]\n",
      "loss: 0.857124  [44864/60000]\n",
      "loss: 0.919025  [51264/60000]\n",
      "loss: 0.934442  [57664/60000]\n",
      "loss: 0.792192  [   64/60000]\n",
      "loss: 0.826784  [ 6464/60000]\n",
      "loss: 0.890632  [12864/60000]\n",
      "loss: 1.028850  [19264/60000]\n",
      "loss: 0.919979  [25664/60000]\n",
      "loss: 0.945948  [32064/60000]\n",
      "loss: 0.938929  [38464/60000]\n",
      "loss: 0.899258  [44864/60000]\n",
      "loss: 0.921709  [51264/60000]\n",
      "loss: 0.890788  [57664/60000]\n",
      "loss: 0.811167  [   64/60000]\n",
      "loss: 0.932631  [ 6464/60000]\n",
      "loss: 0.679895  [12864/60000]\n",
      "loss: 1.032599  [19264/60000]\n",
      "loss: 1.113867  [25664/60000]\n",
      "loss: 0.980657  [32064/60000]\n",
      "loss: 0.811840  [38464/60000]\n",
      "loss: 0.826601  [44864/60000]\n",
      "loss: 0.957387  [51264/60000]\n",
      "loss: 0.816250  [57664/60000]\n",
      "loss: 0.919119  [   64/60000]\n",
      "loss: 0.928835  [ 6464/60000]\n",
      "loss: 0.816769  [12864/60000]\n",
      "loss: 1.022641  [19264/60000]\n",
      "loss: 1.004894  [25664/60000]\n",
      "loss: 0.863156  [32064/60000]\n",
      "loss: 0.903213  [38464/60000]\n",
      "loss: 0.903424  [44864/60000]\n",
      "loss: 1.080048  [51264/60000]\n",
      "loss: 0.850423  [57664/60000]\n",
      "loss: 0.795071  [   64/60000]\n",
      "loss: 0.814038  [ 6464/60000]\n",
      "loss: 0.794874  [12864/60000]\n",
      "loss: 1.007108  [19264/60000]\n",
      "loss: 1.098318  [25664/60000]\n",
      "loss: 0.773280  [32064/60000]\n",
      "loss: 0.903949  [38464/60000]\n",
      "loss: 0.906317  [44864/60000]\n",
      "loss: 0.914695  [51264/60000]\n",
      "loss: 0.887238  [57664/60000]\n",
      "loss: 0.846399  [   64/60000]\n",
      "loss: 0.867987  [ 6464/60000]\n",
      "loss: 0.669330  [12864/60000]\n",
      "loss: 0.949249  [19264/60000]\n",
      "loss: 0.920429  [25664/60000]\n",
      "loss: 0.956548  [32064/60000]\n",
      "loss: 0.869704  [38464/60000]\n",
      "loss: 0.828138  [44864/60000]\n",
      "loss: 1.016350  [51264/60000]\n",
      "loss: 0.826250  [57664/60000]\n",
      "loss: 0.907083  [   64/60000]\n",
      "loss: 0.961525  [ 6464/60000]\n",
      "loss: 0.710590  [12864/60000]\n",
      "loss: 0.906840  [19264/60000]\n",
      "loss: 0.933528  [25664/60000]\n",
      "loss: 0.855847  [32064/60000]\n",
      "loss: 0.862328  [38464/60000]\n",
      "loss: 0.815423  [44864/60000]\n",
      "loss: 1.004429  [51264/60000]\n",
      "loss: 0.735210  [57664/60000]\n",
      "loss: 0.836953  [   64/60000]\n",
      "loss: 0.776028  [ 6464/60000]\n",
      "loss: 0.748519  [12864/60000]\n",
      "loss: 1.049277  [19264/60000]\n",
      "loss: 1.044530  [25664/60000]\n",
      "loss: 0.830416  [32064/60000]\n",
      "loss: 0.937790  [38464/60000]\n",
      "loss: 0.847451  [44864/60000]\n",
      "loss: 0.866180  [51264/60000]\n",
      "loss: 0.810239  [57664/60000]\n",
      "loss: 0.814041  [   64/60000]\n",
      "loss: 0.905665  [ 6464/60000]\n",
      "loss: 0.703311  [12864/60000]\n",
      "loss: 0.994824  [19264/60000]\n",
      "loss: 0.869673  [25664/60000]\n",
      "loss: 0.913720  [32064/60000]\n",
      "loss: 0.794405  [38464/60000]\n",
      "loss: 0.912864  [44864/60000]\n",
      "loss: 0.980937  [51264/60000]\n",
      "loss: 0.849504  [57664/60000]\n",
      "loss: 0.818259  [   64/60000]\n",
      "loss: 0.899631  [ 6464/60000]\n",
      "loss: 0.769895  [12864/60000]\n",
      "loss: 0.979555  [19264/60000]\n",
      "loss: 0.983563  [25664/60000]\n",
      "loss: 0.886890  [32064/60000]\n",
      "loss: 0.886823  [38464/60000]\n",
      "loss: 0.799141  [44864/60000]\n",
      "loss: 0.943028  [51264/60000]\n",
      "loss: 0.839365  [57664/60000]\n",
      "loss: 0.744033  [   64/60000]\n",
      "loss: 0.810367  [ 6464/60000]\n",
      "loss: 0.676671  [12864/60000]\n",
      "loss: 0.996096  [19264/60000]\n",
      "loss: 0.897594  [25664/60000]\n",
      "loss: 0.740680  [32064/60000]\n",
      "loss: 0.850028  [38464/60000]\n",
      "loss: 0.807707  [44864/60000]\n",
      "loss: 0.926459  [51264/60000]\n",
      "loss: 0.824271  [57664/60000]\n",
      "loss: 0.851100  [   64/60000]\n",
      "loss: 0.871047  [ 6464/60000]\n",
      "loss: 0.650749  [12864/60000]\n",
      "loss: 0.973761  [19264/60000]\n",
      "loss: 0.892675  [25664/60000]\n",
      "loss: 0.838903  [32064/60000]\n",
      "loss: 0.778078  [38464/60000]\n",
      "loss: 0.829107  [44864/60000]\n",
      "loss: 0.870141  [51264/60000]\n",
      "loss: 0.774446  [57664/60000]\n",
      "loss: 0.829255  [   64/60000]\n",
      "loss: 0.866140  [ 6464/60000]\n",
      "loss: 0.686331  [12864/60000]\n",
      "loss: 0.899909  [19264/60000]\n",
      "loss: 0.920106  [25664/60000]\n",
      "loss: 0.814465  [32064/60000]\n",
      "loss: 0.871267  [38464/60000]\n",
      "loss: 0.756862  [44864/60000]\n",
      "loss: 1.066847  [51264/60000]\n",
      "loss: 0.811595  [57664/60000]\n",
      "loss: 0.773099  [   64/60000]\n",
      "loss: 0.872211  [ 6464/60000]\n",
      "loss: 0.795605  [12864/60000]\n",
      "loss: 0.933296  [19264/60000]\n",
      "loss: 0.947616  [25664/60000]\n",
      "loss: 0.780953  [32064/60000]\n",
      "loss: 0.851018  [38464/60000]\n",
      "loss: 0.876836  [44864/60000]\n",
      "loss: 1.082715  [51264/60000]\n",
      "loss: 0.830944  [57664/60000]\n",
      "loss: 0.777015  [   64/60000]\n",
      "loss: 0.825967  [ 6464/60000]\n",
      "loss: 0.774136  [12864/60000]\n",
      "loss: 1.006906  [19264/60000]\n",
      "loss: 1.091728  [25664/60000]\n",
      "loss: 0.856963  [32064/60000]\n",
      "loss: 0.927956  [38464/60000]\n",
      "loss: 0.858317  [44864/60000]\n",
      "loss: 0.822863  [51264/60000]\n",
      "loss: 0.824159  [57664/60000]\n",
      "loss: 0.924761  [   64/60000]\n",
      "loss: 0.831648  [ 6464/60000]\n",
      "loss: 0.688573  [12864/60000]\n",
      "loss: 0.959281  [19264/60000]\n",
      "loss: 0.966153  [25664/60000]\n",
      "loss: 0.807095  [32064/60000]\n",
      "loss: 0.823599  [38464/60000]\n",
      "loss: 0.799228  [44864/60000]\n",
      "loss: 0.974855  [51264/60000]\n",
      "loss: 0.865686  [57664/60000]\n",
      "loss: 0.742575  [   64/60000]\n",
      "loss: 0.810512  [ 6464/60000]\n",
      "loss: 0.651316  [12864/60000]\n",
      "loss: 1.149338  [19264/60000]\n",
      "loss: 0.941274  [25664/60000]\n",
      "loss: 0.811017  [32064/60000]\n",
      "loss: 0.866292  [38464/60000]\n",
      "loss: 0.961244  [44864/60000]\n",
      "loss: 0.926714  [51264/60000]\n",
      "loss: 0.769882  [57664/60000]\n",
      "loss: 0.682216  [   64/60000]\n",
      "loss: 0.852319  [ 6464/60000]\n",
      "loss: 0.682671  [12864/60000]\n",
      "loss: 0.939015  [19264/60000]\n",
      "loss: 1.034873  [25664/60000]\n",
      "loss: 0.833965  [32064/60000]\n",
      "loss: 0.768438  [38464/60000]\n",
      "loss: 0.775419  [44864/60000]\n",
      "loss: 0.939664  [51264/60000]\n",
      "loss: 0.777288  [57664/60000]\n",
      "loss: 0.694227  [   64/60000]\n",
      "loss: 0.779899  [ 6464/60000]\n",
      "loss: 0.643240  [12864/60000]\n",
      "loss: 0.920737  [19264/60000]\n",
      "loss: 0.979196  [25664/60000]\n",
      "loss: 0.718523  [32064/60000]\n",
      "loss: 0.823709  [38464/60000]\n",
      "loss: 0.909205  [44864/60000]\n",
      "loss: 0.934101  [51264/60000]\n",
      "loss: 0.855655  [57664/60000]\n",
      "loss: 0.810288  [   64/60000]\n",
      "loss: 0.829061  [ 6464/60000]\n",
      "loss: 0.715877  [12864/60000]\n",
      "loss: 0.962487  [19264/60000]\n",
      "loss: 0.912281  [25664/60000]\n",
      "loss: 0.814451  [32064/60000]\n",
      "loss: 0.765419  [38464/60000]\n",
      "loss: 0.768552  [44864/60000]\n",
      "loss: 1.028910  [51264/60000]\n",
      "loss: 0.861960  [57664/60000]\n",
      "loss: 0.795210  [   64/60000]\n",
      "loss: 0.791424  [ 6464/60000]\n",
      "loss: 0.722324  [12864/60000]\n",
      "loss: 0.950583  [19264/60000]\n",
      "loss: 0.873359  [25664/60000]\n",
      "loss: 0.771870  [32064/60000]\n",
      "loss: 0.881839  [38464/60000]\n",
      "loss: 0.849264  [44864/60000]\n",
      "loss: 0.962826  [51264/60000]\n",
      "loss: 0.827179  [57664/60000]\n",
      "loss: 0.743909  [   64/60000]\n",
      "loss: 0.824809  [ 6464/60000]\n",
      "loss: 0.661144  [12864/60000]\n",
      "loss: 0.960580  [19264/60000]\n",
      "loss: 1.026652  [25664/60000]\n",
      "loss: 0.813381  [32064/60000]\n",
      "loss: 0.775748  [38464/60000]\n",
      "loss: 0.839996  [44864/60000]\n",
      "loss: 0.954652  [51264/60000]\n",
      "loss: 0.830605  [57664/60000]\n",
      "loss: 0.739619  [   64/60000]\n",
      "loss: 0.742776  [ 6464/60000]\n",
      "loss: 0.727954  [12864/60000]\n",
      "loss: 1.029864  [19264/60000]\n",
      "loss: 0.939167  [25664/60000]\n",
      "loss: 0.798380  [32064/60000]\n",
      "loss: 0.836789  [38464/60000]\n",
      "loss: 0.818530  [44864/60000]\n",
      "loss: 0.944694  [51264/60000]\n",
      "loss: 0.854258  [57664/60000]\n",
      "loss: 0.720853  [   64/60000]\n",
      "loss: 0.786771  [ 6464/60000]\n",
      "loss: 0.618595  [12864/60000]\n",
      "loss: 0.893701  [19264/60000]\n",
      "loss: 0.898742  [25664/60000]\n",
      "loss: 0.767491  [32064/60000]\n",
      "loss: 0.842937  [38464/60000]\n",
      "loss: 0.904293  [44864/60000]\n",
      "loss: 1.029356  [51264/60000]\n",
      "loss: 0.724036  [57664/60000]\n",
      "loss: 0.728348  [   64/60000]\n",
      "loss: 0.825144  [ 6464/60000]\n",
      "loss: 0.715319  [12864/60000]\n",
      "loss: 0.945434  [19264/60000]\n",
      "loss: 0.878477  [25664/60000]\n",
      "loss: 0.846758  [32064/60000]\n",
      "loss: 0.751130  [38464/60000]\n",
      "loss: 0.807384  [44864/60000]\n",
      "loss: 0.943209  [51264/60000]\n",
      "loss: 0.753531  [57664/60000]\n",
      "loss: 0.752943  [   64/60000]\n",
      "loss: 0.731843  [ 6464/60000]\n",
      "loss: 0.629729  [12864/60000]\n",
      "loss: 1.005825  [19264/60000]\n",
      "loss: 0.870853  [25664/60000]\n",
      "loss: 0.736688  [32064/60000]\n",
      "loss: 0.843259  [38464/60000]\n",
      "loss: 0.831375  [44864/60000]\n",
      "loss: 0.938681  [51264/60000]\n",
      "loss: 0.715558  [57664/60000]\n",
      "loss: 0.626860  [   64/60000]\n",
      "loss: 0.762083  [ 6464/60000]\n",
      "loss: 0.672021  [12864/60000]\n",
      "loss: 0.967194  [19264/60000]\n",
      "loss: 0.871370  [25664/60000]\n",
      "loss: 0.928575  [32064/60000]\n",
      "loss: 0.729555  [38464/60000]\n",
      "loss: 0.758783  [44864/60000]\n",
      "loss: 0.874543  [51264/60000]\n",
      "loss: 0.754678  [57664/60000]\n",
      "loss: 0.745249  [   64/60000]\n",
      "loss: 0.729690  [ 6464/60000]\n",
      "loss: 0.618992  [12864/60000]\n",
      "loss: 1.008714  [19264/60000]\n",
      "loss: 0.792237  [25664/60000]\n",
      "loss: 0.770019  [32064/60000]\n",
      "loss: 0.779647  [38464/60000]\n",
      "loss: 0.806844  [44864/60000]\n",
      "loss: 0.837805  [51264/60000]\n",
      "loss: 0.739680  [57664/60000]\n",
      "loss: 0.698147  [   64/60000]\n",
      "loss: 0.772569  [ 6464/60000]\n",
      "loss: 0.680777  [12864/60000]\n",
      "loss: 0.926334  [19264/60000]\n",
      "loss: 0.809461  [25664/60000]\n",
      "loss: 0.802736  [32064/60000]\n",
      "loss: 0.818263  [38464/60000]\n",
      "loss: 0.857079  [44864/60000]\n",
      "loss: 1.106163  [51264/60000]\n",
      "loss: 0.800233  [57664/60000]\n",
      "loss: 0.760750  [   64/60000]\n",
      "loss: 0.745870  [ 6464/60000]\n",
      "loss: 0.640350  [12864/60000]\n",
      "loss: 0.904121  [19264/60000]\n",
      "loss: 0.973926  [25664/60000]\n",
      "loss: 0.715969  [32064/60000]\n",
      "loss: 0.666355  [38464/60000]\n",
      "loss: 0.901410  [44864/60000]\n",
      "loss: 0.862659  [51264/60000]\n",
      "loss: 0.674433  [57664/60000]\n",
      "loss: 0.758297  [   64/60000]\n",
      "loss: 0.763268  [ 6464/60000]\n",
      "loss: 0.618766  [12864/60000]\n",
      "loss: 0.912792  [19264/60000]\n",
      "loss: 0.869673  [25664/60000]\n",
      "loss: 0.774012  [32064/60000]\n",
      "loss: 0.845953  [38464/60000]\n",
      "loss: 0.721957  [44864/60000]\n",
      "loss: 0.852422  [51264/60000]\n",
      "loss: 0.717259  [57664/60000]\n",
      "loss: 0.722077  [   64/60000]\n",
      "loss: 0.716428  [ 6464/60000]\n",
      "loss: 0.632280  [12864/60000]\n",
      "loss: 0.927907  [19264/60000]\n",
      "loss: 0.891289  [25664/60000]\n",
      "loss: 0.780051  [32064/60000]\n",
      "loss: 0.700298  [38464/60000]\n",
      "loss: 0.831969  [44864/60000]\n",
      "loss: 1.012746  [51264/60000]\n",
      "loss: 0.748502  [57664/60000]\n",
      "loss: 0.739930  [   64/60000]\n",
      "loss: 0.741000  [ 6464/60000]\n",
      "loss: 0.721019  [12864/60000]\n",
      "loss: 0.925748  [19264/60000]\n",
      "loss: 0.866387  [25664/60000]\n",
      "loss: 0.836084  [32064/60000]\n",
      "loss: 0.835141  [38464/60000]\n",
      "loss: 0.844410  [44864/60000]\n",
      "loss: 0.931233  [51264/60000]\n",
      "loss: 0.812624  [57664/60000]\n",
      "loss: 0.746372  [   64/60000]\n",
      "loss: 0.762053  [ 6464/60000]\n",
      "loss: 0.735016  [12864/60000]\n",
      "loss: 0.882532  [19264/60000]\n",
      "loss: 0.869950  [25664/60000]\n",
      "loss: 0.717209  [32064/60000]\n",
      "loss: 0.826711  [38464/60000]\n",
      "loss: 0.778255  [44864/60000]\n",
      "loss: 0.871835  [51264/60000]\n",
      "loss: 0.752109  [57664/60000]\n",
      "loss: 0.651897  [   64/60000]\n",
      "loss: 0.766893  [ 6464/60000]\n",
      "loss: 0.617109  [12864/60000]\n",
      "loss: 0.957857  [19264/60000]\n",
      "loss: 0.928941  [25664/60000]\n",
      "loss: 0.703463  [32064/60000]\n",
      "loss: 0.793236  [38464/60000]\n",
      "loss: 0.766107  [44864/60000]\n",
      "loss: 0.946244  [51264/60000]\n",
      "loss: 0.850440  [57664/60000]\n",
      "loss: 0.652476  [   64/60000]\n",
      "loss: 0.730401  [ 6464/60000]\n",
      "loss: 0.635829  [12864/60000]\n",
      "loss: 0.940064  [19264/60000]\n",
      "loss: 0.909441  [25664/60000]\n",
      "loss: 0.759874  [32064/60000]\n",
      "loss: 0.810117  [38464/60000]\n",
      "loss: 0.777237  [44864/60000]\n",
      "loss: 0.912080  [51264/60000]\n",
      "loss: 0.785818  [57664/60000]\n",
      "loss: 0.644236  [   64/60000]\n",
      "loss: 0.779117  [ 6464/60000]\n",
      "loss: 0.644602  [12864/60000]\n",
      "loss: 0.852724  [19264/60000]\n",
      "loss: 0.814712  [25664/60000]\n",
      "loss: 0.682567  [32064/60000]\n",
      "loss: 0.797100  [38464/60000]\n",
      "loss: 0.865136  [44864/60000]\n",
      "loss: 0.847529  [51264/60000]\n",
      "loss: 0.701661  [57664/60000]\n",
      "loss: 0.788362  [   64/60000]\n",
      "loss: 0.753729  [ 6464/60000]\n",
      "loss: 0.621487  [12864/60000]\n",
      "loss: 0.937191  [19264/60000]\n",
      "loss: 0.825767  [25664/60000]\n",
      "loss: 0.795715  [32064/60000]\n",
      "loss: 0.711344  [38464/60000]\n",
      "loss: 0.739704  [44864/60000]\n",
      "loss: 0.834446  [51264/60000]\n",
      "loss: 0.666948  [57664/60000]\n",
      "loss: 0.632247  [   64/60000]\n",
      "loss: 0.775405  [ 6464/60000]\n",
      "loss: 0.618000  [12864/60000]\n",
      "loss: 0.917044  [19264/60000]\n",
      "loss: 0.859854  [25664/60000]\n",
      "loss: 0.642282  [32064/60000]\n",
      "loss: 0.735675  [38464/60000]\n",
      "loss: 0.817452  [44864/60000]\n",
      "loss: 0.918126  [51264/60000]\n",
      "loss: 0.761994  [57664/60000]\n",
      "loss: 0.717582  [   64/60000]\n",
      "loss: 0.647970  [ 6464/60000]\n",
      "loss: 0.656348  [12864/60000]\n",
      "loss: 0.899046  [19264/60000]\n",
      "loss: 0.908048  [25664/60000]\n",
      "loss: 0.815079  [32064/60000]\n",
      "loss: 0.691294  [38464/60000]\n",
      "loss: 0.804379  [44864/60000]\n",
      "loss: 0.951703  [51264/60000]\n",
      "loss: 0.736243  [57664/60000]\n",
      "loss: 0.771091  [   64/60000]\n",
      "loss: 0.774964  [ 6464/60000]\n",
      "loss: 0.634631  [12864/60000]\n",
      "loss: 0.892153  [19264/60000]\n",
      "loss: 0.816957  [25664/60000]\n",
      "loss: 0.836665  [32064/60000]\n",
      "loss: 0.815343  [38464/60000]\n",
      "loss: 0.750554  [44864/60000]\n",
      "loss: 0.841222  [51264/60000]\n",
      "loss: 0.803379  [57664/60000]\n",
      "loss: 0.616336  [   64/60000]\n",
      "loss: 0.809586  [ 6464/60000]\n",
      "loss: 0.653854  [12864/60000]\n",
      "loss: 0.872697  [19264/60000]\n",
      "loss: 0.998232  [25664/60000]\n",
      "loss: 0.793241  [32064/60000]\n",
      "loss: 0.791492  [38464/60000]\n",
      "loss: 0.700104  [44864/60000]\n",
      "loss: 0.947521  [51264/60000]\n",
      "loss: 0.689086  [57664/60000]\n"
     ]
    }
   ],
   "source": [
    "model = model.to(\"cuda\")\n",
    "for i in range(100):\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "model = model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 0.695426 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 0.695426 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 0.695426 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 0.695426 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 0.695426 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 0.695426 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 0.695426 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 74.9%, Avg loss: 0.695426 \n",
      "\n",
      "945 ms ± 15 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "test(test_dataloader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(\"cpu\")\n",
    "\n",
    "model_int8 = torch.ao.quantization.quantize_dynamic(\n",
    "    model,  # the original model\n",
    "    {torch.nn.Linear, torch.nn.Conv2d, torch.nn.AvgPool2d, torch.nn.ReLU},  # a set of layers to dynamically quantize\n",
    "    dtype=torch.qint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 0.695496 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 0.695496 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 0.695496 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 0.695496 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 0.695496 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 0.695496 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 0.695496 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 75.0%, Avg loss: 0.695496 \n",
      "\n",
      "339 ms ± 5.59 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "test(test_dataloader, model_int8, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_int8, \"model_post_dynamic_quantized.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
